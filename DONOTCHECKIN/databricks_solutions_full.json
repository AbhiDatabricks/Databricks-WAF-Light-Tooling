{
  "DG-01-01": {
    "identifier": "DG-01-01",
    "practice": "Establish data governance process",
    "capabilities": "nan",
    "details": "Data governance is the management of the availability, usability, integrity, and security of an organization's data. By strengthening data governance, organizations can ensure the quality of data that is critical for accurate analysis and decision making, helping to identify new opportunities, improve customer satisfaction, and ultimately increase revenue. It helps organizations comply with data privacy regulations and improve security measures, reducing the risk of data breaches and penalties. \nEffective data governance also eliminates redundancies and streamlines data management, resulting in cost savings and increased operational efficiency. \nTopics to cover with data goevernance include\n- Data ownership, roles and responsibilities\n- Policies and procedures to guide data quality, privacy, security, and compliance\n- Data quality to ensure the accuracy, completeness, and reliability of data\n- Data security and compliance\n- Data architecture and integration\n- Metadata management to ensure that data is well understood\nUnity catalog is at the center of the Databricks Data Intelligence Platform and helps with many aspects of data governance from metadata management, lineage, to access control.\nAWS | AZURE | GCP",
    "pillar": "Data And AI Governance"
  },
  "DG-01-02": {
    "identifier": "DG-01-02",
    "practice": "Manage metadata for all data assets in one place",
    "capabilities": "Unity Catalog",
    "details": "The benefits of managing all metadata in one place are similar to the benefits of maintaining a single source of truth for all your data. These include reduced data redundancy, increased data integrity, and avoiding misunderstandings based on different definitions or taxonomies. It's also easier to implement global policies, standards, and rules when dealing with one source.\n\nAs a best practice, run the lakehouse in a single account with one Unity Catalog. The top-level container of objects in Unity Catalog is a metastore. It stores data assets (such as tables and views) and the permissions that govern access to them. Use a single metastore per cloud region and do not access metastores across regions to avoid latency issues.\nAWS | Azure | GCP\n\nDatabricks recommends using catalogs to provide segregation across your organization\u2019s information architecture. Often this means that catalogs can correspond to software development environment scope, team, or business unit.\nAWS | Azure | GCP ",
    "pillar": "Data And AI Governance"
  },
  "DG-01-03": {
    "identifier": "DG-01-03",
    "practice": "Track data and AI lineage to drive visibility of the data",
    "capabilities": "Unity Catalog",
    "details": "Data lineage is a powerful tool that helps data leaders drive greater visibility and understanding of the data in their organizations. It describes the transformation and refinement of data from source to insight. Lineage includes the capture of all relevant metadata and events associated with the data in its lifecycle, including the source of the data set, what other data sets were used to create it, who created it and when, what transformations were performed, what other data sets use it, and many other events and attributes. Data lineage can be used for many data-related use cases:\n\n- Compliance and audit readiness: Data lineage helps organizations trace the source of tables and fields. This is important for meeting the requirements of many compliance regulations, such as General Data Protection Regulation (GDPR), California Consumer Privacy Act (CCPA), Health Insurance Portability and Accountability Act (HIPAA), Basel Committee on Banking Supervision (BCBS) 239, and Sarbanes-Oxley Act (SOX).\n\n- Impact analysis/change management: Data goes through multiple transformations from the source to the final business-ready table. Understanding the potential impact of data changes on downstream users becomes important from a risk-management perspective. This impact can be easily determined using the data lineage collected by Unity Catalog.\n\n- Data quality assurance: Understanding where a data set came from and what transformations have been applied provides much better context for data scientists and analysts, enabling them to gain better and more accurate insights.\n\n- Debugging and diagnostics: In the event of an unexpected result, data lineage helps data teams perform root cause analysis by tracing the error back to its source. This dramatically reduces debugging time.\n\nUnity Catalog captures runtime data lineage across queries run on Databricks. Lineage is supported for all languages and is captured down to the column level. Lineage data includes notebooks, workflows, and dashboards related to the query. Lineage can be visualized in Catalog Explorer in near real-time and retrieved with the Databricks Data Lineage REST API.\nAWS | Azure | GCP ",
    "pillar": "Data And AI Governance"
  },
  "DG-01-04": {
    "identifier": "DG-01-04",
    "practice": "Add consistent descriptions to your metadata",
    "capabilities": "Databricks IQ; Unity Catalog",
    "details": "Enriching metadata with comments can help accelerate processes...\n\nAI-generated comments are intended to provide a general description of tables and columns based on the schema. The descriptions are tuned for data in a business and enterprise context, using example schemas from several open datasets across various industries. The model was evaluated with hundreds of simulated samples to verify it avoids generating harmful or inappropriate descriptions.\nAWS | Azure | GCP",
    "pillar": "Data And AI Governance"
  },
  "DG-01-05": {
    "identifier": "DG-01-05",
    "practice": "Allow easy data discovery for data consumers ",
    "capabilities": "Unity Catalog",
    "details": "Easy data discovery enables data scientists, data analysts, and data engineers to quickly discover and reference relevant data and accelerate time to value.\n\nDatabricks Catalog Explorer provides a UI to explore and manage data, schemas (databases), tables, and permissions, data owners, external locations, and credentials. Additionally, you can use the Insights tab in Catalog Explorer to view the most frequent recent queries and users of any table registered in Unity Catalog.\nAWS | Azure | GCP",
    "pillar": "Data And AI Governance"
  },
  "DG-01-06": {
    "identifier": "DG-01-06",
    "practice": "Govern AI assets together with data",
    "capabilities": "Unity Catalog",
    "details": "Governing all your AI assets is important because it ensures unified visibility and control over data and AI assets, which is crucial for maintaining data quality, security, and compliance across different platforms and clouds. This governance facilitates the management of access policies consistently and efficiently, enhancing operational intelligence and ensuring that AI systems operate within regulatory guidelines and business requirements.\nWith Unity Catalog, organizations can implement a unified governance framework for their structured and unstructured data, machine learning models, notebooks, features, functions, and files, enhancing security and compliance across clouds and platforms.\nAWS | Azure | GCP\n\nModel aliases in machine learning workflows allow you to assign a mutable, named reference to a specific version of a registered model. This functionality is beneficial for tracking and managing different stages of a model\u2019s lifecycle, indicating the current deployment status of any given model version.\nAWS | Azure | GCP",
    "pillar": "Data And AI Governance"
  },
  "DG-02-01": {
    "identifier": "DG-02-01",
    "practice": "Centralize access control for all data and AI assets",
    "capabilities": "Unity Catalog",
    "details": "Centralizing access control for all data assets is important because it simplifies the security and governance of your data and AI assets by providing a central place to administer and audit access to these assets. This approach helps in managing data and AI object access more efficiently, ensuring that operational requirements around segregation of duty are enforced, which is crucial for regulatory compliance and risk avoidance.\n\nDatabricks provides access to audit logs of activities performed by Databricks users, allowing your enterprise to monitor detailed Databricks usage patterns. There are two types of logs: Workspace-level audit logs with workspace-level events and account-level audit logs with account-level events.\nAWS | Azure | GCP\nAWS | Azure | GCP",
    "pillar": "Data And AI Governance"
  },
  "DG-02-02": {
    "identifier": "DG-02-02",
    "practice": "Configure audit logging",
    "capabilities": "Unity Catalog",
    "details": "Audit logging is important for providing a detailed account of system activities (user actions, changes to settings, etc.) that could impact the system's integrity. Whereas standard system logs are designed to help developers troubleshoot errors, audit logs provide a historical record of activity for compliance purposes and other business policy enforcement. Maintaining robust audit logs can help identify and ensure preparedness in the face of threats, breaches, fraud, and other system issues.\n\nDatabricks provides access to audit logs of activities performed by Databricks users, allowing your enterprise to monitor detailed Databricks usage patterns. There are two types of logs: Workspace-level audit logs with workspace-level events and account-level audit logs with account-level events.\nAWS | Azure | GCP\nAWS | Azure | GCP",
    "pillar": "Data And AI Governance"
  },
  "DG-02-03": {
    "identifier": "DG-02-03",
    "practice": "Audit data platform events",
    "capabilities": "Unity Catalog",
    "details": "Auditing data platform events is important because (see above)\n\nUnity Catalog captures an audit log of actions performed against the metastore. This enables admins to access fine-grained details about who accessed a given dataset and what actions they performed. \nAWS | Azure | GCP\nAWS | Azure | GCP\n\nFor secure sharing with Delta Sharing, Databricks provides audit logs to monitor Delta Sharing events, including:\n\n- When someone creates, modifies, updates, or deletes a share or a recipient.\n- When a recipient accesses an activation link and downloads the credential.\n- When a recipient accesses shares or data in shared tables.\n- When a recipient\u2019s credential is rotated or expires. \nAWS | Azure | GCP",
    "pillar": "Data And AI Governance"
  },
  "DG-03-01": {
    "identifier": "DG-03-01",
    "practice": "Define and document data quality standards ",
    "capabilities": "nan",
    "details": "Defining clear and actionable data quality standards is crucial, because it helps ensure that data used for analysis, reporting, and decision-making is reliable and trustworthy. Documenting these standards helps ensure that they are upheld.\n\nData quality standards should be based on the specific needs of the business and should address dimensions of data quality such as accuracy, completeness, consistency, timeliness, and reliability\n- Accuracy: Ensure data accurately reflects real-world values.\n- Completeness: All necessary data should be captured and no critical data should be missing.\n- Consistency: Data across all systems should be consistent and not contradict other data.\n- Timeliness: Data should be updated and available in a timely manner.\n- Reliability: Data should be sourced and processed in a way that ensures its dependability.",
    "pillar": "Data And AI Governance"
  },
  "DG-03-02": {
    "identifier": "DG-03-02",
    "practice": "Use data quality tools for profiling, cleansing, validating, and monitoring data",
    "capabilities": "nan",
    "details": "Leverage data quality tools for profiling, cleansing, validating, and monitoring data. These tools help in automating the processes of detecting and correcting data quality issues, which is vital for scaling data quality initiatives across large datasets typical in data lakes.\n\nFor teams using DLT, you can use expectations to define data quality constraints on the contents of a dataset. Expectations allow you to guarantee data arriving in tables meets data quality requirements and provide insights into data quality for each pipeline update.\nAWS | Azure | GCP",
    "pillar": "Data And AI Governance"
  },
  "DG-03-03": {
    "identifier": "DG-03-03",
    "practice": "Implement and enforce standardized data formats and definitions",
    "capabilities": "nan",
    "details": "Standardized data formats and definitions helps achieve uniformity in data representation across all systems to facilitate easier data integration and analysis, reduce costs, and improve decision-making via enhance communication and collaboration between different teams and departments. It also helps provide structure for creating and maintaining data quality.\n\nDevelop and enforce a standard data dictionary that includes definitions, formats, and acceptable values for all data elements used across the organization.\n\nUse consistent naming conventions, date formats, and measurement units across all databases and applications to prevent discrepancies and confusion.",
    "pillar": "Data And AI Governance"
  },
  "R-01-01": {
    "identifier": "R-01-01",
    "practice": "Use a data format that supports ACID transactions",
    "capabilities": "Delta Lake",
    "details": "Delta Lake is an open source storage format that brings reliability to data lakes. Delta Lake provides ACID transactions, schema enforcement, scalable metadata handling, and unifies streaming and batch data processing. Delta Lake runs on top of your existing data lake and is fully compatible with Apache Spark APIs. Delta Lake on Databricks allows you to configure Delta Lake based on your workload patterns. \n\nAWS | Azure | GCP",
    "pillar": "Reliability"
  },
  "R-01-02": {
    "identifier": "R-01-02",
    "practice": "Use a resilient distributed data engine for all workloads",
    "capabilities": "Apache Spark; Photon",
    "details": "Apache Spark, as the compute engine of the Databricks lakehouse, is based on resilient distributed data processing. In case of an internal Spark task not returning a result as expected, Apache Spark automatically reschedules the missing tasks and continues with the execution of the entire job. This is helpful for failures outside the code, like a short network issue or a revoked spot VM. Working with both the SQL API and the Spark DataFrame API comes with this resilience built into the engine.\n\nIn the Databricks lakehouse, Photon, a native vectorized engine entirely written in C++, is high performance compute compatible with Apache Spark APIs. \n\nAWS | Azure | GCP",
    "pillar": "Reliability"
  },
  "R-01-03": {
    "identifier": "R-01-03",
    "practice": "Automatically rescue invalid or nonconforming data ",
    "capabilities": "Delta Live Tables",
    "details": "Invalid or nonconforming data can lead to crashes of workloads that rely on an established data format. To increase the end-to-end resilience of the whole process, it is best practice to filter out invalid and nonconforming data at ingestion. Supporting rescued data ensures you never lose or miss out on data during ingest or ETL. The rescued data column contains any data that wasn\u2019t parsed, either because it was missing from the given schema, because there was a type mismatch, or the column casing in the record or file didn\u2019t match that in the schema. \n\nAWS | Azure | GCP\nAWS | Azure | GCP",
    "pillar": "Reliability"
  },
  "R-01-04": {
    "identifier": "R-01-04",
    "practice": "Configure jobs for automatic retries and termination",
    "capabilities": "Databricks Workflows",
    "details": "Distributed systems are complex, and a failure at one point can potentially cascade throughout the system.\n\nDatabricks jobs support an automatic retry policy that determines when and how many times failed runs are retried.\n\nDelta Live Tables also automates failure recovery by using escalating retries to balance speed with reliability. See Development and production modes.\n\nOn the other hand, a task that hangs can prevent the whole job from finishing, thus incurring high costs. Databricks jobs support a timeout configuration to terminate jobs that take longer than expected. \n\nAWS | Azure | GCP",
    "pillar": "Reliability"
  },
  "R-01-05": {
    "identifier": "R-01-05",
    "practice": "Use a scalable and production-grade model serving infrastructure",
    "capabilities": "Databricks Model Serving",
    "details": "For batch and streaming inference, use Databricks jobs and MLflow to deploy models as Apache Spark UDFs to leverage job scheduling, retries, autoscaling, and so on. \nModel serving provides a scalable and production-grade model real-time serving infrastructure. It processes your machine learning models using MLflow and exposes them as REST API endpoints. This functionality uses serverless compute, which means that the endpoints and associated compute resources are managed and run in the Databricks cloud account. \n\nAWS | Azure | GCP\nAWS | Azure | GCP",
    "pillar": "Reliability"
  },
  "R-01-06": {
    "identifier": "R-01-06",
    "practice": "Use managed services for your workloads",
    "capabilities": "Databricks Platform",
    "details": "Leverage managed services of the Databricks Data Intelligence Platform like serverless compute, model serving, or Delta Live Tables where possible. These services are - without extra effort by the customer - operated by Databricks in a reliable and scalable way, making workloads more reliable.\n\nAWS | Azure | GCP\nAWS | Azure | GCP\nAWS | Azure | GCP",
    "pillar": "Reliability"
  },
  "R-02-01": {
    "identifier": "R-02-01",
    "practice": "Use a layered storage architecture",
    "capabilities": "Databricks Lakehouse Architecture",
    "details": "Curate data by creating a layered architecture and ensuring data quality increases as data moves through the layers. A common layering approach is:\n\nRaw layer (bronze): Source data gets ingested into the lakehouse into the first layer and should be persisted there. When all downstream data is created from the raw layer, rebuilding the subsequent layers from this layer is possible if needed.\n\nCurated layer (silver): The purpose of the second layer is to hold cleansed, refined, filtered and aggregated data. The goal of this layer is to provide a sound, reliable foundation for analyses and reports across all roles and functions.\n\nFinal layer (gold): The third layer is created around business or project needs. It provides a different view as data products to other business units or projects, preparing data around security needs (such as anonymized data) or optimizing for performance (such as with pre aggregated views). The data products in this layer are seen as the truth for the business.\n\nThe final layer should only contain high-quality data and can be fully trusted from a business point of view.\nAWS | Azure | GCP",
    "pillar": "Reliability"
  },
  "R-02-02": {
    "identifier": "R-02-02",
    "practice": "Improve data integrity by reducing data redundancy",
    "capabilities": "Databricks Platform",
    "details": "Copying or duplicating data creates data redundancy and will lead to lost integrity, lost data lineage, and often different access permissions. This will decrease the quality of the data in the lakehouse. A temporary or throwaway copy of data is not harmful on its own - it is sometimes necessary for boosting agility, experimentation and innovation. However, if these copies become operational and regularly used for business decisions, they become data silos. These data silos getting out of sync has a significant negative impact on data integrity and quality, raising questions such as \u201cWhich data set is the master?\u201d or \u201cIs the data set up to date?\u201d.\n\nAWS | Azure | GCP",
    "pillar": "Reliability"
  },
  "R-02-03": {
    "identifier": "R-02-03",
    "practice": "Actively manage schemas",
    "capabilities": "Unity Catalog",
    "details": "Uncontrolled schema changes can lead to invalid data and failing jobs that use these data sets. Databricks has several methods to validate and enforce the schema:\n\nDelta Lake supports schema validation and schema enforcement by automatically handling schema variations to prevent the insertion of bad records during ingestion.\n\nAuto Loader detects the addition of new columns as it processes your data. By default, the addition of a new column causes your streams to stop with an UnknownFieldException. Auto Loader supports several modes for schema evolution. \n\nAWS | Azure | GCP",
    "pillar": "Reliability"
  },
  "R-02-04": {
    "identifier": "R-02-04",
    "practice": "Use constraints and data expectations",
    "capabilities": "Delta Live Tables",
    "details": "Delta tables support standard SQL constraint management clauses that ensure that the quality and integrity of data added to a table are automatically verified. When a constraint is violated, Delta Lake throws an InvariantViolationException error to signal that the new data can\u2019t be added. See Constraints on Databricks.\n\nTo further improve this handling, Delta Live Tables supports Expectations: Expectations define data quality constraints on the contents of a data set. An expectation consists of a description, an invariant, and an action to take when a record fails the invariant. Expectations to queries use Python decorators or SQL constraint clauses. See Manage data quality with Delta Live Tables. \n\nAWS | Azure | GCP",
    "pillar": "Reliability"
  },
  "R-02-05": {
    "identifier": "R-02-05",
    "practice": "Take a data-centric approach to machine learning",
    "capabilities": "Databricks Platform",
    "details": "One guiding principle that continues to lie at the heart of the AI vision for the Databricks Data Intelligence Platform is taking a data-centric approach to machine learning. With the increasing prevalence of generative AI, this perspective remains just as important. The core constituents of any ML project can be viewed simply as data pipelines \n\nFeature engineering, training, inference, and monitoring pipelines are data pipelines. They must be as robust as other production data engineering processes. Data quality is crucial in any ML application, so ML data pipelines should employ systematic approaches to monitoring and mitigating data quality issues. Avoid tools that make it challenging to join data from ML predictions, model monitoring, and so on, with the rest of your data. The simplest way to achieve this is to develop ML applications on the same platform used to manage production data. For example, instead of downloading training data to a laptop, where it is hard to govern and reproduce results, secure the data in cloud storage and make that storage available to your training process. \n\neBook",
    "pillar": "Reliability"
  },
  "R-03-01": {
    "identifier": "R-03-01",
    "practice": "Enable autoscaling for ETL workloads",
    "capabilities": "Databricks Workflows",
    "details": "Autoscaling allows clusters to resize automatically based on workloads. Autoscaling can benefit many use cases and scenarios from both a cost and performance perspective. The documentation provides considerations for determining whether to use Autoscaling and how to get the most benefit.\n\nFor streaming workloads, Databricks recommends using Delta Live Tables with autoscaling. See Use autoscaling to increase efficiency and reduce resource usage. \n\nAWS | Azure | GCP\nAWS | Azure | GCP",
    "pillar": "Reliability"
  },
  "R-03-02": {
    "identifier": "R-03-02",
    "practice": "Use autoscaling for SQL Warehouses",
    "capabilities": "Databricks SQL",
    "details": "The scaling parameter of a SQL warehouse sets the minimum and the maximum number of clusters over which queries sent to the warehouse are distributed. The default is a minimum of one and a maximum of one cluster.\n\nTo handle more concurrent users for a given warehouse, increase the cluster count. To learn how Databricks adds clusters to and removes clusters from a warehouse, see SQL warehouse sizing, scaling, and queuing behavior. \n\nAWS | Azure | GCP\nAWS | Azure | GCP\n\nDatabricks enhanced autoscaling optimizes cluster utilization by automatically allocating cluster resources based on workload volume, with minimal impact on the data processing latency of your pipelines. \n\nAWS | Azure | GCP\nAWS | Azure | GCP",
    "pillar": "Reliability"
  },
  "R-04-01": {
    "identifier": "R-04-01",
    "practice": "Recover from Structured Streaming query failures",
    "capabilities": "Structured Streaming",
    "details": "Structured Streaming provides fault-tolerance and data consistency for streaming queries. Using Databricks workflows, you can easily configure your Structured Streaming queries to restart on failure automatically. The restarted query continues where the failed one left off. \n\nAWS | Azure | GCP",
    "pillar": "Reliability"
  },
  "R-04-02": {
    "identifier": "R-04-02",
    "practice": "Recover ETL jobs using data time travel capabilities",
    "capabilities": "Delta Lake - Delta Time Travel",
    "details": "Despite thorough testing, a job in production can fail or produce some unexpected, even invalid, data. Sometimes this can be fixed with an additional job after understanding the source of the issue and fixing the pipeline that led to the issue in the first place. However, often this is not straightforward, and the respective job should be rolled back. Using Delta Time travel allows users to easily roll back changes to an older version or timestamp, repair the pipeline, and restart the fixed pipeline. See What is Delta Lake time travel?.\n\nA convenient way to do so is the RESTORE command. \n\nAWS | Azure | GCP\nBlog",
    "pillar": "Reliability"
  },
  "R-04-03": {
    "identifier": "R-04-03",
    "practice": "Leverage a job automation framework with built-in recovery",
    "capabilities": "Databricks Workflows",
    "details": "Databricks Workflows are built for recovery. When a task in a multi-task job fails (and, as such, all dependent tasks), Databricks Workflows provide a matrix view of the runs, which lets you examine the issue that led to the failure. See View runs for a job. Whether it was a short network issue or a real issue in the data, you can fix it and start a repair run in Databricks Workflows. It runs only the failed and dependent tasks and keep the successful results from the earlier run, saving time and money. \n\nAWS | Azure | GCP",
    "pillar": "Reliability"
  },
  "R-04-04": {
    "identifier": "R-04-04",
    "practice": "Configure a disaster recovery pattern",
    "capabilities": "nan",
    "details": "Databricks is often a core part of an overall data ecosystem that includes many services, including upstream data ingestion services (batch/streaming), cloud-native storage, downstream tools and services such as business intelligence apps, and orchestration tooling. Some of your use cases might be particularly sensitive to a regional service-wide outage.\n\nDisaster recovery involves a set of policies, tools, and procedures that enable the recovery or continuation of vital technology infrastructure and systems following a natural or human-induced disaster. A large cloud service like Azure, AWS, or GCP serves many customers and has built-in guards against a single failure. For example, a region is a group of buildings connected to different power sources to guarantee that a single power loss will not shut down a region. However, cloud region failures can happen, and the degree of disruption and its impact on your organization can vary.\n\nEssential parts of a disaster recovery strategy are selecting a strategy (active/active or active/passive), selecting the right toolset, and testing both failover and restore. \n\nAWS | Azure | GCP",
    "pillar": "Reliability"
  },
  "R-05-01": {
    "identifier": "R-05-01",
    "practice": "Monitor data platform events",
    "capabilities": "nan",
    "details": "Monitor Data platform and ML events",
    "pillar": "Reliability"
  },
  "R-05-02": {
    "identifier": "R-05-02",
    "practice": "Monitor cloud events",
    "capabilities": "nan",
    "details": "Monitor events on your cloud provider",
    "pillar": "Reliability"
  },
  "CO-01-01": {
    "identifier": "CO-01-01",
    "practice": "Use performance optimized data formats",
    "capabilities": "Delta Lake",
    "details": "Delta Lake is an open-source storage layer that enhances data lakes by providing ACID transactions, scalable metadata handling, and schema enforcement, primarily built on Apache Spark and Parquet. It supports a lakehouse architecture, allowing integration with various compute engines and programming languages. Delta Lake offers significant performance optimizations such as data skipping, indexing, and file management techniques like compaction and Z-Ordering. Additionally, it includes features like time travel for data versioning and Delta Live Tables for managing batch and streaming data pipelines efficiently. These capabilities make Delta Lake suitable for handling large-scale, transactional workloads and real-time analytics, ensuring data integrity and consistency across diverse data management scenarios\nBlog",
    "pillar": "Cost Optimization"
  },
  "CO-01-02": {
    "identifier": "CO-01-02",
    "practice": "Use job clusters",
    "capabilities": "Databricks Workflows",
    "details": "A job is a way to run non-interactive code in a Databricks cluster. For example, you can run an extract, transform, and load (ETL) workload interactively or on a schedule. Of course, you can also run jobs interactively in the notebook UI. However, on job clusters, the non-interactive workloads will cost significantly less than on all-purpose clusters.\nAn additional advantage is that every job or workflow runs on a new cluster, isolating workloads from one another.\n\nAWS | Azure | GCP\nPricing",
    "pillar": "Cost Optimization"
  },
  "CO-01-03": {
    "identifier": "CO-01-03",
    "practice": "Use SQL warehouse for SQL workloads",
    "capabilities": "Databricks Serverless SQL",
    "details": "For interactive SQL workloads, a Databricks SQL warehouse is the most cost-efficient engine \n\nAWS | Azure | GCP",
    "pillar": "Cost Optimization"
  },
  "CO-01-04": {
    "identifier": "CO-01-04",
    "practice": "Use up-to-date runtimes for your workloads",
    "capabilities": "Databricks Cluster Configuration",
    "details": "The Databricks platform provides different runtimes that are optimized for data engineering tasks (Databricks Runtime) or for Machine Learning (Databricks Runtime for Machine Learning). The runtimes are built to provide the best selection of libraries for the tasks and ensure that all provided libraries are up-to-date and work together optimally. Databricks Runtime is released on a regular cadence and offers performance improvements between major releases. These improvements in performance often lead to cost savings due to more efficient usage of cluster resources.\n\nAWS | Azure | GCP",
    "pillar": "Cost Optimization"
  },
  "CO-01-05": {
    "identifier": "CO-01-05",
    "practice": "Only use GPUs for the right workloads",
    "capabilities": "Databricks Cluster Configuration",
    "details": "Virtual machines with GPUs can dramatically speed up computational processes for deep learning, but have a significantly higher price than CPU-only machines. Use GPU instances only for workloads that have GPU-accelerated libraries.\nMost workloads do not use GPU-accelerated libraries do not benefit from GPU-enabled instances. Workspace admins can restrict GPU machines and clusters to prevent unnecessary use. \n\nAWS | Azure | GCP\nBlog",
    "pillar": "Cost Optimization"
  },
  "CO-01-06": {
    "identifier": "CO-01-06",
    "practice": "Use Serverless for your workloads",
    "capabilities": "Databricks Serverless",
    "details": "BI workloads typically use data in bursts and generate multiple concurrent queries. For example, someone using a BI tool might update a dashboard, write a query, or simply analyze query results without interacting further with the platform. This example demonstrates two requirements:\n\nTerminate clusters during idle periods to save costs.\nHave compute resources available quickly (for both start-up and scale-up) to satisfy user queries when they request new or updated data with the BI tool.\n\nServerless SQL warehouses start and scale up in seconds, so both immediate availability and termination during idle times can be achieved. This results in a great user experience and overall cost savings.\nAdditionally, serverless SQL warehouses scale down earlier than non-serverless warehouses, resulting lower costs. \n\nAWS | Azure | GCP\n\n\nDatabricks Model Serving provides a unified interface to deploy, govern, and query AI models. Each model you serve is available as a REST API that you can integrate into your web or client application.\n\nModel Serving provides a highly available and low-latency service for deploying models. The service automatically scales up or down to meet demand changes, saving infrastructure costs while optimizing latency performance. This functionality uses serverless compute. \n\nAWS | Azure | GCP",
    "pillar": "Cost Optimization"
  },
  "CO-01-07": {
    "identifier": "CO-01-07",
    "practice": "Use the right instance type",
    "capabilities": "Databricks Cluster Configuration",
    "details": "Based on your workloads, it is also important to choose the right instance family to get the best performance/price ratio. Some simple rules of thumb are:\nMemory optimized for ML, heavy shuffle & spill workloads\nCompute optimized for Structured Streaming workloads, maintenance jobs (e.g. Optimize & Vacuum)\nStorage optimized for workloads that benefit from caching, e.g. ad-hoc & interactive data analysis\nGPU optimized for specific ML & DL workloads\nGeneral purpose in absence of specific requirements",
    "pillar": "Cost Optimization"
  },
  "CO-01-08": {
    "identifier": "CO-01-08",
    "practice": "Choose the most efficient cluster size",
    "capabilities": "Databricks Cluster Configuration",
    "details": "Number of workers, instance type and size are important factors for compute configurations. Along with those when sizing compute, consider the data consumed, computational complexity, data partitioning, and parallelism required. For simple ETL workloads, focus on compute-optimized configurations, while memory and storage are important for shuffle-heavy workloads. Balancing the number of workers and instance size is crucial, as both can affect network I/O. Lastly, consider caching benefits for workloads that require frequent re-reads of the same data, using storage-optimized configurations with Delta Cache. \n\nAWS | Azure | GCP\nAWS | Azure | GCP",
    "pillar": "Cost Optimization"
  },
  "CO-01-09": {
    "identifier": "CO-01-09",
    "practice": "Evaluate performance optimized query engines",
    "capabilities": "Databricks Cluster Configuration",
    "details": "Photon is a high-performance Databricks-native vectorized query engine that speeds up your SQL workloads and DataFrame API calls (for data ingestion, ETL, streaming, data science, and interactive queries). Photon is compatible with Apache Spark APIs, so getting started is as easy as turning it on \u2013 no code changes and no lock-in.\nThe observed speedup can lead to significant cost savings, and jobs that run regularly should be evaluated to see whether they are not only faster but also cheaper with Photon.",
    "pillar": "Cost Optimization"
  },
  "CO-02-01": {
    "identifier": "CO-02-01",
    "practice": "Leverage auto-scaling compute",
    "capabilities": "Databricks Cluster Configuration",
    "details": "Autoscaling allows your workloads to use the right amount of compute required to complete your jobs.\n\nEnable autoscaling for batch workloads.\nEnable autoscaling for SQL warehouse.\nAWS | Azure | GCP\n\nCompute auto-scaling has limitations scaling down cluster size for Structured Streaming workloads. Databricks recommends using Delta Live Tables with Enhanced Autoscaling for streaming workloads.\nUse Delta Live Tables Enhanced Autoscaling.\nAWS | Azure | GCP",
    "pillar": "Cost Optimization"
  },
  "CO-02-02": {
    "identifier": "CO-02-02",
    "practice": "Use auto termination",
    "capabilities": "Databricks Cluster Configuration",
    "details": "Databricks provides a number of features to help control costs by reducing idle resources and controlling when compute resources can be deployed.\nConfigure auto termination for all interactive clusters. After a specified idle time, the cluster shuts down. See Automatic termination.\n\nIf a starting time that is significantly shorter than a full cluster start would be acceptable, consider using cluster pools. See Pool best practices. Databricks pools reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances. When a cluster is attached to a pool, cluster nodes are created using the pool\u2019s idle instances. If the pool has no idle instances, the pool expands by allocating a new instance from the instance provider in order to accommodate the cluster\u2019s request. When a cluster releases an instance, it returns to the pool and is free for another cluster to use. Only clusters attached to a pool can use that pool\u2019s idle instances.\nDatabricks does not charge DBUs while instances are idle in the pool, resulting in cost savings. Instance provider billing does apply. \n\nAWS | Azure | GCP\nAWS | Azure | GCP",
    "pillar": "Cost Optimization"
  },
  "CO-02-03": {
    "identifier": "CO-02-03",
    "practice": "Use compute policies to control costs",
    "capabilities": "Databricks Cluster Policies",
    "details": "Cluster policies can enforce many cost specific restrictions for clusters. See Operational Excellence - Use cluster policies. For example:\n\nEnable cluster autoscaling with a set minimum number of worker nodes.\nEnable cluster auto termination with a reasonable value (for example, 1 hour) to avoid paying for idle times.\nEnsure that only cost-efficient VM instances can be selected. Follow the best practices for cluster configuration. See Compute configuration best practices.\nApply a spot instance strategy.\n\nAWS | Azure | GCP",
    "pillar": "Cost Optimization"
  },
  "CO-03-01": {
    "identifier": "CO-03-01",
    "practice": "Monitor costs",
    "capabilities": "Unity Catalog",
    "details": "The account console allows viewing the billable usage. As a Databricks account owner or account admin, you can also use the account console to download billable usage logs. Databricks system tables provide detailed usage details with system tables you can use these to monitor usage.\n\nAs a best practice, the full costs (including VMs, storage, and network infrastructure) should be monitored. This can be achieved by cloud provider cost management tools or by adding third party tools.\n\nAWS | Azure | GCP",
    "pillar": "Cost Optimization"
  },
  "CO-03-02": {
    "identifier": "CO-03-02",
    "practice": "Tag clusters for cost attribution",
    "capabilities": "Databricks Cluster Configuration",
    "details": "To monitor cost and accurately attribute Databricks usage to your organization\u2019s business units and teams (for example, for chargebacks), you can tag clusters and pools. These tags propagate to detailed DBU usage reports and to cloud provider VMs and blob storage instances for cost analysis.\n\nEnsure that cost control and attribution are already in mind when setting up workspaces and clusters for teams and use cases. This streamlines tagging and improves the accuracy of cost attributions.\n\nFor the overall costs, DBU virtual machine, disk, and any associated network costs must be considered. For serverless SQL warehouses this is simpler since the DBU costs already include virtual machine and disk costs.\n\nAWS | Azure | GCP",
    "pillar": "Cost Optimization"
  },
  "CO-03-03": {
    "identifier": "CO-03-03",
    "practice": "Implement observability to track & chargeback cost",
    "capabilities": "Databricks Cluster Configuration",
    "details": "When working with complex technical ecosystems, proactively understanding the unknowns is key to maintaining platform stability and controlling costs. Observability provides a way to analyze and optimize systems based on the data they generate. This is different from monitoring, which focuses on identifying new patterns rather than tracking known issues.\nDatabricks provide great observability capabilities using System tables that are Databricks-hosted analytical stores of a customer account\u2019s operational data found in the system catalog. They provide historical observability across the account and include user-friendly tabular information on platform telemetry.",
    "pillar": "Cost Optimization"
  },
  "CO-03-04": {
    "identifier": "CO-03-04",
    "practice": "Share cost reports regularly",
    "capabilities": "Unity Catalog - System Tables",
    "details": "Create cost reports every month to track growth and anomalies in consumption. Share these reports broken down to use cases or teams with the teams that own the respective workloads by using cluster tagging. This avoids surprises and allows teams to proactively adapt their workloads if costs get too high.\n\nUse the account console to get high level usage - AWS | Azure | GCP\n\nDatabricks recommends using Unity Catalog system tables to generate detailed usage reports (by usage tags, etc) for enhanced cost reporting.",
    "pillar": "Cost Optimization"
  },
  "CO-03-05": {
    "identifier": "CO-03-05",
    "practice": "Monitor and manage Delta Sharing egress costs",
    "capabilities": "System Tables",
    "details": "Unlike other data sharing platforms, Delta Sharing does not require data replication. This model has many advantages, but it means that your cloud vendor may charge data egress fees when you share data across clouds or regions. See Monitor and manage Delta Sharing egress costs (for providers) to monitor and manage egress charges.",
    "pillar": "Cost Optimization"
  },
  "CO-04-01": {
    "identifier": "CO-04-01",
    "practice": "Balance always-on and triggered streaming",
    "capabilities": "Databricks Streaming",
    "details": "Traditionally, when people think about streaming, terms such as \u201creal-time,\u201d \u201c24/7,\u201d or \u201calways on\u201d come to mind. If data ingestion happens in \u201creal-time\u201d, the underlying cluster needs to run 24/7, producing consumption costs every single hour of the day.\n\nHowever, not every use case that is based on a continuous stream of events needs these events to be added to the analytics data set immediately. If the business requirement for the use case only needs fresh data every few hours or every day, then this requirement can be achieved with only several runs a day, leading to a significant cost reduction for the workload. Databricks recommends using Structured Streaming with trigger AvailableNow for incremental workloads that do not have low latency requirements. See Configuring incremental batch processing.\n\nAWS | Azure | GCP",
    "pillar": "Cost Optimization"
  },
  "CO-04-02": {
    "identifier": "CO-04-02",
    "practice": "Balance between on-demand and capacity excess instances",
    "capabilities": "Databricks Cluster Configuration",
    "details": "Spot instances use cloud virtual machine excess resources that are available at a cheaper price. To save cost, Databricks supports creating clusters using spot instances. It is recommended to always have the first instance (Spark driver) as an on-demand virtual machine. Spot instances are a great selection for workloads when it is acceptable to take longer because one or more spot instances have been evicted by the cloud provider. \n\nAWS | Azure | GCP",
    "pillar": "Cost Optimization"
  },
  "PE-01-01": {
    "identifier": "PE-01-01",
    "practice": "Use serverless architecture",
    "capabilities": "Databricks Serverless Compute",
    "details": "With the serverless compute on the Databricks Data Intelligence Platform, the compute layer runs in the customer\u2019s Databricks account. Workspace admins can create serverless SQL warehouses that enable instant compute and are managed by Databricks. A serverless SQL warehouse uses compute clusters hosted in the Databricks customer account. Use them with Databricks SQL queries just like you usually would with the original Databricks SQL warehouses. Serverless compute comes with a very fast starting time for SQL warehouses (10s and below), and the infrastructure is managed by Databricks.\n\nThis leads to improved productivity:\n\nCloud administrators no longer have to manage complex cloud environments, for example by adjusting quotas, creating and maintaining networking assets, and joining billing sources.\n\nUsers benefit from near-zero waiting times for cluster start and improved concurrency on their queries.\n\nCloud administrators can refocus their time on higher-value projects instead of managing low-level cloud components. \n\nAWS | Azure | GCP",
    "pillar": "Performance efficiency"
  },
  "PE-01-02": {
    "identifier": "PE-01-02",
    "practice": "Use an enterprise grade model serving service",
    "capabilities": "Databricks Model Serving",
    "details": "Databricks Model Serving provides a unified interface to deploy, govern, and query AI models. Each model you serve is available as a REST API that you can integrate into your web or client application.\n\nModel Serving provides a highly available and low-latency service for deploying models. The service automatically scales up or down to meet demand changes, saving infrastructure costs while optimizing latency performance. This functionality uses serverless compute. \n\nAWS | Azure | GCP",
    "pillar": "Performance efficiency"
  },
  "PE-02-01": {
    "identifier": "PE-02-01",
    "practice": "Understand your data ingestion and access patterns",
    "capabilities": "nan",
    "details": "From a performance perspective, data access patterns - such as \u201caggregations versus point access\u201d or \u201cscan versus search\u201d - behave differently depending on the data size. Large files are more efficient for scan queries and smaller files better for search since you have to read fewer data to find the specific row(s).\n\nFor ingestion patterns, it\u2019s common to use DML statements. DML statements are most performant when the data is clustered, and you can simply isolate the section of data. Keeping the data clustered and isolatable on ingestion is important: Consider keeping a natural time sort order and apply as many filters as possible to the ingest target table. For append-only and overwrite ingestion workloads, there isn't much to consider, as this is a relatively cheap operation.\n\nThe ingestion and access patterns often point to an obvious data layout and clustering. If they do not, decide what is more important to your business and skew toward how to solve that goal better. \n\nAWS | Azure | GCP",
    "pillar": "Performance efficiency"
  },
  "PE-02-02": {
    "identifier": "PE-02-02",
    "practice": "Use parallel computation where it is beneficial",
    "capabilities": "Apache Spark",
    "details": "Time to value is an important dimension when working with data. While many use cases can be easily implemented on a single machine (small data, few and simple computation steps), often use cases come up that:\n\nNeed to process large data sets.\n\nHave long running times due to complicated algorithms.\n\nMust be repeated 100s and 1000s of times.\n\nThe cluster environment of the Databricks platform is a great environment to distribute these workloads efficiently. It automatically parallelism SQL queries across all nodes of a cluster and it provides libraries for Python and Scala to do the same. Under the hood, the engines Apache Spark and Photon analyze the queries, determine the optimal way of parallel execution, and manage the distributed execution in a resilient way.\nHere are some parallel procesing capabilities in databricks that works with apache sparrk\n\nAWS | Azure | GCP\nAWS | Azure | GCP\nAWS | Azure | GCP\nMLlib\nAWS | Azure | GCP\nAWS | Azure | GCP",
    "pillar": "Performance efficiency"
  },
  "PE-02-03": {
    "identifier": "PE-02-03",
    "practice": "Analyze the whole chain of execution",
    "capabilities": "Workflows; Unity Catalog",
    "details": "Most pipelines or consumption patterns use a chain of systems. For example, for BI tools the performance is impacted by several factors:\nThe BI tool itself.\nThe connector that connects the BI tool and the SQL engine.\nThe SQL engine where the BI tool sends the query.\nFor best-in-class performance, the whole chain needs to be taken into account and selected/tuned for best performance. \n\nAWS | Azure | GCP",
    "pillar": "Performance efficiency"
  },
  "PE-02-04": {
    "identifier": "PE-02-04",
    "practice": "Prefer larger clusters",
    "capabilities": "Databricks Cluster Configuration",
    "details": "Plan for larger clusters, especially when the workload scales linearly. In that case, it is not more expensive to use a large cluster for a workload than to use a smaller one. It\u2019s just faster. The key is that you're renting the cluster for the length of the workload. So, if you spin up two worker clusters and it takes an hour, you're paying for those workers for the full hour. Similarly, if you spin up a four-worker cluster and it takes only half an hour (here comes the linear scalability into play), the costs are the same. If costs are the primary driver with a very flexible SLA, an autoscaling cluster is almost always going to be the cheapest but not necessarily the fastest. ",
    "pillar": "Performance efficiency"
  },
  "PE-02-05": {
    "identifier": "PE-02-05",
    "practice": "Use native Spark operations",
    "capabilities": "Apache Spark",
    "details": "User Defined Functions (UDFs) are a great way to extend the functionality of Spark SQL. However, don\u2019t use Python or Scala UDFs if a native function exists:\nSpark SQL\nPySpark\n\nReasons:\nTo transfer data between Python and Spark, serialization is needed. This drastically slows down queries.\nHigher efforts for implementing and testing functionality already existing in the platform.\n\nIf native functions are missing and should be implemented as Python UDFs, use Pandas UDFs. Apache Arrow ensures data moves efficiently back and forth between Spark and Python.\n\nAWS | Azure | GCP\nApache Arrow ",
    "pillar": "Performance efficiency"
  },
  "PE-02-06": {
    "identifier": "PE-02-06",
    "practice": "Use native platform engines",
    "capabilities": "Photon",
    "details": "Photon is the engine on Databricks that provides fast query performance at low cost \u2013 from data ingestion, ETL, streaming, data science, and interactive queries \u2013 directly on your data lake. Photon is compatible with Apache Spark APIs, so getting started is as easy as turning it on \u2013 no code changes and no lock-in.\n\nPhoton is part of a high-performance runtime that runs your existing SQL and DataFrame API calls faster and reduces your total cost per workload. Photon is used by default in Databricks SQL warehouses. \nPhoton is available by default for all Databricks SQL Warehouse Types\n\nAWS | Azure | GCP",
    "pillar": "Performance efficiency"
  },
  "PE-02-07": {
    "identifier": "PE-02-07",
    "practice": "Understand your hardware and workload type",
    "capabilities": "Databricks Cluster Configuration",
    "details": "Not all cloud VMs are created equally. The different families of machines offered by cloud providers are all different enough to matter. There are obvious differences - RAM and cores - and more subtle differences - processor type and generation, network bandwidth guarantees, and local high-speed storage versus local disk versus remote disk. There are also differences in the \u201cspot\u201d markets. These should be understood before deciding on the best VM type for your workload. \n\nAWS | Azure | GCP\n\nPlease note - Serverless compute manages clusters automatically, so this is not needed for serverless compute.",
    "pillar": "Performance efficiency"
  },
  "PE-02-08": {
    "identifier": "PE-02-08",
    "practice": "Use caching",
    "capabilities": "Databricks Cluster Configuration",
    "details": "There are two types of caching available in Databricks: Delta caching and Spark caching. \nUse Disk Cache and Avoid Spark Caching\nAWS | Azure | GCP\nSpark performance tuning\n\nAdditional Cache Types: \nQuery Result Cache - AWS | Azure | GCP \nDatabricks SQL UI caching - AWS | Azure | GCP  \nPrewarm Delta cache for BI workloads\n\nPrewarm clusters (Serverless compute manages clusters automatically, so this is not needed for serverless compute.)\nAWS | Azure | GCP",
    "pillar": "Performance efficiency"
  },
  "PE-02-09": {
    "identifier": "PE-02-09",
    "practice": "Use compaction",
    "capabilities": "Optimize with ZOrder",
    "details": "Delta Lake on Databricks can improve the speed of reading queries from a table. One way to improve this speed is to coalesce small files into larger ones. You trigger compaction by running the OPTIMIZE command. See Compact data files with optimize on Delta Lake.\n\nYou can also compact small files automatically using Auto Optimize. See Consider file size tuning. \n\nAWS | Azure | GCP",
    "pillar": "Performance efficiency"
  },
  "PE-02-10": {
    "identifier": "PE-02-10",
    "practice": "Use data skipping",
    "capabilities": "Optimize with ZOrder",
    "details": "Data skipping: To achieve this, data skipping information is collected automatically when you write data into a Delta table (by default Delta Lake on Databricks collects statistics on the first 32 columns defined in your table schema). Delta Lake on Databricks takes advantage of this information (minimum and maximum values) at query time to provide faster queries. See Data skipping for Delta Lake.\n\nFor best results, apply Z-ordering, a technique to collocate related information in the same set of files. This co-locality is automatically used on Databricks by Delta Lake data-skipping algorithms. This behavior dramatically reduces the amount of data Delta Lake on Databricks needs to read.\n\nDynamic file pruning: Dynamic file pruning (DFP) can significantly improve the performance of many queries on Delta tables. DFP is especially efficient for non-partitioned tables or joins on non-partitioned columns. \nAWS | Azure | GCP\nAWS | Azure | GCP\nAWS | Azure | GCP\nDelta Lake liquid clustering replaces table partitioning and ZORDER to simplify data layout decisions and optimize query performance. Liquid clustering provides flexibility to redefine clustering keys without rewriting existing data, allowing data layout to evolve alongside analytic needs over time.\n\nAWS | Azure | GCP",
    "pillar": "Performance efficiency"
  },
  "PE-02-11": {
    "identifier": "PE-02-11",
    "practice": "Enable Predictive Optimization on your metastore",
    "capabilities": "Predictive Optimization",
    "details": "Predictive optimization removes the need to manually manage maintenance operations for Delta tables on Databricks.\n\nWith predictive optimization enabled, Databricks automatically identifies tables that would benefit from maintenance operations and runs them for the user. Maintenance operations are only run as necessary, eliminating both unnecessary runs for maintenance operations and the burden associated with tracking and troubleshooting performance.\n\nAWS | Azure | GCP",
    "pillar": "Performance efficiency"
  },
  "PE-02-12": {
    "identifier": "PE-02-12",
    "practice": "Avoid over-partitioning",
    "capabilities": "Partitioning",
    "details": "In the past, partitioning was the most common way to skip data. However, partitioning is static and manifests as a file system hierarchy. There is no easy way to change partitions if the access patterns change over time. Often, partitioning leads to over-partitioning - in other words, too many partitions with too small files, which results in bad query performance. See Partitions.\n\nIn the meantime, a much better choice than partitioning is Z-ordering.\nAWS | Azure | GCP",
    "pillar": "Performance efficiency"
  },
  "PE-02-14": {
    "identifier": "PE-02-14",
    "practice": "Consider file size tuning",
    "capabilities": "Auto Optimize",
    "details": "The term auto optimize is sometimes used to describe functionality controlled by the settings delta.autoCompact and delta.optimizeWrite. This term has been retired in favor of describing each setting individually. See Configure Delta Lake to control data file size.\n\nAuto Optimize is particularly useful in the following scenarios:\nStreaming use cases where latency in the order of minutes is acceptable.\nMERGE INTO is the preferred method of writing into Delta Lake.\nCREATE TABLE AS SELECT or INSERT INTO are commonly used operations. \n\nAWS | Azure | GCP",
    "pillar": "Performance efficiency"
  },
  "PE-02-15": {
    "identifier": "PE-02-15",
    "practice": "Optimize join performance",
    "capabilities": "Databricks Adaptive Query Execution",
    "details": "A range join occurs when two relations are joined using a point in interval or interval overlap condition. The range join optimization support in Databricks Runtime can bring orders of magnitude improvement in query performance but requires careful manual tuning \n\nAWS | Azure | GCP\n\nData skew is a condition in which a table\u2019s data is unevenly distributed among partitions in the cluster. Data skew can severely downgrade the performance of queries, especially those with joins. Joins between big tables require shuffling data, and the skew can lead to an extreme imbalance of work in the cluster. \nAWS | Azure | GCP\nAWS | Azure | GCP",
    "pillar": "Performance efficiency"
  },
  "PE-02-16": {
    "identifier": "PE-02-16",
    "practice": "Run analyze table to collect table statistics",
    "capabilities": "Analyze Table",
    "details": "Run analyze table to collect statistics on the entire table for the query plan\nThis information is persisted in the metastore and helps the query optimizer by:\n\nChoosing the proper join type.\nSelecting the correct build side in a hash-join.\nCalibrating the join order in a multi-way join.\n\nIt should be run alongside OPTIMIZE on a daily basis and is recommended on tables < 5TB. The only caveat is that analyze table is not incremental.\nAWS | Azure | GCP",
    "pillar": "Performance efficiency"
  },
  "PE-03-01": {
    "identifier": "PE-03-01",
    "practice": "Test on data representative of production data",
    "capabilities": "nan",
    "details": "Run performance testing on production data (read-only) or similar data. When using similar data, characteristics like volume, file layout, and data skews should be like production data, since this has a significant impact on performance.",
    "pillar": "Performance efficiency"
  },
  "PE-03-02": {
    "identifier": "PE-03-02",
    "practice": "Take prewarming of resources into account",
    "capabilities": "Databricks Pools",
    "details": "Take prewarming of resources into account\nThe first query on a new cluster is slower than all the others:\n\nIn general, cluster resources need to initialize on multiple layers.\n\nWhen caching is part of the setup, the first run ensures that the data is in the cache, which speeds up subsequent jobs.\n\nPrewarming resources - running specific queries for the sake of initializing resources and filling caches (for example, after a cluster restart) - can significantly increase the performance of the first queries. So, to understand the behavior for the different scenarios, test the performance of the first execution (with and without prewarming) and subsequent executions.\n\nAWS | Azure | GCP",
    "pillar": "Performance efficiency"
  },
  "PE-03-03": {
    "identifier": "PE-03-03",
    "practice": "Identify bottlenecks",
    "capabilities": "nan",
    "details": "Bottlenecks are areas in your workload that might worsen the overall performance when the load in production increases. Identifying these at design time and testing against higher workloads will help to keep the workloads stable in production.",
    "pillar": "Performance efficiency"
  },
  "PE-04-01": {
    "identifier": "PE-04-01",
    "practice": "Monitor query performanance",
    "capabilities": "nan",
    "details": "Query Profile: Utilize the query profile feature to troubleshoot performance bottlenecks during a query's execution. It provides visualization of each query task and related metrics such as time spent, number of rows processed, and memory consumption\n\nSQL Warehouse Monitoring: Monitor SQL warehouses by viewing live statistics, peak query count charts, running clusters charts, and query history table",
    "pillar": "Performance efficiency"
  },
  "PE-04-02": {
    "identifier": "PE-04-02",
    "practice": "Monitor streaming workloads",
    "capabilities": "nan",
    "details": "Structured Streaming Monitoring: For streaming queries, use the built-in monitoring in the Spark UI under the Streaming tab or push metrics to external services using Apache Spark\u2019s Streaming Query Listener interfac",
    "pillar": "Performance efficiency"
  },
  "PE-04-03": {
    "identifier": "PE-04-03",
    "practice": "Monitor job performance",
    "capabilities": "nan",
    "details": "View and manage job runs through the Databricks UI, which provides details on job output, logs, metrics, and the success or failure of each task in the job run",
    "pillar": "Performance efficiency"
  }
}