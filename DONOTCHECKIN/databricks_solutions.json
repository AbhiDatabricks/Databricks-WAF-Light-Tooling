{
  "DG-01-01": {
    "identifier": "DG-01-01",
    "solution": "",
    "practice": "Establish data governance process",
    "pillar": "",
    "details": "Data governance is the management of the availability, usability, integrity, and security of an organization's data. By strengthening data governance, organizations can ensure the quality of data that is critical for accurate analysis and decision making, helping to identify new opportunities, improve customer satisfaction, and ultimately increase revenue. It helps organizations comply with data privacy regulations and improve security measures, reducing the risk of data breaches and penalties. \nEffective data governance also eliminates redundancies and streamlines data management, resulting in cost savings and increased operational efficiency. \nTopics to cover with data goevernance include\n- Data ownership, roles and responsibilities\n- Policies and procedures to guide data quality, privacy, security, and compliance\n- Data quality to ensure the accuracy, completeness, and reliability of data\n- Data security and compliance\n- Data architecture and integration\n- Metadata management to ensure that data is well understood\nUnity catalog is at the center of the Databricks Data Intelligence Platform and helps with many aspects of data governance from metadata management, lineage, to access control.\nAWS | AZURE | GCP"
  },
  "DG-01-02": {
    "identifier": "DG-01-02",
    "solution": "",
    "practice": "Manage metadata for all data assets in one place",
    "pillar": "",
    "details": "The benefits of managing all metadata in one place are similar to the benefits of maintaining a single source of truth for all your data. These include reduced data redundancy, increased data integrity, and avoiding misunderstandings based on different definitions or taxonomies. It's also easier to implement global policies, standards, and rules when dealing with one source.\n\nAs a best practice, run the lakehouse in a single account with one Unity Catalog. The top-level container of objects in Unity Catalog is a metastore. It stores data assets (such as tables and views) and the permissions that govern access to them. Use a single metastore per cloud region and do not access metastores across regions to avoid latency issues.\nAWS | Azure | GCP\n\nDatabricks recommends using catalogs to provide segregation across your organization\u2019s information architecture. Often this means that catalogs can correspond to software development environment scope, team, or business unit.\nAWS | Azure | GCP "
  },
  "DG-01-03": {
    "identifier": "DG-01-03",
    "solution": "",
    "practice": "Track data and AI lineage to drive visibility of the data",
    "pillar": "",
    "details": "Data lineage is a powerful tool that helps data leaders drive greater visibility and understanding of the data in their organizations. It describes the transformation and refinement of data from source to insight. Lineage includes the capture of all relevant metadata and events associated with the data in its lifecycle, including the source of the data set, what other data sets were used to create it, who created it and when, what transformations were performed, what other data sets use it, and many other events and attributes. Data lineage can be used for many data-related use cases:\n\n- Compliance and audit readiness: Data lineage helps organizations trace the source of tables and fields. This is important for meeting the requirements of many compliance regulations, such as General Data Protection Regulation (GDPR), California Consumer Privacy Act (CCPA), Health Insurance Portability and Accountability Act (HIPAA), Basel Committee on Banking Supervision (BCBS) 239, and Sarbanes-Oxley Act (SOX).\n\n- Impact analysis/change management: Data goes through multiple transformations from the source to the final business-ready table. Understanding the potential impact of data changes on downstream users becomes important from a risk-management perspective. This impact can be easily determined using the data lineage collected by Unity Catalog.\n\n- Data quality assurance: Understanding where a data set came from and what transformations have been applied provides much better context for data scientists and analysts, enabling them to gain better and more accurate insights.\n\n- Debugging and diagnostics: In the event of an unexpected result, data lineage helps data teams perform root cause analysis by tracing the error back to its source. This dramatically reduces debugging time.\n\nUnity Catalog captures runtime data lineage across queries run on Databricks. Lineage is supported for all languages and is captured down to the column level. Lineage data includes notebooks, workflows, and dashboards related to the query. Lineage can be visualized in Catalog Explorer in near real-time and retrieved with the Databricks Data Lineage REST API.\nAWS | Azure | GCP "
  },
  "DG-01-04": {
    "identifier": "DG-01-04",
    "solution": "",
    "practice": "Add consistent descriptions to your metadata",
    "pillar": "",
    "details": "Enriching metadata with comments can help accelerate processes...\n\nAI-generated comments are intended to provide a general description of tables and columns based on the schema. The descriptions are tuned for data in a business and enterprise context, using example schemas from several open datasets across various industries. The model was evaluated with hundreds of simulated samples to verify it avoids generating harmful or inappropriate descriptions.\nAWS | Azure | GCP"
  },
  "DG-01-05": {
    "identifier": "DG-01-05",
    "solution": "",
    "practice": "Allow easy data discovery for data consumers ",
    "pillar": "",
    "details": "Easy data discovery enables data scientists, data analysts, and data engineers to quickly discover and reference relevant data and accelerate time to value.\n\nDatabricks Catalog Explorer provides a UI to explore and manage data, schemas (databases), tables, and permissions, data owners, external locations, and credentials. Additionally, you can use the Insights tab in Catalog Explorer to view the most frequent recent queries and users of any table registered in Unity Catalog.\nAWS | Azure | GCP"
  },
  "DG-01-06": {
    "identifier": "DG-01-06",
    "solution": "",
    "practice": "Govern AI assets together with data",
    "pillar": "",
    "details": "Governing all your AI assets is important because it ensures unified visibility and control over data and AI assets, which is crucial for maintaining data quality, security, and compliance across different platforms and clouds. This governance facilitates the management of access policies consistently and efficiently, enhancing operational intelligence and ensuring that AI systems operate within regulatory guidelines and business requirements.\nWith Unity Catalog, organizations can implement a unified governance framework for their structured and unstructured data, machine learning models, notebooks, features, functions, and files, enhancing security and compliance across clouds and platforms.\nAWS | Azure | GCP\n\nModel aliases in machine learning workflows allow you to assign a mutable, named reference to a specific version of a registered model. This functionality is beneficial for tracking and managing different stages of a model\u2019s lifecycle, indicating the current deployment status of any given model version.\nAWS | Azure | GCP"
  },
  "DG-02-01": {
    "identifier": "DG-02-01",
    "solution": "",
    "practice": "Centralize access control for all data and AI assets",
    "pillar": "",
    "details": "Centralizing access control for all data assets is important because it simplifies the security and governance of your data and AI assets by providing a central place to administer and audit access to these assets. This approach helps in managing data and AI object access more efficiently, ensuring that operational requirements around segregation of duty are enforced, which is crucial for regulatory compliance and risk avoidance.\n\nDatabricks provides access to audit logs of activities performed by Databricks users, allowing your enterprise to monitor detailed Databricks usage patterns. There are two types of logs: Workspace-level audit logs with workspace-level events and account-level audit logs with account-level events.\nAWS | Azure | GCP\nAWS | Azure | GCP"
  },
  "DG-02-02": {
    "identifier": "DG-02-02",
    "solution": "",
    "practice": "Configure audit logging",
    "pillar": "",
    "details": "Audit logging is important for providing a detailed account of system activities (user actions, changes to settings, etc.) that could impact the system's integrity. Whereas standard system logs are designed to help developers troubleshoot errors, audit logs provide a historical record of activity for compliance purposes and other business policy enforcement. Maintaining robust audit logs can help identify and ensure preparedness in the face of threats, breaches, fraud, and other system issues.\n\nDatabricks provides access to audit logs of activities performed by Databricks users, allowing your enterprise to monitor detailed Databricks usage patterns. There are two types of logs: Workspace-level audit logs with workspace-level events and account-level audit logs with account-level events.\nAWS | Azure | GCP\nAWS | Azure | GCP"
  },
  "DG-02-03": {
    "identifier": "DG-02-03",
    "solution": "",
    "practice": "Audit data platform events",
    "pillar": "",
    "details": "Auditing data platform events is important because (see above)\n\nUnity Catalog captures an audit log of actions performed against the metastore. This enables admins to access fine-grained details about who accessed a given dataset and what actions they performed. \nAWS | Azure | GCP\nAWS | Azure | GCP\n\nFor secure sharing with Delta Sharing, Databricks provides audit logs to monitor Delta Sharing events, including:\n\n- When someone creates, modifies, updates, or deletes a share or a recipient.\n- When a recipient accesses an activation link and downloads the credential.\n- When a recipient accesses shares or data in shared tables.\n- When a recipient\u2019s credential is rotated or expires. \nAWS | Azure | GCP"
  },
  "DG-03-01": {
    "identifier": "DG-03-01",
    "solution": "",
    "practice": "Define and document data quality standards ",
    "pillar": "",
    "details": "Defining clear and actionable data quality standards is crucial, because it helps ensure that data used for analysis, reporting, and decision-making is reliable and trustworthy. Documenting these standards helps ensure that they are upheld.\n\nData quality standards should be based on the specific needs of the business and should address dimensions of data quality such as accuracy, completeness, consistency, timeliness, and reliability\n- Accuracy: Ensure data accurately reflects real-world values.\n- Completeness: All necessary data should be captured and no critical data should be missing.\n- Consistency: Data across all systems should be consistent and not contradict other data.\n- Timeliness: Data should be updated and available in a timely manner.\n- Reliability: Data should be sourced and processed in a way that ensures its dependability."
  },
  "DG-03-02": {
    "identifier": "DG-03-02",
    "solution": "",
    "practice": "Use data quality tools for profiling, cleansing, validating, and monitoring data",
    "pillar": "",
    "details": "Leverage data quality tools for profiling, cleansing, validating, and monitoring data. These tools help in automating the processes of detecting and correcting data quality issues, which is vital for scaling data quality initiatives across large datasets typical in data lakes.\n\nFor teams using DLT, you can use expectations to define data quality constraints on the contents of a dataset. Expectations allow you to guarantee data arriving in tables meets data quality requirements and provide insights into data quality for each pipeline update.\nAWS | Azure | GCP"
  },
  "DG-03-03": {
    "identifier": "DG-03-03",
    "solution": "",
    "practice": "Implement and enforce standardized data formats and definitions",
    "pillar": "",
    "details": "Standardized data formats and definitions helps achieve uniformity in data representation across all systems to facilitate easier data integration and analysis, reduce costs, and improve decision-making via enhance communication and collaboration between different teams and departments. It also helps provide structure for creating and maintaining data quality.\n\nDevelop and enforce a standard data dictionary that includes definitions, formats, and acceptable values for all data elements used across the organization.\n\nUse consistent naming conventions, date formats, and measurement units across all databases and applications to prevent discrepancies and confusion."
  },
  "IU-01-01": {
    "identifier": "IU-01-01",
    "solution": "",
    "practice": "Use standard and reusable integration patterns for external integration",
    "pillar": "",
    "details": "Integration standards are important because they provide guidelines for how data should be represented, exchanged, and processed across different systems and applications. These standards help ensure that data is compatible, high quality, and interoperable across various sources and destinations.\n\nThe Databricks Lakehouse comes with a comprehensive [REST API](/api/workspace/introduction) that allows you to manage nearly all aspects of the platform programmatically. The REST API server runs in the control plane and provides a unified endpoint to manage the <Databricks> platform. \n\nThe REST API provides the lowest level of integration that can always be used. However, the preferred way to integrate with <Databricks> is to use higher level abstractions such as the [Databricks SDKs](/dev-tools/index-sdk.md) or [CLI tools](/dev-tools/index-cli.md). CLI tools are shell-based and allow to easily integrate the Databricks platform into CI/CD and MLOps workflows\n\nAWS | Azure | GCP\nAWS | Azure | GCP"
  },
  "IU-01-02": {
    "identifier": "IU-01-02",
    "solution": "",
    "practice": "Use optimized connectors to ingest data sources into the lakehouse",
    "pillar": "",
    "details": "Databricks provides optimized connectors for stream messaging services such as Apache Kafka for near-real time data ingestion of data.\nDatabricks provides built-in integrations to many cloud-native data systems, as well as extensible JDBC support to connect to other data systems.\n\nOne option for integrating data sources without ETL is Lakehouse Federation. Lakehouse Federation is the query federation platform for Databricks. The term query federation describes a collection of features that allow users and systems to run queries against multiple data sources without having to migrate all the data into a unified system. Databricks uses Unity Catalog to manage query federation. Unity Catalog\u2019s data governance and data lineage tools ensure that data access is managed and audited for all federated queries run by users in your Databricks workspaces.\nNote\nAny query in the Databricks platform that uses a Lakehouse Federation source will be sent to that source. Make sure the source system can handle the load. Also be aware that if the source system is deployed in a different cloud region or cloud, there will be an egress cost for each query.\nConsider to offload access to underlying databases via materialized views to avoid high/concurrent loads on operational databases and reduce egress costs.\n\nAWS | Azure | GCP\nAWS | Azure | GCP"
  },
  "IU-01-03": {
    "identifier": "IU-01-03",
    "solution": "",
    "practice": "Use certified partner tools",
    "pillar": "",
    "details": "Using certified partner tools makes it easy to connect data between two systems. These integrations allow you to use the partner solutions while keeping you data unified in the Lakehouse.\n\nBusinesses have different needs, and no single tool can meet all of them. Partner Connect allows you to explore and easily integrate with our partners, which cover all aspects of the lakehouse: data ingestion, preparation and transformation, BI and visualization, machine learning, data quality, and so on. Partner Connect lets you create trial accounts with selected Databricks technology partners and connect your Databricks workspace to partner solutions from the Databricks UI. Try partner solutions using your data in the Databricks lakehouse, and then adopt the solutions that best meet your business needs. \n\nAWS | Azure | GCP"
  },
  "IU-01-04": {
    "identifier": "IU-01-04",
    "solution": "",
    "practice": "Reduce complexity of data engineering pipelines",
    "pillar": "",
    "details": "Investing in reducing the complexity of data engineering pipelines enables scalability, agility and flexibility to be able to expand and innovate faster. Simplified pipelines make it easier to manage and adapt all of the operational needs of a data engineering pipeline: task orchestration, cluster management, monitoring, data quality, and error handling.\n\nDelta Live Tables is a framework for building reliable, maintainable, and testable data processing pipelines. You define the transformations to perform on your data, and Delta Live Tables manages task orchestration, cluster management, monitoring, data quality, and error handling. See What is Delta Live Tables?.\n\nAuto Loader incrementally and efficiently processes new data files as they arrive in cloud storage. It can reliably read data files from cloud storage. An essential aspect of both Delta Live Tables and Auto Loader is their declarative nature: Without them, one has to build complex pipelines that integrate different cloud services - such as a notification service and a queuing service - to reliably read cloud files based on events and allow the combining of batch and streaming sources reliably.\n\nAuto Loader and Delta Live Tables reduce system dependencies and complexity and significantly improve the interoperability with the cloud storage and between different paradigms like batch and streaming. As a side effect, the simplicity of pipelines increases platform usability.\n\nAWS | Azure | GCP\nAWS | Azure | GCP"
  },
  "IU-02-01": {
    "identifier": "IU-02-01",
    "solution": "",
    "practice": "Use open data formats",
    "pillar": "",
    "details": "Using an open data format means there are no restrictions on its use. This is important because it removes barriers to accessing and using the data for analysis and driving business insights. Open formats, such as those built on Apache Spark, also add features that boost performance with support for ACID transactions, unified streaming, and batch data processing. Furthermore, open source is community-driven, meaning the community is constantly working on improving existing features and adding new ones, making it easier for users to get the most out of their projects.\n\nThe Delta Lake framework has many advantages, from reliability features to high-performance enhancements, and it is also a fully open data format. \nAdditionally, Delta Lake comes with a Delta Standalone library, which opens the Delta format for development projects. It is a single-node Java library that can read from and write to Delta tables. Dozens of third-party tools and applications support Delta Lake. Specifically, this library provides APIs to interact with table metadata in the transaction log, implementing the Delta Transaction Log Protocol to achieve the transactional guarantees of the Delta format.\nAWS | Azure | GCP\nAWS | Azure | GCP\n\nUniForm takes advantage of the fact that both Delta Lake and Iceberg consist of Parquet data files and a metadata layer. UniForm automatically generates Iceberg metadata asynchronously, allowing Iceberg clients to read Delta tables as if they were Iceberg tables. You can expect negligible Delta write overhead when UniForm is enabled, as the Iceberg conversion and transaction occurs asynchronously after the Delta commit.\n\nA single copy of the data files provides access to both format clients.\nAWS | Azure | GCP"
  },
  "IU-02-02": {
    "identifier": "IU-02-02",
    "solution": "",
    "practice": "Enable secure data sharing for all data and AI assets ",
    "pillar": "",
    "details": "Sharing data can lead to better collaboration and decision-making. However, when sharing data, it's important to maintain control over the data use, ensure confidentiality, and establish guidelines for data collection, storage, processing, and sharing - in order to protect your data and remain in ensure compliance with relevant laws and regulations around data sharing.\n\nDelta Sharing provides an open solution for securely sharing live data from your lakehouse to any computing platform. Recipients do not need to be on the Databricks platform, on the same cloud, or on any cloud at all. Delta Sharing is natively integrated with Unity Catalog, enabling organizations to centrally manage and audit shared data across the enterprise and confidently share data assets while meeting security and compliance requirements.\n\nData providers can share live data from where it resides in their cloud storage without replicating or moving it to another system. This approach reduces the operational costs of data sharing because data providers don't have to replicate data multiple times across clouds, geographies, or data platforms to each of their data consumer \nAWS | Azure | GCP\n\nIf you want to share data with users who don't have access to your Unity Catalog metastore, you can use Databricks-to-Databricks Delta Sharing, as long as the recipients have access to a Databricks workspace that is enabled for Unity Catalog. Databricks-to-Databricks sharing lets you share data with users in other Databricks accounts, across cloud regions, across cloud providers. It\u2019s a great way to securely share data across different Unity Catalog metastores in your own Databricks account. \nAWS | Azure | GCP"
  },
  "IU-02-03": {
    "identifier": "IU-02-03",
    "solution": "",
    "practice": "Use open standards for your AI workflows",
    "pillar": "",
    "details": "Like using an open source data format, using open standards for your AI workflows has similar benefits of flexibility, agility, cost, and security.\n\nMLflow is an open source platform for managing the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry. Using MLflow on Databricks provides both advantages: You can write your ML workflow using an open and portable tool and use reliable services operated by Databricks (Tracking Server, Model Registry). It also adds enterprise-grade, scalable model serving, allowing you to host MLflow models as REST endpoints. \n\nAWS | Azure | GCP"
  },
  "IU-03-01": {
    "identifier": "IU-03-01",
    "solution": "",
    "practice": "Provide a self-service experience across the platform",
    "pillar": "",
    "details": "There are several benefits of a platform where users have autonomy to use the tools and capabilities depending on their needs. Investing in creating a self-service platform makes it easy to scale to serve more users and drives greater efficiency by minimizing the need for human involvement to provision users, resolve issues, and process access requests.\n\nThe Databricks Data Intelligence Platform has all the capabilities required to provide a self-service experience. While there might be a mandatory approval step, the best practice is to fully automate the setup when a business unit requests access to the lakehouse. Automatically provision their new environment, sync users and use SSO for authentication, provide access control to common data and separate object storages for their own data, and so on. Together with a central data catalog containing semantically consistent and business-ready data sets, this quickly and securely provides access for new business units to the Lakehouse capabilities and the data they need. \n\nBlog"
  },
  "IU-03-02": {
    "identifier": "IU-03-02",
    "solution": "",
    "practice": "Use serverless services",
    "pillar": "",
    "details": "Using serverless has multiple benefits beyond just cost savings, scalability, and performance. Development is faster when developers can focus on code instead of infrastructure. They also have greater flexibility in how they design their applications.\n\nFor serverless compute on the Databricks platform, the compute layer runs in the customer\u2019s Databricks account. Cloud administrators no longer have to manage complex cloud environments that involve adjusting quotas, creating and maintaining networking assets, and joining billing sources. Users benefit from near-zero waiting times for cluster start and improved concurrency on their queries. \n\nAWS | Azure | GCP"
  },
  "IU-03-03": {
    "identifier": "IU-03-03",
    "solution": "",
    "practice": "Use pre-defined compute templates ",
    "pillar": "",
    "details": "Pre-defined compute templates can help bring some of the benefits of serverless when it's not available for a particular tool or in your region. Mainly, these templates remove the need for developers to provision their own compute and determine infrastructure needs, thus speeding up time to development.\n\nRemove the burden of defining a cluster (VM type, node size, and cluster size) from end users. This can be achieved in the following ways:\n\nProvide shared clusters as immediate environments for users. On these clusters, use auto scaling down to a very minimum of nodes to avoid high idle costs.\n\nUse cluster policies to define t-shirt-sized clusters (S, M, L) for projects as a standardized work environment.\nBlog\nAWS | Azure | GCP"
  },
  "IU-03-04": {
    "identifier": "IU-03-04",
    "solution": "",
    "practice": "Use AI capabilities to increase productivity ",
    "pillar": "",
    "details": "In addition to increasing productivity, AI tools can also help identify patterns in errors and provide additional insights based on the input. Overall, incorporating these tools into the development process can greatly reduce errors and facilitate decision-making - leading to faster time to release.\n\nDatabricks Assistant lets you query data through a conversational interface, making you more productive inside Databricks. Describe your task in English and let the Assistant generate SQL queries, explain complex code and automatically fix errors. The Assistant leverages Unity Catalog metadata to understand your tables, columns, descriptions and popular data assets across your company to provide responses that are personalized to you. \n\nAWS | Azure | GCP"
  },
  "IU-04-01": {
    "identifier": "IU-04-01",
    "solution": "",
    "practice": "Offer reusable data-as-products that the business can trust",
    "pillar": "",
    "details": "Data-as-products is a data strategy that can help organizations harness the full potential of their data assets. The raw data into a structured, accessible and valuable product. \n\nProducing high-quality data-as-product is a primary goal of any data platform. The idea is that data engineering teams apply product thinking to the curated data: The data assets are their products, and the data scientists, ML and BI engineers, or any other business teams that consume data are their customers. These customers should be able to discover, address, and create value from these data-as-products through a self-service experience without the intervention of the specialized data teams. \n\neBook"
  },
  "IU-04-02": {
    "identifier": "IU-04-02",
    "solution": "",
    "practice": "Publish data products semantically consistent across the enterprise",
    "pillar": "",
    "details": "A data lake usually contains data from different source systems. These systems sometimes name the same concept differently (such as customer vs. account) or mean different concepts by the same identifier. For business users to easily combine these data sets in a meaningful way, the data must be made homogeneous across all sources to be semantically consistent. In addition, for some data to be valuable for analysis, internal business rules must be applied correctly, such as revenue recognition. To ensure that all users are using the correctly interpreted data, data sets with these rules must be made available and published to Unity Catalog. Access to source data must be limited to teams that understand the correct usage. \n\neBook"
  },
  "IU-04-03": {
    "identifier": "IU-04-03",
    "solution": "",
    "practice": "Provide a central catalog for discovery and lineage",
    "pillar": "",
    "details": "A central catalog for discovery and lineage helps data consumers access data from multiple sources across the enterprise, thus reducing operational overhead for the central governance team.\n\nIn Unity Catalog, administrators and data stewards manage users and their access to data centrally across all workspaces in a Databricks account. Users in different workspaces can share the same data and, depending on user privileges granted centrally in Unity Catalog, can access data together.\n\nAWS | Azure | GCP"
  },
  "OE-01-01": {
    "identifier": "OE-01-01",
    "solution": "",
    "practice": "Create a dedicated Lakehouse operations team",
    "pillar": "",
    "details": "It is a general best practice to have a platform operations team to enable data teams to work on one or more data platforms. This team is responsible for coming up with blueprints and best practices internally. They provide tooling - for example, for infrastructure automation and self-service access - and ensure that security and compliance needs are met. This way, the burden of securing platform data is on a central team, so that distributed teams can focus on working with data and producing new insights. "
  },
  "OE-01-02": {
    "identifier": "OE-01-02",
    "solution": "",
    "practice": "Use enterprise source code management (SCM) ",
    "pillar": "",
    "details": "Source code management (SCM) helps developers work more effectively, which can lead to faster release velocity and lower development costs. Having a tool that helps track changes, maintain code integrity, detect bugs, and roll back to previous versions if need be is an important component of your overall solution architecture.\n\nDatabricks Git folders allow users to store notebooks or other files in a Git repository, providing features like cloning a repository, committing and pushing, pulling, branch management and viewing file diffs. Use Git folders for better code visibility and tracking. \n\nAWS | Azure | GCP"
  },
  "OE-01-03": {
    "identifier": "OE-01-03",
    "solution": "",
    "practice": "Standardize DevOps processes (CI/CD)",
    "pillar": "",
    "details": "Continuous integration and continuous delivery (CI/CD) refer to developing and delivering software in short, frequent cycles using automation pipelines. While this is by no means a new process, having been ubiquitous in traditional software engineering for decades, it is becoming an increasingly necessary process for data engineering and data science teams. For data products to be valuable, they must be delivered in a timely way. Additionally, consumers must have confidence in the validity of outcomes within these products. By automating the building, testing, and deployment of code, development teams can deliver releases more frequently and reliably than manual processes still prevalent across many data engineering and data science teams.\n\nFor more information about best practices for code development using Databricks Git folders, see CI/CD techniques with Git and Databricks Git folders (Repos). This, together with the Databricks REST API, allows you to build automated deployment processes with GitHub Actions, Azure DevOps pipelines, or Jenkins jobs. \n\nAWS | Azure | GCP\nAWS | Azure | GCP"
  },
  "OE-01-04": {
    "identifier": "OE-01-04",
    "solution": "",
    "practice": "Standardize MLOps processes across enterprise",
    "pillar": "",
    "details": "MLOps processes provide reproducibility of ML pipelines, enabling more tightly-coupled collaboration across data teams, reducing conflict with devops and IT, and accelerating release velocity. As many models are used to drive key business decisions, standardizing MLops processes ensure that models are developed, tested, and deployed in a consistent and reliable way.\n\nBuilding and deploying ML models is complex. There are many options available to achieve this, but little in the way of well-defined standards. As a result, over the past few years, we have seen the emergence of machine learning operations (MLOps). MLOps is a set of processes and automation for managing models, data, and code to improve performance stability and long-term efficiency in ML systems. It covers data preparation, exploratory data analysis (EDA), feature engineering, model training, model validation, deployment, and monitoring. \n\nAWS | Azure | GCP\nAWS | Azure | GCP\neBook"
  },
  "OE-01-05": {
    "identifier": "OE-01-05",
    "solution": "",
    "practice": "Define environment isolation strategy",
    "pillar": "",
    "details": "Organizing workspaces in Databricks offers several benefits that enhance the efficiency and security of data operations within an enterprise. Key advantages include:\n\n1. Structured Environment: Implementing structured environments such as development, staging, and production workspaces helps streamline processes and maintain consistency across different stages of project development\n\n2. Isolation and Security: Workspaces can be isolated by line of business or data product, which not only improves governance by clearly dividing users and roles but also enhances security by restricting access based on specific business needs or data characteristics\n\n3. Flexibility and Collaboration: Data product-based isolation allows for more flexibility and collaborative opportunities within shared development environments and sandbox workspaces, fostering innovation without risking the main workflow\n\n4. Disaster Recovery: Organized workspaces facilitate the implementation of disaster recovery plans and regional backups, ensuring data integrity and availability in case of incidents[1].\n\n5. Governance and Compliance: Centralized governance through a Center of Excellence (COE) and the use of Databricks' governance features help minimize risks and ensure compliance with regulatory requirements\n\n6. Automation and Efficiency: Automating cloud processes, including infrastructure management, CI/CD, backup, and monitoring, reduces manual overhead and increases operational efficiency\n\nThese benefits collectively support a robust, secure, and efficient framework for managing large-scale data operations, making Databricks an effective tool for enterprise data management and analysis\nBlog"
  },
  "OE-01-06": {
    "identifier": "OE-01-06",
    "solution": "",
    "practice": "Streamline the usage and management of various\nlarge language model (LLM) providers",
    "pillar": "",
    "details": "External models are third-party models hosted outside of Databricks. Supported by Model Serving AI Gateway, Databricks external models via the AI Gateway allow you to streamline the usage and management of various large language model (LLM) providers, such as OpenAI and Anthropic, within an organization. You can also use Mosaic AI Model Serving as a provider to serve predictive ML models, which offers rate limits for those endpoints. As part of this support, Model Serving offers a high-level interface that simplifies the interaction with these services by providing a unified endpoint to handle specific LLM-related requests. In addition, Databricks support for external models provides centralized credential management. By storing API keys in one secure location, organizations can enhance their security posture by minimizing the exposure of sensitive API keys throughout the system. It also helps to prevent exposing these keys within code or requiring end users to manage keys safely.\n\nAWS | Azure | GCP"
  },
  "OE-01-07": {
    "identifier": "OE-01-07",
    "solution": "",
    "practice": "Define catalog strategy for your enterprise",
    "pillar": "",
    "details": ""
  },
  "OE-01-08": {
    "identifier": "OE-01-08",
    "solution": "",
    "practice": "Compare LLM outputs on set prompts",
    "pillar": "",
    "details": "New, no-code visual tools allow users to compare models\u2019 output based on set prompts, which are automatically tracked within MLflow. With integration into Mosaic AI Model Serving, customers can deploy the best model to production. The AI Playground is a chat-like environment where you can test, prompt and compare LLMs.\n\nAWS | Azure | GCP"
  },
  "OE-01-09": {
    "identifier": "OE-01-09",
    "solution": "",
    "practice": "Build models with all representative, accurate and relevant data sources",
    "pillar": "",
    "details": "Harnessing internal data and intellectual\nproperty to customize large AI models can offer\na significant competitive edge. However, this\nprocess can be complex, involving coordination\nacross various parts of the organization. The Data\nIntelligence Platform addresses this challenge\nby integrating data across traditionally isolated\ndepartments and systems. This integration\nfacilitates a more cohesive data and AI strategy,\nenabling the effective training, testing and\nevaluation of models using a comprehensive\ndataset. Use caution when preparing data for\ntraditional models and GenAI training to ensure\nthat you are not unintentionally including data\nthat causes legal conflicts, such as copyright\nviolations, privacy violations or HIPAA violations."
  },
  "OE-02-01": {
    "identifier": "OE-02-01",
    "solution": "",
    "practice": "Use Infrastructure as Code for deployments and maintenance",
    "pillar": "",
    "details": "Infrastructure as Code allows developers and operations teams to automatically manage, monitor, and provision resources, instead of manually configuring hardware devices, operating systems, applications, and services.\n\nHashiCorp Terraform is a popular open source tool for creating safe and predictable cloud infrastructure across several cloud providers. The Databricks Terraform provider manages Databricks workspaces and the associated cloud infrastructure using a flexible, powerful tool. The goal of the Databricks Terraform provider is to support all Databricks REST APIs, supporting automation of the most complicated aspects of deploying and managing your data platforms. The Databricks Terraform provider is the recommended tool to deploy and manage clusters and jobs reliably, provision Databricks workspaces, and configure data access. \nAWS | Azure | GCP"
  },
  "OE-02-02": {
    "identifier": "OE-02-02",
    "solution": "",
    "practice": "Standardize compute configurations",
    "pillar": "",
    "details": "Pre-defined compute templates can help bring some of the benefits of serverless when it's not available. Mainly, these templates remove the need for developers to provision their own compute and determine infrastructure needs, thus speeding up time to development.\n\nUse cluster policies to define t-shirt-sized clusters (S, M, L) for projects as a standardized work environment.\nBlog\nAWS | Azure | GCP"
  },
  "OE-02-03": {
    "identifier": "OE-02-03",
    "solution": "",
    "practice": "Use automated workflows for jobs",
    "pillar": "",
    "details": "Setting up automated workflows for jobs can help reduce unnecessary manual tasks and improve productivity through the DevOps process of creating and deploying jobs.\n\nWe recommend using workflows with jobs to schedule data processing and data analysis tasks on Databricks clusters with scalable resources. Jobs can consist of a single task or a large, multitask workflow with complex dependencies. Databricks manages task orchestration, cluster management, monitoring, and error reporting for all your jobs. You can run your jobs immediately or periodically through an easy-to-use scheduling system. You can implement job tasks using notebooks, JARS, Delta Live Tables pipelines, or Python, Scala, Spark submit, and Java applications. See Introduction to Databricks Workflows. \nAWS | Azure | GCP\n\nThe comprehensive Databricks REST API is used by external orchestrators to orchestrate Databricks assets, notebooks, and jobs. \nAWS | Azure | GCP"
  },
  "OE-02-04": {
    "identifier": "OE-02-04",
    "solution": "",
    "practice": "Use automated and event driven file ingestion ",
    "pillar": "",
    "details": "Event-driven (vs. schedule-driven) file ingestion has several benefits, including efficiency, increased data freshness, and real-time data ingestion. Running a job only when an event occurs ensures you're not wasting resources, thus saving costs. Event-driven architectures can also incorporate security measures like role-based access control and audit logging, thus improving overall security posture and protecting data.\n\nAuto Loader incrementally and efficiently processes new data files as they arrive in cloud storage. It can ingest many file formats like JSON, CSV, PARQUET, AVRO, ORC, TEXT, and BINARYFILE. With an input folder on the cloud storage, Auto Loader automatically processes new files as they arrive.\nFor one-off ingestions, consider using the command COPY INTO instead. \n\nAWS | Azure | GCP\nAWS | Azure | GCP"
  },
  "OE-02-05": {
    "identifier": "OE-02-05",
    "solution": "",
    "practice": "Use ETL frameworks for data pipelines",
    "pillar": "",
    "details": "Using ETL frameworks helps improve data quality, saves time, and makes it easier to analyze larger datasets. They also help eliminate data errors, bottlenecks, and latency.\n\nDelta Live Tables is a declarative framework for building reliable, maintainable, and testable data processing pipelines. You define the transformations to perform on your data and Delta Live Tables manages task orchestration, cluster management, monitoring, data quality, and error handling.\n\nWith Delta Live Tables, easily define end-to-end data pipelines in SQL or Python: Specify the data source, the transformation logic, and the destination state of the data. Delta Live Tables maintains dependencies and automatically determines the infrastructure to run the job in.\n\nTo manage data quality, Delta Live Tables monitors data quality trends over time, preventing bad data from flowing into tables through validation and integrity checks with predefined error policies.\n\nAWS | Azure | GCP"
  },
  "OE-02-06": {
    "identifier": "OE-02-06",
    "solution": "",
    "practice": "Follow the deploy-code approach for ML workloads",
    "pillar": "",
    "details": "Deploying models as code is taking a DevOps approach to machine learning. This approach has several benefits. Models are versioned along with other code, releases are deployed as a unit, and deployments are reproducible (fallback is possible).\n\nThe deploy-code approach to model deployment involves developing and testing training code and ancillary code in a training environment, followed by promotion to a staging environment for further testing on a subset of data. Once validated, the code is promoted to a production environment for final training and testing on the full dataset. The model and ancillary pipelines are then deployed. \n\nAWS | Azure | GCP\nAWS | Azure | GCP\neBook"
  },
  "OE-02-07": {
    "identifier": "OE-02-07",
    "solution": "",
    "practice": "Use a model registry to decouple code and model lifecycle",
    "pillar": "",
    "details": "Since model lifecycles do not correspond one-to-one with code lifecycles, it makes sense for model management to have its own service. \n\nUnity Catalog in Databricks offers a comprehensive solution for managing the lifecycle of machine learning (ML) models, providing features such as centralized access control, versioning, deployment, and monitoring. By enabling Unity Catalog in your workspace, you can train, register, and manage different versions of ML models using MLflow APIs, with support for aliases to facilitate model promotion and rollback. \n\nThe catalog ensures robust governance with account-level permissions and integrates seamlessly with other tools and systems. It also supports model serving at scale and provides detailed lineage and real-time performance monitoring, ensuring that models remain effective and compliant throughout their lifecycle.\n\nAWS | Azure | GCP"
  },
  "OE-02-08": {
    "identifier": "OE-02-08",
    "solution": "",
    "practice": "Automate ML experiment tracking",
    "pillar": "",
    "details": "Tracking ML experiments is the process of saving relevant metadata for each experiment and organizing the experiments. This metadata includes experiment inputs/outputs, parameters, models, and other artifacts. The goal of experiment tracking is to create reproducible results across every stage of the ML model development process. Automating this process makes scaling the number of experiments easier, and ensures cosistency in the metadata captured across all experiments.\n\nDatabricks Autologging is a no-code solution that extends MLflow automatic logging to deliver automatic experiment tracking for machine learning training sessions on Databricks. Databricks Auto Logging automatically captures model parameters, metrics, files and lineage information when you train models with training runs recorded as MLflow tracking runs. \n\nAWS | Azure | GCP"
  },
  "OE-02-09": {
    "identifier": "OE-02-09",
    "solution": "",
    "practice": "Reuse the same infrastructure to manage ML pipelines",
    "pillar": "",
    "details": "The data use for ML pipelines comes from the same sources as data used for other data pipelines. In addition, ML and data pipelines are similar in that they both should be monitored regularly and involve preparation of data for analysis by business users or model training. They both also need to be scalable and secure. In both cases, the infrastructure used should support these activities.\n\nML pipelines should be automated using many of the same techniques as other data pipelines. Use Databricks Terraform provider to automate deployment. ML requires deploying infrastructure such as inference jobs, serving endpoints, and featurization jobs. All ML pipelines can be automated as Workflows with Jobs, and many data-centric ML pipelines can use the more specialized Auto Loader to ingest images and other data and Delta Live Tables to compute features or to monitor metrics. \n\nTerraform docs"
  },
  "OE-02-10": {
    "identifier": "OE-02-10",
    "solution": "",
    "practice": "Utilize declarative management for complex data and ML pipelines",
    "pillar": "",
    "details": "Declarative frameworks within MLOps, allow teams to define desired outcomes in high-level terms, letting the system handle the specifics of execution, thereby simplifying the deployment and scaling of ML models. These frameworks support continuous integration and delivery, automate testing and infrastructure management, and ensure model governance and compliance, ultimately speeding up time to market and enhancing productivity across the ML lifecycle.\n\nDatabricks Asset Bundles (DABs) are a new tool for streamlining the development of complex data, analytics, and ML projects for the Databricks platform. Bundles make it easy to manage complex projects during active development by providing CI/CD capabilities in your software development workflow with a single concise and declarative YAML syntax. By using bundles to automate your project\u2019s tests, deployments, and configuration management you can reduce errors while promoting software best practices across your organization as templated projects. \n\nAWS | Azure | GCP"
  },
  "OE-02-11": {
    "identifier": "OE-02-11",
    "solution": "",
    "practice": "Automate LLM evaluation",
    "pillar": "",
    "details": "The \u201cLLM-as-a-judge\u201d feature in MLflow 2.8 automates LLM evaluation, offering a practical alternative to human judgment. It\u2019s designed to be efficient and cost-effective, maintaining consistency with human scores. This tool supports various metrics, including standard and customizable GenAI metrics, and allows users to select an LLM as a judge and define specific grading criteria.\n\nAWS | Azure | GCP"
  },
  "OE-03-01": {
    "identifier": "OE-03-01",
    "solution": "",
    "practice": "Establish monitoring processes",
    "pillar": "",
    "details": "Monitoring provides important information about jobs and data that can help drive decision-making when a job fails or data is missing - as well as alerts when these events take place, so you can take timely action.\n\nYou can track metrics such as the number of failed jobs, tables that are not receiving data, the latest ingested timestamp, table ingest rate, and the runtime of queries"
  },
  "OE-03-02": {
    "identifier": "OE-03-02",
    "solution": "",
    "practice": "Use native and external tools for platform monitoring",
    "pillar": "",
    "details": "Using native and external tools for monitoring provides information about your data and workloads that may help identify a potential problem before it occurs.\n\nIntegration with Cloud Monitoring Services: \n- For Databricks on AWS, you can integrate with Amazon CloudWatch to monitor your Databricks environment. CloudWatch allows you to derive metrics from logs and set up alerts.\n- Similarly, for Azure Databricks, you can use Azure Monitor to send monitoring data from Databricks and set up advanced monitoring and alerting capabilities.\n\nDatabricks Lakehouse Monitoring: This feature lets you monitor the statistical properties and quality of the data in all the tables in your account. It also allows you to track the performance of machine learning models by monitoring inference tables that contain model inputs and predictions\nAWS | Azure | GCP\n\nDatabricks SQL alerts can monitor the metrics table for security-based conditions, ensuring data integrity and timely response to potential issues:\n- Statistic range Alert: Triggers when a specific statistic, such as the fraction of missing values, exceeds a predetermined threshold\n- Data distribution shift alert: Activates upon shifts in data distribution, as indicated by the drift metrics table\n- Baseline divergence alert: Alerts if data significantly diverges from a baseline, suggesting potential needs for data analysis or model retraining, particularly in InferenceLog analysis\nAWS | Azure | GCP\n\nDelta Live Tables pipeline monitoring: Use built-in features in Delta Live Tables for monitoring and observability for pipelines, including data lineage, update history, and data quality reporting.\nAWS | Azure | GCP"
  },
  "OE-03-03": {
    "identifier": "OE-03-03",
    "solution": "",
    "practice": "Establish an incident response strategy",
    "pillar": "",
    "details": "It is critical to have an incident response strategy to minimize the risk of business disruption and financial, operational, and reputational damage to your organization.\n\nThis includes understanding the process for submitting.a support ticket with Databricks and the esclation process as well as having it documented for your broader organization to reference.\n\nDatabricks regularly provides previews to give you a chance to evaluate and provide feedback on features before they\u2019re generally available (GA). Previews come in various degrees of maturity with different support options based on release type. This documentation outlines the support details for each release type.\n\nAWS | Azure | GCP"
  },
  "OE-03-04": {
    "identifier": "OE-03-04",
    "solution": "",
    "practice": "Triggering actions in response to a specific event",
    "pillar": "",
    "details": "Triggering actions in response to specific events helps automate processes and minimize the need to manually manage models and workflows.\n\nWebhooks in the MLflow Model Registry enable you to automate machine learning workflow by\ntriggering actions in response to specific events. These webhooks facilitate seamless integrations, allowing for the automatic execution of various processes. For example, webhooks are used for: \n- CI workflow trigger: Validate your model automatically when creating a new version\n- Team notifications: Send alerts through a messaging app when a model stage transition request is received\n- Model fairness evaluation: Invoke a workflow to assess model fairness and bias upon a production transition request\n- Automated deployment: Trigger a deployment pipeline when a new tag is created on a model\n\nAWS | Azure | GCP"
  },
  "OE-04-01": {
    "identifier": "OE-04-01",
    "solution": "",
    "practice": "Manage service limits and quotas",
    "pillar": "",
    "details": "Manage service limits and quotas is important for maintaining a well-functioning infrastructure and preventing unexpected costs. They prevent accidental provisioning of more resources than needed and limit API requests. As a result, they also protect against potential security incidents that may increase bills.\n\nEvery service launched on a cloud will have to take limits into account, such as access rate limits, number of instances, number of users, and memory requirements. For your cloud provider, check the cloud limits. Before designing a solution, these limits must be understood. \nAWS | Azure | GCP\nAWS | Azure | GCP\n\n\"Databricks platform limits: These are specific limits for Databricks resources. The limits for the overall platform are documented in Limits.\nAWS | Azure | GCP\n\nUnity Catalog limits: Unity Catalog Resource Quotas\nAWS | Azure | GCP"
  },
  "OE-04-02": {
    "identifier": "OE-04-02",
    "solution": "",
    "practice": "Invest in capacity planning",
    "pillar": "",
    "details": "Capacity planning involves managing cloud resources, such as storage, compute, and networking to maintain performance while optimizing costs.\n\nUnderstanding and planning for high priority (volume) events is important. If the provisioned cloud resources are not sufficients and workloads can't scale, such increases in volume will cause an outage. \n\nPlan for fluctuation in the expected load that can occur for several reasons like sudden business changes or even world events. Test variations of load, including unexpected ones, to ensure that your workloads can scale. Ensure all regions can adequately scale to support the total load if a region fails. To be taken into consideration:\n\n- Technology and service limits and limitations of the cloud. \n- SLAs when determining the services to use in the design.\n- Cost analysis to determine how much improvement will be realized in the application if costs are increased. "
  },
  "SCP-01-01": {
    "identifier": "SCP-01-01",
    "solution": "",
    "practice": "Authenticate via single sign-on.",
    "pillar": "",
    "details": "Single sign-on enables you to authenticate your users using your organization\u2019s identity provider. Databricks recommends configuring SSO for greater security and improved usability. Once SSO is configured, you can enable fine-grained access control, such as multi-factor authentication, via your identity provider. Unified login allows you to manage one SSO configuration in your account that is used for the account and Databricks workspaces. If your account was created before June 21, 2023, you can also manage SSO individually on your account and workspaces. See Set up SSO in your Databricks account console and Set up SSO for your workspace. \n\nAWS | Azure | GCP"
  },
  "SCP-01-02": {
    "identifier": "SCP-01-02",
    "solution": "",
    "practice": "Use multifactor authentication.",
    "pillar": "",
    "details": "Use MFA with your identity provider and use SSO with databricks."
  },
  "SCP-01-03": {
    "identifier": "SCP-01-03",
    "solution": "",
    "practice": "Disable local passwords.",
    "pillar": "",
    "details": "Use SSO with databricks, to ensure no local password are used with databricks. for Admins Use a Password policy.\nDatabricks allows you to set a password policy for all users, including the Account Owner. You can set password length, complexity requirements, and expiration rules to ensure that passwords are secure and regularly updated."
  },
  "SCP-01-04": {
    "identifier": "SCP-01-04",
    "solution": "",
    "practice": "Set complex local passwords.",
    "pillar": "",
    "details": "Password policy: Databricks allows you to set a password policy for all users, including the Account Owner. You can set password length, complexity requirements, and expiration rules to ensure that passwords are secure and regularly updated."
  },
  "SCP-01-05": {
    "identifier": "SCP-01-05",
    "solution": "",
    "practice": "Separate admin accounts from normal user accounts.",
    "pillar": "",
    "details": "There are two main levels of admin privileges available on the Databricks platform:\n\nAccount admins: Manage the Databricks account, including workspace creation, user management, cloud resources, and account usage monitoring.\nAWS | Azure | GCP\n\nWorkspace admins: Manage workspace identities, access control, settings, and features for individual workspaces in the account.\nAWS | Azure | GCP"
  },
  "SCP-01-06": {
    "identifier": "SCP-01-06",
    "solution": "",
    "practice": "Use token management.",
    "pillar": "",
    "details": "When personal access tokens are enabled on a workspace, users with the CAN USE permission can generate personal access tokens to access Databricks REST APIs, and they can generate these tokens with any expiration date they like, including an indefinite lifetime. By default, no non-admin workspace users have the CAN USE permission, meaning that they cannot create or use personal access tokens.\n\nAs a Databricks workspace admin, you can disable personal access tokens for a workspace, monitor and revoke tokens, control which non-admin users can create tokens and use tokens, and set a maximum lifetime for new tokens.\n\nAWS | Azure | GCP"
  },
  "SCP-01-07": {
    "identifier": "SCP-01-07",
    "solution": "",
    "practice": "SCIM synchronization of users and groups.",
    "pillar": "",
    "details": "SCIM lets you use an identity provider (IdP) to create users in Databricks, give them the proper level of access, and remove access (deprovision them) when they leave your organization or no longer need access to Databricks.\n\nYou can use a SCIM provisioning connector in your IdP or invoke the Identity and Access Management SCIM APIs to manage provisioning. You can also use these APIs to manage \nidentities in Databricks directly, without an IdP.\n\nAWS | Azure | GCP"
  },
  "SCP-01-08": {
    "identifier": "SCP-01-08",
    "solution": "",
    "practice": "Limit cluster creation rights.",
    "pillar": "",
    "details": "A policy is a tool workspace admins can use to limit a user or group\u2019s compute creation permissions based on a set of policy rules.\nPolicies provide the following benefits:\n\nLimit users to creating clusters with prescribed settings.\nLimit users to creating a certain number of clusters.\nSimplify the user interface and enable more users to create their own clusters (by fixing and hiding some values).\nControl cost by limiting per cluster maximum cost (by setting limits on attributes whose values contribute to hourly price).\nEnforce cluster-scoped library installations (Public Preview).\n\nAWS | Azure | GCP"
  },
  "SCP-01-09": {
    "identifier": "SCP-01-09",
    "solution": "",
    "practice": "Store and use secrets securely.",
    "pillar": "",
    "details": "https://docs.databricks.com/en/security/secrets/index.html"
  },
  "SCP-01-10": {
    "identifier": "SCP-01-10",
    "solution": "",
    "practice": "Cross-account IAM role configuration.",
    "pillar": "",
    "details": "https://docs.databricks.com/en/administration-guide/cloud-configurations/aws/permissions.html"
  },
  "SCP-01-11": {
    "identifier": "SCP-01-11",
    "solution": "",
    "practice": "Customer-approved workspace login.",
    "pillar": "",
    "details": "Single sign-on enables you to authenticate your users using your organization\u2019s identity provider. Databricks recommends configuring SSO for greater security and improved usability. Once SSO is configured, you can enable fine-grained access control, such as multi-factor authentication, via your identity provider. Unified login allows you to manage one SSO configuration in your account that is used for the account and Databricks workspaces. If your account was created before June 21, 2023, you can also manage SSO individually on your account and workspaces. See Set up SSO in your Databricks account console and Set up SSO for your workspace. \n\nAWS | Azure | GCP"
  },
  "SCP-01-12": {
    "identifier": "SCP-01-12",
    "solution": "",
    "practice": "Use clusters that support user isolation.",
    "pillar": "",
    "details": "Access mode is a security feature that determines who can use the compute and what data they can access via the compute. Every compute in Databricks has an access mode.\n\nDatabricks recommends that you use shared access mode for all workloads. Only use the single user access mode if your required functionality is not supported by shared access mode. \n\nAWS | Azure | GCP\nBlog\nAWS | Azure | GCP"
  },
  "SCP-01-13": {
    "identifier": "SCP-01-13",
    "solution": "",
    "practice": "Use service principals to run production jobs.",
    "pillar": "",
    "details": "A service principal is an identity that you create in Databricks for use with automated tools, jobs, and applications. Service principals give automated tools and scripts API-only access to Databricks resources, providing greater security than using users or groups.\n\nYou can grant and restrict a service principal\u2019s access to resources in the same way as you can a Databricks user. For example, you can do the following:\n\nAWS | Azure | GCP"
  },
  "SCP-02-01": {
    "identifier": "SCP-02-01",
    "solution": "",
    "practice": "Avoid storing production data in DBFS.",
    "pillar": "",
    "details": "Because the DBFS root is accessible to all users in a workspace, all users can access any data stored here. It is important to instruct users to avoid using this location for storing sensitive data. The default location for managed tables in the Hive metastore on Databricks is the DBFS root; to prevent end users who create managed tables from writing to the DBFS root, declare a location on external storage when creating databases in the Hive metastore.\n\nAWS | Azure | GCP"
  },
  "SCP-02-02": {
    "identifier": "SCP-02-02",
    "solution": "",
    "practice": "Secure access to cloud storage.",
    "pillar": "",
    "details": "Databricks recommends using Unity Catalog to manage access to all data stored in cloud object storage. Unity Catalog provides a suite of tools to configure secure connections to cloud object storage.\n\nAWS | Azure | GCP"
  },
  "SCP-02-03": {
    "identifier": "SCP-02-03",
    "solution": "",
    "practice": "Use data exfiltration settings within the admin console.",
    "pillar": "",
    "details": "https://www.databricks.com/blog/2021/02/02/data-exfiltration-protection-with-databricks-on-aws.html"
  },
  "SCP-02-04": {
    "identifier": "SCP-02-04",
    "solution": "",
    "practice": "Use bucket versioning.",
    "pillar": "",
    "details": "You can use S3 bucket versioning to provide additional redundancy for data stored with Delta Lake. Databricks recommends implementing a lifecycle management policy for all S3 buckets with versioning enabled. Databricks recommends retaining three versions."
  },
  "SCP-02-05": {
    "identifier": "SCP-02-05",
    "solution": "",
    "practice": "Encrypt storage and restrict access.",
    "pillar": "",
    "details": "An external location is an object that combines a cloud storage path with a storage credential that authorizes access to the cloud storage path. Each storage location is subject to Unity Catalog access-control policies that control which users and groups can access the credential. If a user does not have access to a storage location in Unity Catalog, the request fails and Unity Catalog does not attempt to authenticate to your cloud tenant on the user\u2019s behalf.  \n\nAWS | Azure | GCP"
  },
  "SCP-02-06": {
    "identifier": "SCP-02-06",
    "solution": "",
    "practice": "Add a customer-managed key for managed services.",
    "pillar": "",
    "details": "Databricks has two customer-managed key use cases that involve different types of data and locations:\n\nManaged services: Data in the Databricks control plane (notebooks, secrets, and Databricks SQL query data).\nWorkspace storage: Your workspace root S3 buckets and the EBS volumes of compute resources in the classic compute plane.\n\nAWS | Azure | GCP"
  },
  "SCP-02-07": {
    "identifier": "SCP-02-07",
    "solution": "",
    "practice": "Add a customer-managed key for workspace storage.",
    "pillar": "",
    "details": "https://www.databricks.com/blog/2021/02/02/data-exfiltration-protection-with-databricks-on-aws.html"
  },
  "SCP-03-01": {
    "identifier": "SCP-03-01",
    "solution": "",
    "practice": "Deploy with a customer-managed VPC or VNet.",
    "pillar": "",
    "details": "By default, clusters are created in a single AWS VPC (Virtual Private Cloud) that Databricks creates and configures in your AWS account. You can optionally create your Databricks workspaces in your own VPC, a feature known as customer-managed VPC. You can use a customer-managed VPC to exercise more control over your network configurations to comply with specific cloud security and governance standards your organization may require. To configure your workspace to use AWS Private Link for any type of connection, your workspace must use a customer-managed VPC. \n\nAWS | Azure | GCP"
  },
  "SCP-03-02": {
    "identifier": "SCP-03-02",
    "solution": "",
    "practice": "Use IP access lists.",
    "pillar": "",
    "details": "https://docs.databricks.com/en/security/network/front-end/ip-access-list.html"
  },
  "SCP-03-03": {
    "identifier": "SCP-03-03",
    "solution": "",
    "practice": "Implement network exfiltration protections.",
    "pillar": "",
    "details": "https://www.databricks.com/blog/2021/02/02/data-exfiltration-protection-with-databricks-on-aws.html"
  },
  "SCP-03-04": {
    "identifier": "SCP-03-04",
    "solution": "",
    "practice": "Apply VPC service controls.",
    "pillar": "",
    "details": "https://docs.aws.amazon.com/awsconsolehelpdocs/latest/gsg/implementing-console-private-access-policies.html "
  },
  "SCP-03-05": {
    "identifier": "SCP-03-05",
    "solution": "",
    "practice": "Use VPC endpoint policies.",
    "pillar": "",
    "details": "https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-access.html"
  },
  "SCP-03-06": {
    "identifier": "SCP-03-06",
    "solution": "",
    "practice": "Configure Private Link.",
    "pillar": "",
    "details": "https://docs.databricks.com/en/security/network/classic/privatelink.html"
  },
  "SCP-04-01": {
    "identifier": "SCP-04-01",
    "solution": "",
    "practice": "Review the Shared Responsibility Model.",
    "pillar": "",
    "details": "The Databricks shared responsibility model outlines the security and compliance obligations of Databricks, the cloud service provider and the customer with respect to the data and services on the Databricks platform.\n\nAWS | Azure | GCP"
  },
  "SCP-05-01": {
    "identifier": "SCP-05-01",
    "solution": "",
    "practice": "Review the Databricks compliance programs.",
    "pillar": "",
    "details": "https://docs.databricks.com/en/security/index.html"
  },
  "SCP-06-01": {
    "identifier": "SCP-06-01",
    "solution": "",
    "practice": "Monitor workspace using System tables",
    "pillar": "",
    "details": "https://community.databricks.com/t5/technical-blog/overwatch-the-observability-tool-for-databricks/ba-p/54120"
  },
  "SCP-06-02": {
    "identifier": "SCP-06-02",
    "solution": "",
    "practice": "Use Databricks audit log.",
    "pillar": "",
    "details": "https://docs.databricks.com/en/administration-guide/system-tables/audit-logs.html"
  },
  "SCP-06-03": {
    "identifier": "SCP-06-03",
    "solution": "",
    "practice": "Monitor provisioning activities.",
    "pillar": "",
    "details": "https://docs.databricks.com/en/administration-guide/system-tables/audit-logs.html"
  },
  "SCP-06-04": {
    "identifier": "SCP-06-04",
    "solution": "",
    "practice": "Use Enhanced Security Monitoring or Compliance Security Profile",
    "pillar": "",
    "details": "Enhanced Security Monitoring (ESM) and Compliance Security Profile (CSP) provide the most secure baseline for\nDatabricks deployments.\nEnhanced Security Monitoring provides:\n1. An AMI with enhanced CIS Level 1 hardening\n2. Behavior-based malware monitoring and file integrity monitoring (Capsule8)\n3. Malware and anti-virus detection (ClamAV)\n4. Qualys vulnerability reports from a representative host OS\nThe Compliance Security Profile takes all the benefits above, and layers on additional security controls required to meet\ncompliance requirements:\n1. FIPS 140-2 Level 1 validated encryption modules (where possible)\n2. AWS Nitro VM enforcement for data at rest and in transit encryption\n3. Cluster update enforcement (auto-restart after 25 days)\n4. HIPAA, PCI-DSS, FedRAMP Moderate compliant features and controls\n\nAWS | Azure | GCP\nAWS | Azure | GCP"
  },
  "SCP-06-05": {
    "identifier": "SCP-06-05",
    "solution": "",
    "practice": "Configure tagging to monitor usage and enable charge-back.",
    "pillar": "",
    "details": "To track Databricks usage through to AWS resource billing can configure tagging on clusters or pools. These can also be\nenforced via cluster policies for different groups within your organization.\n\nAWS | Azure | GCP"
  },
  "SCP-07-01": {
    "identifier": "SCP-07-01",
    "solution": "",
    "practice": "Use AWS Nitro instances.",
    "pillar": "",
    "details": "AWS Nitro instances can provide two major security benefits:\n1. AWS Nitro instances use NVMe disks that automatically encrypt data at rest. From AWS docs as of July 19 2021:\nThe data on NVMe instance storage is encrypted using an XTS-AES-256 block cipher implemented in a\nhardware module on the instance. The encryption keys are generated using the hardware module and\nare unique to each NVMe instance storage device. All encryption keys are destroyed when the instance is\nstopped or terminated and cannot be recovered. You cannot disable this encryption and you cannot\nprovide your own encryption key.\n2. Many AWS Nitro instances also automatically encrypt data in transit between hosts. You can configure instance\ntypes included in the Encryption in Transit section of the AWS Nitro documentation so that intra-cluster\n(inter-host) traffic will be encrypted in-transit.\nDatabricks cannot authoritatively provide detail on capabilities in AWS, and the information above is provided on a\nbest-effort basis as a convenience to Databricks customers."
  },
  "SCP-07-02": {
    "identifier": "SCP-07-02",
    "solution": "",
    "practice": "Service quotas.",
    "pillar": "",
    "details": "Every service launched on a cloud will have to take limits into account, such as access rate limits, number of instances, number of users, and memory requirements. For your cloud provider, check the cloud limits. Before designing a solution, these limits must be understood. \n\nAWS | Azure | GCP\nAWS | Azure | GCP\nAWS | Azure | GCP"
  },
  "SCP-07-03": {
    "identifier": "SCP-07-03",
    "solution": "",
    "practice": "Leverage CI/CD processes to scan code for hard-coded secrets.",
    "pillar": "",
    "details": "Mature organizations often build production workloads by using CI/CD to integrate code scanning, better control\npermissions, perform linting, and more. When there is highly sensitive data analyzed, a CI/CD process can also allow\nscanning for known scenarios such as hard coded secrets."
  },
  "SCP-07-04": {
    "identifier": "SCP-07-04",
    "solution": "",
    "practice": "Isolate sensitive workloads into different workspaces.",
    "pillar": "",
    "details": "While Databricks has numerous capabilities for isolating different workloads, such as table ACLs and IAM passthrough for\nvery sensitive workloads, the primary isolation method is to move sensitive workloads to a different workspace. This\nsometimes happens when a customer has very different teams (for example, a security team and a marketing team) who\nmust both analyze different data in Databricks.\n\nBlog"
  },
  "SCP-07-05": {
    "identifier": "SCP-07-05",
    "solution": "",
    "practice": "Controlling libraries.",
    "pillar": "",
    "details": "By default, Databricks allows customers to install Python, R, or scala libraries from the standard public repositories, such\nas pypi, CRAN, or maven.\nThose who are concerned about supply-chain attacks, can host their own repositories and then configure Databricks to\nuse those instead. You can block access to other sources of libraries. Documentation for doing so is outside the scope of\nthis document, but reach out to your Databricks team for assistance as required. \n\nAWS | Azure | GCP"
  },
  "R-01-01": {
    "identifier": "R-01-01",
    "solution": "",
    "practice": "Use a data format that supports ACID transactions",
    "pillar": "",
    "details": "Delta Lake is an open source storage format that brings reliability to data lakes. Delta Lake provides ACID transactions, schema enforcement, scalable metadata handling, and unifies streaming and batch data processing. Delta Lake runs on top of your existing data lake and is fully compatible with Apache Spark APIs. Delta Lake on Databricks allows you to configure Delta Lake based on your workload patterns. \n\nAWS | Azure | GCP"
  },
  "R-01-02": {
    "identifier": "R-01-02",
    "solution": "",
    "practice": "Use a resilient distributed data engine for all workloads",
    "pillar": "",
    "details": "Apache Spark, as the compute engine of the Databricks lakehouse, is based on resilient distributed data processing. In case of an internal Spark task not returning a result as expected, Apache Spark automatically reschedules the missing tasks and continues with the execution of the entire job. This is helpful for failures outside the code, like a short network issue or a revoked spot VM. Working with both the SQL API and the Spark DataFrame API comes with this resilience built into the engine.\n\nIn the Databricks lakehouse, Photon, a native vectorized engine entirely written in C++, is high performance compute compatible with Apache Spark APIs. \n\nAWS | Azure | GCP"
  },
  "R-01-03": {
    "identifier": "R-01-03",
    "solution": "",
    "practice": "Automatically rescue invalid or nonconforming data ",
    "pillar": "",
    "details": "Invalid or nonconforming data can lead to crashes of workloads that rely on an established data format. To increase the end-to-end resilience of the whole process, it is best practice to filter out invalid and nonconforming data at ingestion. Supporting rescued data ensures you never lose or miss out on data during ingest or ETL. The rescued data column contains any data that wasn\u2019t parsed, either because it was missing from the given schema, because there was a type mismatch, or the column casing in the record or file didn\u2019t match that in the schema. \n\nAWS | Azure | GCP\nAWS | Azure | GCP"
  },
  "R-01-04": {
    "identifier": "R-01-04",
    "solution": "",
    "practice": "Configure jobs for automatic retries and termination",
    "pillar": "",
    "details": "Distributed systems are complex, and a failure at one point can potentially cascade throughout the system.\n\nDatabricks jobs support an automatic retry policy that determines when and how many times failed runs are retried.\n\nDelta Live Tables also automates failure recovery by using escalating retries to balance speed with reliability. See Development and production modes.\n\nOn the other hand, a task that hangs can prevent the whole job from finishing, thus incurring high costs. Databricks jobs support a timeout configuration to terminate jobs that take longer than expected. \n\nAWS | Azure | GCP"
  },
  "R-01-05": {
    "identifier": "R-01-05",
    "solution": "",
    "practice": "Use a scalable and production-grade model serving infrastructure",
    "pillar": "",
    "details": "For batch and streaming inference, use Databricks jobs and MLflow to deploy models as Apache Spark UDFs to leverage job scheduling, retries, autoscaling, and so on. \nModel serving provides a scalable and production-grade model real-time serving infrastructure. It processes your machine learning models using MLflow and exposes them as REST API endpoints. This functionality uses serverless compute, which means that the endpoints and associated compute resources are managed and run in the Databricks cloud account. \n\nAWS | Azure | GCP\nAWS | Azure | GCP"
  },
  "R-01-06": {
    "identifier": "R-01-06",
    "solution": "",
    "practice": "Use managed services for your workloads",
    "pillar": "",
    "details": "Leverage managed services of the Databricks Data Intelligence Platform like serverless compute, model serving, or Delta Live Tables where possible. These services are - without extra effort by the customer - operated by Databricks in a reliable and scalable way, making workloads more reliable.\n\nAWS | Azure | GCP\nAWS | Azure | GCP\nAWS | Azure | GCP"
  },
  "R-02-01": {
    "identifier": "R-02-01",
    "solution": "",
    "practice": "Use a layered storage architecture",
    "pillar": "",
    "details": "Curate data by creating a layered architecture and ensuring data quality increases as data moves through the layers. A common layering approach is:\n\nRaw layer (bronze): Source data gets ingested into the lakehouse into the first layer and should be persisted there. When all downstream data is created from the raw layer, rebuilding the subsequent layers from this layer is possible if needed.\n\nCurated layer (silver): The purpose of the second layer is to hold cleansed, refined, filtered and aggregated data. The goal of this layer is to provide a sound, reliable foundation for analyses and reports across all roles and functions.\n\nFinal layer (gold): The third layer is created around business or project needs. It provides a different view as data products to other business units or projects, preparing data around security needs (such as anonymized data) or optimizing for performance (such as with pre aggregated views). The data products in this layer are seen as the truth for the business.\n\nThe final layer should only contain high-quality data and can be fully trusted from a business point of view.\nAWS | Azure | GCP"
  },
  "R-02-02": {
    "identifier": "R-02-02",
    "solution": "",
    "practice": "Improve data integrity by reducing data redundancy",
    "pillar": "",
    "details": "Copying or duplicating data creates data redundancy and will lead to lost integrity, lost data lineage, and often different access permissions. This will decrease the quality of the data in the lakehouse. A temporary or throwaway copy of data is not harmful on its own - it is sometimes necessary for boosting agility, experimentation and innovation. However, if these copies become operational and regularly used for business decisions, they become data silos. These data silos getting out of sync has a significant negative impact on data integrity and quality, raising questions such as \u201cWhich data set is the master?\u201d or \u201cIs the data set up to date?\u201d.\n\nAWS | Azure | GCP"
  },
  "R-02-03": {
    "identifier": "R-02-03",
    "solution": "",
    "practice": "Actively manage schemas",
    "pillar": "",
    "details": "Uncontrolled schema changes can lead to invalid data and failing jobs that use these data sets. Databricks has several methods to validate and enforce the schema:\n\nDelta Lake supports schema validation and schema enforcement by automatically handling schema variations to prevent the insertion of bad records during ingestion.\n\nAuto Loader detects the addition of new columns as it processes your data. By default, the addition of a new column causes your streams to stop with an UnknownFieldException. Auto Loader supports several modes for schema evolution. \n\nAWS | Azure | GCP"
  },
  "R-02-04": {
    "identifier": "R-02-04",
    "solution": "",
    "practice": "Use constraints and data expectations",
    "pillar": "",
    "details": "Delta tables support standard SQL constraint management clauses that ensure that the quality and integrity of data added to a table are automatically verified. When a constraint is violated, Delta Lake throws an InvariantViolationException error to signal that the new data can\u2019t be added. See Constraints on Databricks.\n\nTo further improve this handling, Delta Live Tables supports Expectations: Expectations define data quality constraints on the contents of a data set. An expectation consists of a description, an invariant, and an action to take when a record fails the invariant. Expectations to queries use Python decorators or SQL constraint clauses. See Manage data quality with Delta Live Tables. \n\nAWS | Azure | GCP"
  },
  "R-02-05": {
    "identifier": "R-02-05",
    "solution": "",
    "practice": "Take a data-centric approach to machine learning",
    "pillar": "",
    "details": "One guiding principle that continues to lie at the heart of the AI vision for the Databricks Data Intelligence Platform is taking a data-centric approach to machine learning. With the increasing prevalence of generative AI, this perspective remains just as important. The core constituents of any ML project can be viewed simply as data pipelines \n\nFeature engineering, training, inference, and monitoring pipelines are data pipelines. They must be as robust as other production data engineering processes. Data quality is crucial in any ML application, so ML data pipelines should employ systematic approaches to monitoring and mitigating data quality issues. Avoid tools that make it challenging to join data from ML predictions, model monitoring, and so on, with the rest of your data. The simplest way to achieve this is to develop ML applications on the same platform used to manage production data. For example, instead of downloading training data to a laptop, where it is hard to govern and reproduce results, secure the data in cloud storage and make that storage available to your training process. \n\neBook"
  },
  "R-03-01": {
    "identifier": "R-03-01",
    "solution": "",
    "practice": "Enable autoscaling for ETL workloads",
    "pillar": "",
    "details": "Autoscaling allows clusters to resize automatically based on workloads. Autoscaling can benefit many use cases and scenarios from both a cost and performance perspective. The documentation provides considerations for determining whether to use Autoscaling and how to get the most benefit.\n\nFor streaming workloads, Databricks recommends using Delta Live Tables with autoscaling. See Use autoscaling to increase efficiency and reduce resource usage. \n\nAWS | Azure | GCP\nAWS | Azure | GCP"
  },
  "R-03-02": {
    "identifier": "R-03-02",
    "solution": "",
    "practice": "Use autoscaling for SQL Warehouses",
    "pillar": "",
    "details": "The scaling parameter of a SQL warehouse sets the minimum and the maximum number of clusters over which queries sent to the warehouse are distributed. The default is a minimum of one and a maximum of one cluster.\n\nTo handle more concurrent users for a given warehouse, increase the cluster count. To learn how Databricks adds clusters to and removes clusters from a warehouse, see SQL warehouse sizing, scaling, and queuing behavior. \n\nAWS | Azure | GCP\nAWS | Azure | GCP\n\nDatabricks enhanced autoscaling optimizes cluster utilization by automatically allocating cluster resources based on workload volume, with minimal impact on the data processing latency of your pipelines. \n\nAWS | Azure | GCP\nAWS | Azure | GCP"
  },
  "R-04-01": {
    "identifier": "R-04-01",
    "solution": "",
    "practice": "Recover from Structured Streaming query failures",
    "pillar": "",
    "details": "Structured Streaming provides fault-tolerance and data consistency for streaming queries. Using Databricks workflows, you can easily configure your Structured Streaming queries to restart on failure automatically. The restarted query continues where the failed one left off. \n\nAWS | Azure | GCP"
  },
  "R-04-02": {
    "identifier": "R-04-02",
    "solution": "",
    "practice": "Recover ETL jobs using data time travel capabilities",
    "pillar": "",
    "details": "Despite thorough testing, a job in production can fail or produce some unexpected, even invalid, data. Sometimes this can be fixed with an additional job after understanding the source of the issue and fixing the pipeline that led to the issue in the first place. However, often this is not straightforward, and the respective job should be rolled back. Using Delta Time travel allows users to easily roll back changes to an older version or timestamp, repair the pipeline, and restart the fixed pipeline. See What is Delta Lake time travel?.\n\nA convenient way to do so is the RESTORE command. \n\nAWS | Azure | GCP\nBlog"
  },
  "R-04-03": {
    "identifier": "R-04-03",
    "solution": "",
    "practice": "Leverage a job automation framework with built-in recovery",
    "pillar": "",
    "details": "Databricks Workflows are built for recovery. When a task in a multi-task job fails (and, as such, all dependent tasks), Databricks Workflows provide a matrix view of the runs, which lets you examine the issue that led to the failure. See View runs for a job. Whether it was a short network issue or a real issue in the data, you can fix it and start a repair run in Databricks Workflows. It runs only the failed and dependent tasks and keep the successful results from the earlier run, saving time and money. \n\nAWS | Azure | GCP"
  },
  "R-04-04": {
    "identifier": "R-04-04",
    "solution": "",
    "practice": "Configure a disaster recovery pattern",
    "pillar": "",
    "details": "Databricks is often a core part of an overall data ecosystem that includes many services, including upstream data ingestion services (batch/streaming), cloud-native storage, downstream tools and services such as business intelligence apps, and orchestration tooling. Some of your use cases might be particularly sensitive to a regional service-wide outage.\n\nDisaster recovery involves a set of policies, tools, and procedures that enable the recovery or continuation of vital technology infrastructure and systems following a natural or human-induced disaster. A large cloud service like Azure, AWS, or GCP serves many customers and has built-in guards against a single failure. For example, a region is a group of buildings connected to different power sources to guarantee that a single power loss will not shut down a region. However, cloud region failures can happen, and the degree of disruption and its impact on your organization can vary.\n\nEssential parts of a disaster recovery strategy are selecting a strategy (active/active or active/passive), selecting the right toolset, and testing both failover and restore. \n\nAWS | Azure | GCP"
  },
  "R-05-01": {
    "identifier": "R-05-01",
    "solution": "",
    "practice": "Monitor data platform events",
    "pillar": "",
    "details": "Monitor Data platform and ML events"
  },
  "R-05-02": {
    "identifier": "R-05-02",
    "solution": "",
    "practice": "Monitor cloud events",
    "pillar": "",
    "details": "Monitor events on your cloud provider"
  },
  "PE-01-01": {
    "identifier": "PE-01-01",
    "solution": "",
    "practice": "Use serverless architecture",
    "pillar": "",
    "details": "With the serverless compute on the Databricks Data Intelligence Platform, the compute layer runs in the customer\u2019s Databricks account. Workspace admins can create serverless SQL warehouses that enable instant compute and are managed by Databricks. A serverless SQL warehouse uses compute clusters hosted in the Databricks customer account. Use them with Databricks SQL queries just like you usually would with the original Databricks SQL warehouses. Serverless compute comes with a very fast starting time for SQL warehouses (10s and below), and the infrastructure is managed by Databricks.\n\nThis leads to improved productivity:\n\nCloud administrators no longer have to manage complex cloud environments, for example by adjusting quotas, creating and maintaining networking assets, and joining billing sources.\n\nUsers benefit from near-zero waiting times for cluster start and improved concurrency on their queries.\n\nCloud administrators can refocus their time on higher-value projects instead of managing low-level cloud components. \n\nAWS | Azure | GCP"
  },
  "PE-01-02": {
    "identifier": "PE-01-02",
    "solution": "",
    "practice": "Use an enterprise grade model serving service",
    "pillar": "",
    "details": "Databricks Model Serving provides a unified interface to deploy, govern, and query AI models. Each model you serve is available as a REST API that you can integrate into your web or client application.\n\nModel Serving provides a highly available and low-latency service for deploying models. The service automatically scales up or down to meet demand changes, saving infrastructure costs while optimizing latency performance. This functionality uses serverless compute. \n\nAWS | Azure | GCP"
  },
  "PE-02-01": {
    "identifier": "PE-02-01",
    "solution": "",
    "practice": "Understand your data ingestion and access patterns",
    "pillar": "",
    "details": "From a performance perspective, data access patterns - such as \u201caggregations versus point access\u201d or \u201cscan versus search\u201d - behave differently depending on the data size. Large files are more efficient for scan queries and smaller files better for search since you have to read fewer data to find the specific row(s).\n\nFor ingestion patterns, it\u2019s common to use DML statements. DML statements are most performant when the data is clustered, and you can simply isolate the section of data. Keeping the data clustered and isolatable on ingestion is important: Consider keeping a natural time sort order and apply as many filters as possible to the ingest target table. For append-only and overwrite ingestion workloads, there isn't much to consider, as this is a relatively cheap operation.\n\nThe ingestion and access patterns often point to an obvious data layout and clustering. If they do not, decide what is more important to your business and skew toward how to solve that goal better. \n\nAWS | Azure | GCP"
  },
  "PE-02-02": {
    "identifier": "PE-02-02",
    "solution": "",
    "practice": "Use parallel computation where it is beneficial",
    "pillar": "",
    "details": "Time to value is an important dimension when working with data. While many use cases can be easily implemented on a single machine (small data, few and simple computation steps), often use cases come up that:\n\nNeed to process large data sets.\n\nHave long running times due to complicated algorithms.\n\nMust be repeated 100s and 1000s of times.\n\nThe cluster environment of the Databricks platform is a great environment to distribute these workloads efficiently. It automatically parallelism SQL queries across all nodes of a cluster and it provides libraries for Python and Scala to do the same. Under the hood, the engines Apache Spark and Photon analyze the queries, determine the optimal way of parallel execution, and manage the distributed execution in a resilient way.\nHere are some parallel procesing capabilities in databricks that works with apache sparrk\n\nAWS | Azure | GCP\nAWS | Azure | GCP\nAWS | Azure | GCP\nMLlib\nAWS | Azure | GCP\nAWS | Azure | GCP"
  },
  "PE-02-03": {
    "identifier": "PE-02-03",
    "solution": "",
    "practice": "Analyze the whole chain of execution",
    "pillar": "",
    "details": "Most pipelines or consumption patterns use a chain of systems. For example, for BI tools the performance is impacted by several factors:\nThe BI tool itself.\nThe connector that connects the BI tool and the SQL engine.\nThe SQL engine where the BI tool sends the query.\nFor best-in-class performance, the whole chain needs to be taken into account and selected/tuned for best performance. \n\nAWS | Azure | GCP"
  },
  "PE-02-04": {
    "identifier": "PE-02-04",
    "solution": "",
    "practice": "Prefer larger clusters",
    "pillar": "",
    "details": "Plan for larger clusters, especially when the workload scales linearly. In that case, it is not more expensive to use a large cluster for a workload than to use a smaller one. It\u2019s just faster. The key is that you're renting the cluster for the length of the workload. So, if you spin up two worker clusters and it takes an hour, you're paying for those workers for the full hour. Similarly, if you spin up a four-worker cluster and it takes only half an hour (here comes the linear scalability into play), the costs are the same. If costs are the primary driver with a very flexible SLA, an autoscaling cluster is almost always going to be the cheapest but not necessarily the fastest. "
  },
  "PE-02-05": {
    "identifier": "PE-02-05",
    "solution": "",
    "practice": "Use native Spark operations",
    "pillar": "",
    "details": "User Defined Functions (UDFs) are a great way to extend the functionality of Spark SQL. However, don\u2019t use Python or Scala UDFs if a native function exists:\nSpark SQL\nPySpark\n\nReasons:\nTo transfer data between Python and Spark, serialization is needed. This drastically slows down queries.\nHigher efforts for implementing and testing functionality already existing in the platform.\n\nIf native functions are missing and should be implemented as Python UDFs, use Pandas UDFs. Apache Arrow ensures data moves efficiently back and forth between Spark and Python.\n\nAWS | Azure | GCP\nApache Arrow "
  },
  "PE-02-06": {
    "identifier": "PE-02-06",
    "solution": "",
    "practice": "Use native platform engines",
    "pillar": "",
    "details": "Photon is the engine on Databricks that provides fast query performance at low cost \u2013 from data ingestion, ETL, streaming, data science, and interactive queries \u2013 directly on your data lake. Photon is compatible with Apache Spark APIs, so getting started is as easy as turning it on \u2013 no code changes and no lock-in.\n\nPhoton is part of a high-performance runtime that runs your existing SQL and DataFrame API calls faster and reduces your total cost per workload. Photon is used by default in Databricks SQL warehouses. \nPhoton is available by default for all Databricks SQL Warehouse Types\n\nAWS | Azure | GCP"
  },
  "PE-02-07": {
    "identifier": "PE-02-07",
    "solution": "",
    "practice": "Understand your hardware and workload type",
    "pillar": "",
    "details": "Not all cloud VMs are created equally. The different families of machines offered by cloud providers are all different enough to matter. There are obvious differences - RAM and cores - and more subtle differences - processor type and generation, network bandwidth guarantees, and local high-speed storage versus local disk versus remote disk. There are also differences in the \u201cspot\u201d markets. These should be understood before deciding on the best VM type for your workload. \n\nAWS | Azure | GCP\n\nPlease note - Serverless compute manages clusters automatically, so this is not needed for serverless compute."
  },
  "PE-02-08": {
    "identifier": "PE-02-08",
    "solution": "",
    "practice": "Use caching",
    "pillar": "",
    "details": "There are two types of caching available in Databricks: Delta caching and Spark caching. \nUse Disk Cache and Avoid Spark Caching\nAWS | Azure | GCP\nSpark performance tuning\n\nAdditional Cache Types: \nQuery Result Cache - AWS | Azure | GCP \nDatabricks SQL UI caching - AWS | Azure | GCP  \nPrewarm Delta cache for BI workloads\n\nPrewarm clusters (Serverless compute manages clusters automatically, so this is not needed for serverless compute.)\nAWS | Azure | GCP"
  },
  "PE-02-09": {
    "identifier": "PE-02-09",
    "solution": "",
    "practice": "Use compaction",
    "pillar": "",
    "details": "Delta Lake on Databricks can improve the speed of reading queries from a table. One way to improve this speed is to coalesce small files into larger ones. You trigger compaction by running the OPTIMIZE command. See Compact data files with optimize on Delta Lake.\n\nYou can also compact small files automatically using Auto Optimize. See Consider file size tuning. \n\nAWS | Azure | GCP"
  },
  "PE-02-10": {
    "identifier": "PE-02-10",
    "solution": "",
    "practice": "Use data skipping",
    "pillar": "",
    "details": "Data skipping: To achieve this, data skipping information is collected automatically when you write data into a Delta table (by default Delta Lake on Databricks collects statistics on the first 32 columns defined in your table schema). Delta Lake on Databricks takes advantage of this information (minimum and maximum values) at query time to provide faster queries. See Data skipping for Delta Lake.\n\nFor best results, apply Z-ordering, a technique to collocate related information in the same set of files. This co-locality is automatically used on Databricks by Delta Lake data-skipping algorithms. This behavior dramatically reduces the amount of data Delta Lake on Databricks needs to read.\n\nDynamic file pruning: Dynamic file pruning (DFP) can significantly improve the performance of many queries on Delta tables. DFP is especially efficient for non-partitioned tables or joins on non-partitioned columns. \nAWS | Azure | GCP\nAWS | Azure | GCP\nAWS | Azure | GCP\nDelta Lake liquid clustering replaces table partitioning and ZORDER to simplify data layout decisions and optimize query performance. Liquid clustering provides flexibility to redefine clustering keys without rewriting existing data, allowing data layout to evolve alongside analytic needs over time.\n\nAWS | Azure | GCP"
  },
  "PE-02-11": {
    "identifier": "PE-02-11",
    "solution": "",
    "practice": "Enable Predictive Optimization on your metastore",
    "pillar": "",
    "details": "Predictive optimization removes the need to manually manage maintenance operations for Delta tables on Databricks.\n\nWith predictive optimization enabled, Databricks automatically identifies tables that would benefit from maintenance operations and runs them for the user. Maintenance operations are only run as necessary, eliminating both unnecessary runs for maintenance operations and the burden associated with tracking and troubleshooting performance.\n\nAWS | Azure | GCP"
  },
  "PE-02-12": {
    "identifier": "PE-02-12",
    "solution": "",
    "practice": "Avoid over-partitioning",
    "pillar": "",
    "details": "In the past, partitioning was the most common way to skip data. However, partitioning is static and manifests as a file system hierarchy. There is no easy way to change partitions if the access patterns change over time. Often, partitioning leads to over-partitioning - in other words, too many partitions with too small files, which results in bad query performance. See Partitions.\n\nIn the meantime, a much better choice than partitioning is Z-ordering.\nAWS | Azure | GCP"
  },
  "PE-02-14": {
    "identifier": "PE-02-14",
    "solution": "",
    "practice": "Consider file size tuning",
    "pillar": "",
    "details": "The term auto optimize is sometimes used to describe functionality controlled by the settings delta.autoCompact and delta.optimizeWrite. This term has been retired in favor of describing each setting individually. See Configure Delta Lake to control data file size.\n\nAuto Optimize is particularly useful in the following scenarios:\nStreaming use cases where latency in the order of minutes is acceptable.\nMERGE INTO is the preferred method of writing into Delta Lake.\nCREATE TABLE AS SELECT or INSERT INTO are commonly used operations. \n\nAWS | Azure | GCP"
  },
  "PE-02-15": {
    "identifier": "PE-02-15",
    "solution": "",
    "practice": "Optimize join performance",
    "pillar": "",
    "details": "A range join occurs when two relations are joined using a point in interval or interval overlap condition. The range join optimization support in Databricks Runtime can bring orders of magnitude improvement in query performance but requires careful manual tuning \n\nAWS | Azure | GCP\n\nData skew is a condition in which a table\u2019s data is unevenly distributed among partitions in the cluster. Data skew can severely downgrade the performance of queries, especially those with joins. Joins between big tables require shuffling data, and the skew can lead to an extreme imbalance of work in the cluster. \nAWS | Azure | GCP\nAWS | Azure | GCP"
  },
  "PE-02-16": {
    "identifier": "PE-02-16",
    "solution": "",
    "practice": "Run analyze table to collect table statistics",
    "pillar": "",
    "details": "Run analyze table to collect statistics on the entire table for the query plan\nThis information is persisted in the metastore and helps the query optimizer by:\n\nChoosing the proper join type.\nSelecting the correct build side in a hash-join.\nCalibrating the join order in a multi-way join.\n\nIt should be run alongside OPTIMIZE on a daily basis and is recommended on tables < 5TB. The only caveat is that analyze table is not incremental.\nAWS | Azure | GCP"
  },
  "PE-03-01": {
    "identifier": "PE-03-01",
    "solution": "",
    "practice": "Test on data representative of production data",
    "pillar": "",
    "details": "Run performance testing on production data (read-only) or similar data. When using similar data, characteristics like volume, file layout, and data skews should be like production data, since this has a significant impact on performance."
  },
  "PE-03-02": {
    "identifier": "PE-03-02",
    "solution": "",
    "practice": "Take prewarming of resources into account",
    "pillar": "",
    "details": "Take prewarming of resources into account\nThe first query on a new cluster is slower than all the others:\n\nIn general, cluster resources need to initialize on multiple layers.\n\nWhen caching is part of the setup, the first run ensures that the data is in the cache, which speeds up subsequent jobs.\n\nPrewarming resources - running specific queries for the sake of initializing resources and filling caches (for example, after a cluster restart) - can significantly increase the performance of the first queries. So, to understand the behavior for the different scenarios, test the performance of the first execution (with and without prewarming) and subsequent executions.\n\nAWS | Azure | GCP"
  },
  "PE-03-03": {
    "identifier": "PE-03-03",
    "solution": "",
    "practice": "Identify bottlenecks",
    "pillar": "",
    "details": "Bottlenecks are areas in your workload that might worsen the overall performance when the load in production increases. Identifying these at design time and testing against higher workloads will help to keep the workloads stable in production."
  },
  "PE-04-01": {
    "identifier": "PE-04-01",
    "solution": "",
    "practice": "Monitor query performanance",
    "pillar": "",
    "details": "Query Profile: Utilize the query profile feature to troubleshoot performance bottlenecks during a query's execution. It provides visualization of each query task and related metrics such as time spent, number of rows processed, and memory consumption\n\nSQL Warehouse Monitoring: Monitor SQL warehouses by viewing live statistics, peak query count charts, running clusters charts, and query history table"
  },
  "PE-04-02": {
    "identifier": "PE-04-02",
    "solution": "",
    "practice": "Monitor streaming workloads",
    "pillar": "",
    "details": "Structured Streaming Monitoring: For streaming queries, use the built-in monitoring in the Spark UI under the Streaming tab or push metrics to external services using Apache Spark\u2019s Streaming Query Listener interfac"
  },
  "PE-04-03": {
    "identifier": "PE-04-03",
    "solution": "",
    "practice": "Monitor job performance",
    "pillar": "",
    "details": "View and manage job runs through the Databricks UI, which provides details on job output, logs, metrics, and the success or failure of each task in the job run"
  },
  "CO-01-01": {
    "identifier": "CO-01-01",
    "solution": "",
    "practice": "Use performance optimized data formats",
    "pillar": "",
    "details": "Delta Lake is an open-source storage layer that enhances data lakes by providing ACID transactions, scalable metadata handling, and schema enforcement, primarily built on Apache Spark and Parquet. It supports a lakehouse architecture, allowing integration with various compute engines and programming languages. Delta Lake offers significant performance optimizations such as data skipping, indexing, and file management techniques like compaction and Z-Ordering. Additionally, it includes features like time travel for data versioning and Delta Live Tables for managing batch and streaming data pipelines efficiently. These capabilities make Delta Lake suitable for handling large-scale, transactional workloads and real-time analytics, ensuring data integrity and consistency across diverse data management scenarios\nBlog"
  },
  "CO-01-02": {
    "identifier": "CO-01-02",
    "solution": "",
    "practice": "Use job clusters",
    "pillar": "",
    "details": "A job is a way to run non-interactive code in a Databricks cluster. For example, you can run an extract, transform, and load (ETL) workload interactively or on a schedule. Of course, you can also run jobs interactively in the notebook UI. However, on job clusters, the non-interactive workloads will cost significantly less than on all-purpose clusters.\nAn additional advantage is that every job or workflow runs on a new cluster, isolating workloads from one another.\n\nAWS | Azure | GCP\nPricing"
  },
  "CO-01-03": {
    "identifier": "CO-01-03",
    "solution": "",
    "practice": "Use SQL warehouse for SQL workloads",
    "pillar": "",
    "details": "For interactive SQL workloads, a Databricks SQL warehouse is the most cost-efficient engine \n\nAWS | Azure | GCP"
  },
  "CO-01-04": {
    "identifier": "CO-01-04",
    "solution": "",
    "practice": "Use up-to-date runtimes for your workloads",
    "pillar": "",
    "details": "The Databricks platform provides different runtimes that are optimized for data engineering tasks (Databricks Runtime) or for Machine Learning (Databricks Runtime for Machine Learning). The runtimes are built to provide the best selection of libraries for the tasks and ensure that all provided libraries are up-to-date and work together optimally. Databricks Runtime is released on a regular cadence and offers performance improvements between major releases. These improvements in performance often lead to cost savings due to more efficient usage of cluster resources.\n\nAWS | Azure | GCP"
  },
  "CO-01-05": {
    "identifier": "CO-01-05",
    "solution": "",
    "practice": "Only use GPUs for the right workloads",
    "pillar": "",
    "details": "Virtual machines with GPUs can dramatically speed up computational processes for deep learning, but have a significantly higher price than CPU-only machines. Use GPU instances only for workloads that have GPU-accelerated libraries.\nMost workloads do not use GPU-accelerated libraries do not benefit from GPU-enabled instances. Workspace admins can restrict GPU machines and clusters to prevent unnecessary use. \n\nAWS | Azure | GCP\nBlog"
  },
  "CO-01-06": {
    "identifier": "CO-01-06",
    "solution": "",
    "practice": "Use Serverless for your workloads",
    "pillar": "",
    "details": "BI workloads typically use data in bursts and generate multiple concurrent queries. For example, someone using a BI tool might update a dashboard, write a query, or simply analyze query results without interacting further with the platform. This example demonstrates two requirements:\n\nTerminate clusters during idle periods to save costs.\nHave compute resources available quickly (for both start-up and scale-up) to satisfy user queries when they request new or updated data with the BI tool.\n\nServerless SQL warehouses start and scale up in seconds, so both immediate availability and termination during idle times can be achieved. This results in a great user experience and overall cost savings.\nAdditionally, serverless SQL warehouses scale down earlier than non-serverless warehouses, resulting lower costs. \n\nAWS | Azure | GCP\n\n\nDatabricks Model Serving provides a unified interface to deploy, govern, and query AI models. Each model you serve is available as a REST API that you can integrate into your web or client application.\n\nModel Serving provides a highly available and low-latency service for deploying models. The service automatically scales up or down to meet demand changes, saving infrastructure costs while optimizing latency performance. This functionality uses serverless compute. \n\nAWS | Azure | GCP"
  },
  "CO-01-07": {
    "identifier": "CO-01-07",
    "solution": "",
    "practice": "Use the right instance type",
    "pillar": "",
    "details": "Based on your workloads, it is also important to choose the right instance family to get the best performance/price ratio. Some simple rules of thumb are:\nMemory optimized for ML, heavy shuffle & spill workloads\nCompute optimized for Structured Streaming workloads, maintenance jobs (e.g. Optimize & Vacuum)\nStorage optimized for workloads that benefit from caching, e.g. ad-hoc & interactive data analysis\nGPU optimized for specific ML & DL workloads\nGeneral purpose in absence of specific requirements"
  },
  "CO-01-08": {
    "identifier": "CO-01-08",
    "solution": "",
    "practice": "Choose the most efficient cluster size",
    "pillar": "",
    "details": "Number of workers, instance type and size are important factors for compute configurations. Along with those when sizing compute, consider the data consumed, computational complexity, data partitioning, and parallelism required. For simple ETL workloads, focus on compute-optimized configurations, while memory and storage are important for shuffle-heavy workloads. Balancing the number of workers and instance size is crucial, as both can affect network I/O. Lastly, consider caching benefits for workloads that require frequent re-reads of the same data, using storage-optimized configurations with Delta Cache. \n\nAWS | Azure | GCP\nAWS | Azure | GCP"
  },
  "CO-01-09": {
    "identifier": "CO-01-09",
    "solution": "",
    "practice": "Evaluate performance optimized query engines",
    "pillar": "",
    "details": "Photon is a high-performance Databricks-native vectorized query engine that speeds up your SQL workloads and DataFrame API calls (for data ingestion, ETL, streaming, data science, and interactive queries). Photon is compatible with Apache Spark APIs, so getting started is as easy as turning it on \u2013 no code changes and no lock-in.\nThe observed speedup can lead to significant cost savings, and jobs that run regularly should be evaluated to see whether they are not only faster but also cheaper with Photon."
  },
  "CO-02-01": {
    "identifier": "CO-02-01",
    "solution": "",
    "practice": "Leverage auto-scaling compute",
    "pillar": "",
    "details": "Autoscaling allows your workloads to use the right amount of compute required to complete your jobs.\n\nEnable autoscaling for batch workloads.\nEnable autoscaling for SQL warehouse.\nAWS | Azure | GCP\n\nCompute auto-scaling has limitations scaling down cluster size for Structured Streaming workloads. Databricks recommends using Delta Live Tables with Enhanced Autoscaling for streaming workloads.\nUse Delta Live Tables Enhanced Autoscaling.\nAWS | Azure | GCP"
  },
  "CO-02-02": {
    "identifier": "CO-02-02",
    "solution": "",
    "practice": "Use auto termination",
    "pillar": "",
    "details": "Databricks provides a number of features to help control costs by reducing idle resources and controlling when compute resources can be deployed.\nConfigure auto termination for all interactive clusters. After a specified idle time, the cluster shuts down. See Automatic termination.\n\nIf a starting time that is significantly shorter than a full cluster start would be acceptable, consider using cluster pools. See Pool best practices. Databricks pools reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances. When a cluster is attached to a pool, cluster nodes are created using the pool\u2019s idle instances. If the pool has no idle instances, the pool expands by allocating a new instance from the instance provider in order to accommodate the cluster\u2019s request. When a cluster releases an instance, it returns to the pool and is free for another cluster to use. Only clusters attached to a pool can use that pool\u2019s idle instances.\nDatabricks does not charge DBUs while instances are idle in the pool, resulting in cost savings. Instance provider billing does apply. \n\nAWS | Azure | GCP\nAWS | Azure | GCP"
  },
  "CO-02-03": {
    "identifier": "CO-02-03",
    "solution": "",
    "practice": "Use compute policies to control costs",
    "pillar": "",
    "details": "Cluster policies can enforce many cost specific restrictions for clusters. See Operational Excellence - Use cluster policies. For example:\n\nEnable cluster autoscaling with a set minimum number of worker nodes.\nEnable cluster auto termination with a reasonable value (for example, 1 hour) to avoid paying for idle times.\nEnsure that only cost-efficient VM instances can be selected. Follow the best practices for cluster configuration. See Compute configuration best practices.\nApply a spot instance strategy.\n\nAWS | Azure | GCP"
  },
  "CO-03-01": {
    "identifier": "CO-03-01",
    "solution": "",
    "practice": "Monitor costs",
    "pillar": "",
    "details": "The account console allows viewing the billable usage. As a Databricks account owner or account admin, you can also use the account console to download billable usage logs. Databricks system tables provide detailed usage details with system tables you can use these to monitor usage.\n\nAs a best practice, the full costs (including VMs, storage, and network infrastructure) should be monitored. This can be achieved by cloud provider cost management tools or by adding third party tools.\n\nAWS | Azure | GCP"
  },
  "CO-03-02": {
    "identifier": "CO-03-02",
    "solution": "",
    "practice": "Tag clusters for cost attribution",
    "pillar": "",
    "details": "To monitor cost and accurately attribute Databricks usage to your organization\u2019s business units and teams (for example, for chargebacks), you can tag clusters and pools. These tags propagate to detailed DBU usage reports and to cloud provider VMs and blob storage instances for cost analysis.\n\nEnsure that cost control and attribution are already in mind when setting up workspaces and clusters for teams and use cases. This streamlines tagging and improves the accuracy of cost attributions.\n\nFor the overall costs, DBU virtual machine, disk, and any associated network costs must be considered. For serverless SQL warehouses this is simpler since the DBU costs already include virtual machine and disk costs.\n\nAWS | Azure | GCP"
  },
  "CO-03-03": {
    "identifier": "CO-03-03",
    "solution": "",
    "practice": "Implement observability to track & chargeback cost",
    "pillar": "",
    "details": "When working with complex technical ecosystems, proactively understanding the unknowns is key to maintaining platform stability and controlling costs. Observability provides a way to analyze and optimize systems based on the data they generate. This is different from monitoring, which focuses on identifying new patterns rather than tracking known issues.\nDatabricks provide great observability capabilities using System tables that are Databricks-hosted analytical stores of a customer account\u2019s operational data found in the system catalog. They provide historical observability across the account and include user-friendly tabular information on platform telemetry."
  },
  "CO-03-04": {
    "identifier": "CO-03-04",
    "solution": "",
    "practice": "Share cost reports regularly",
    "pillar": "",
    "details": "Create cost reports every month to track growth and anomalies in consumption. Share these reports broken down to use cases or teams with the teams that own the respective workloads by using cluster tagging. This avoids surprises and allows teams to proactively adapt their workloads if costs get too high.\n\nUse the account console to get high level usage - AWS | Azure | GCP\n\nDatabricks recommends using Unity Catalog system tables to generate detailed usage reports (by usage tags, etc) for enhanced cost reporting."
  },
  "CO-03-05": {
    "identifier": "CO-03-05",
    "solution": "",
    "practice": "Monitor and manage Delta Sharing egress costs",
    "pillar": "",
    "details": "Unlike other data sharing platforms, Delta Sharing does not require data replication. This model has many advantages, but it means that your cloud vendor may charge data egress fees when you share data across clouds or regions. See Monitor and manage Delta Sharing egress costs (for providers) to monitor and manage egress charges."
  },
  "CO-04-01": {
    "identifier": "CO-04-01",
    "solution": "",
    "practice": "Balance always-on and triggered streaming",
    "pillar": "",
    "details": "Traditionally, when people think about streaming, terms such as \u201creal-time,\u201d \u201c24/7,\u201d or \u201calways on\u201d come to mind. If data ingestion happens in \u201creal-time\u201d, the underlying cluster needs to run 24/7, producing consumption costs every single hour of the day.\n\nHowever, not every use case that is based on a continuous stream of events needs these events to be added to the analytics data set immediately. If the business requirement for the use case only needs fresh data every few hours or every day, then this requirement can be achieved with only several runs a day, leading to a significant cost reduction for the workload. Databricks recommends using Structured Streaming with trigger AvailableNow for incremental workloads that do not have low latency requirements. See Configuring incremental batch processing.\n\nAWS | Azure | GCP"
  },
  "CO-04-02": {
    "identifier": "CO-04-02",
    "solution": "",
    "practice": "Balance between on-demand and capacity excess instances",
    "pillar": "",
    "details": "Spot instances use cloud virtual machine excess resources that are available at a cheaper price. To save cost, Databricks supports creating clusters using spot instances. It is recommended to always have the first instance (Spark driver) as an on-demand virtual machine. Spot instances are a great selection for workloads when it is acceptable to take longer because one or more spot instances have been evicted by the cloud provider. \n\nAWS | Azure | GCP"
  }
}