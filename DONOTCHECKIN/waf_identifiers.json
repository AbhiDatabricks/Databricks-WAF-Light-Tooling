{
  "reliability": [
    {
      "Identifier": "R-01-01",
      "Best Practice": "Use a data format that supports ACID transactions",
      "Details": "Delta Lake is an open source storage format that brings reliability to data lakes. Delta Lake provides ACID transactions, schema enforcement, scalable metadata handling, and unifies streaming and batch data processing. Delta Lake runs on top of your existing data lake and is fully compatible with Apache Spark APIs. Delta Lake on Databricks allows you to configure Delta Lake based on your workload patterns. \n\nAWS | Azure | GCP",
      "Databricks Capabilities": "Delta Lake"
    },
    {
      "Identifier": "R-01-02",
      "Best Practice": "Use a resilient distributed data engine for all workloads",
      "Details": "Apache Spark, as the compute engine of the Databricks lakehouse, is based on resilient distributed data processing. In case of an internal Spark task not returning a result as expected, Apache Spark automatically reschedules the missing tasks and continues with the execution of the entire job. This is helpful for failures outside the code, like a short network issue or a revoked spot VM. Working with both the SQL API and the Spark DataFrame API comes with this resilience built into the engine.\n\nIn the Databricks lakehouse, Photon, a native vectorized engine entirely written in C++, is high performance compute compatible with Apache Spark APIs. \n\nAWS | Azure | GCP",
      "Databricks Capabilities": "Apache Spark; Photon"
    },
    {
      "Identifier": "R-01-03",
      "Best Practice": "Automatically rescue invalid or nonconforming data ",
      "Details": "Invalid or nonconforming data can lead to crashes of workloads that rely on an established data format. To increase the end-to-end resilience of the whole process, it is best practice to filter out invalid and nonconforming data at ingestion. Supporting rescued data ensures you never lose or miss out on data during ingest or ETL. The rescued data column contains any data that wasn\u2019t parsed, either because it was missing from the given schema, because there was a type mismatch, or the column casing in the record or file didn\u2019t match that in the schema. \n\nAWS | Azure | GCP\nAWS | Azure | GCP",
      "Databricks Capabilities": "Delta Live Tables"
    },
    {
      "Identifier": "R-01-04",
      "Best Practice": "Configure jobs for automatic retries and termination",
      "Details": "Distributed systems are complex, and a failure at one point can potentially cascade throughout the system.\n\nDatabricks jobs support an automatic retry policy that determines when and how many times failed runs are retried.\n\nDelta Live Tables also automates failure recovery by using escalating retries to balance speed with reliability. See Development and production modes.\n\nOn the other hand, a task that hangs can prevent the whole job from finishing, thus incurring high costs. Databricks jobs support a timeout configuration to terminate jobs that take longer than expected. \n\nAWS | Azure | GCP",
      "Databricks Capabilities": "Databricks Workflows"
    },
    {
      "Identifier": "R-01-05",
      "Best Practice": "Use a scalable and production-grade model serving infrastructure",
      "Details": "For batch and streaming inference, use Databricks jobs and MLflow to deploy models as Apache Spark UDFs to leverage job scheduling, retries, autoscaling, and so on. \nModel serving provides a scalable and production-grade model real-time serving infrastructure. It processes your machine learning models using MLflow and exposes them as REST API endpoints. This functionality uses serverless compute, which means that the endpoints and associated compute resources are managed and run in the Databricks cloud account. \n\nAWS | Azure | GCP\nAWS | Azure | GCP",
      "Databricks Capabilities": "Databricks Model Serving"
    },
    {
      "Identifier": "R-01-06",
      "Best Practice": "Use managed services for your workloads",
      "Details": "Leverage managed services of the Databricks Data Intelligence Platform like serverless compute, model serving, or Delta Live Tables where possible. These services are - without extra effort by the customer - operated by Databricks in a reliable and scalable way, making workloads more reliable.\n\nAWS | Azure | GCP\nAWS | Azure | GCP\nAWS | Azure | GCP",
      "Databricks Capabilities": "Databricks Platform"
    },
    {
      "Identifier": "R-02-01",
      "Best Practice": "Use a layered storage architecture",
      "Details": "Curate data by creating a layered architecture and ensuring data quality increases as data moves through the layers. A common layering approach is:\n\nRaw layer (bronze): Source data gets ingested into the lakehouse into the first layer and should be persisted there. When all downstream data is created from the raw layer, rebuilding the subsequent layers from this layer is possible if needed.\n\nCurated layer (silver): The purpose of the second layer is to hold cleansed, refined, filtered and aggregated data. The goal of this layer is to provide a sound, reliable foundation for analyses and reports across all roles and functions.\n\nFinal layer (gold): The third layer is created around business or project needs. It provides a different view as data products to other business units or projects, preparing data around security needs (such as anonymized data) or optimizing for performance (such as with pre aggregated views). The data products in this layer are seen as the truth for the business.\n\nThe final layer should only contain high-quality data and can be fully trusted from a business point of view.\nAWS | Azure | GCP",
      "Databricks Capabilities": "Databricks Lakehouse Architecture"
    },
    {
      "Identifier": "R-02-02",
      "Best Practice": "Improve data integrity by reducing data redundancy",
      "Details": "Copying or duplicating data creates data redundancy and will lead to lost integrity, lost data lineage, and often different access permissions. This will decrease the quality of the data in the lakehouse. A temporary or throwaway copy of data is not harmful on its own - it is sometimes necessary for boosting agility, experimentation and innovation. However, if these copies become operational and regularly used for business decisions, they become data silos. These data silos getting out of sync has a significant negative impact on data integrity and quality, raising questions such as \u201cWhich data set is the master?\u201d or \u201cIs the data set up to date?\u201d.\n\nAWS | Azure | GCP",
      "Databricks Capabilities": "Databricks Platform"
    },
    {
      "Identifier": "R-02-03",
      "Best Practice": "Actively manage schemas",
      "Details": "Uncontrolled schema changes can lead to invalid data and failing jobs that use these data sets. Databricks has several methods to validate and enforce the schema:\n\nDelta Lake supports schema validation and schema enforcement by automatically handling schema variations to prevent the insertion of bad records during ingestion.\n\nAuto Loader detects the addition of new columns as it processes your data. By default, the addition of a new column causes your streams to stop with an UnknownFieldException. Auto Loader supports several modes for schema evolution. \n\nAWS | Azure | GCP",
      "Databricks Capabilities": "Unity Catalog"
    },
    {
      "Identifier": "R-02-04",
      "Best Practice": "Use constraints and data expectations",
      "Details": "Delta tables support standard SQL constraint management clauses that ensure that the quality and integrity of data added to a table are automatically verified. When a constraint is violated, Delta Lake throws an InvariantViolationException error to signal that the new data can\u2019t be added. See Constraints on Databricks.\n\nTo further improve this handling, Delta Live Tables supports Expectations: Expectations define data quality constraints on the contents of a data set. An expectation consists of a description, an invariant, and an action to take when a record fails the invariant. Expectations to queries use Python decorators or SQL constraint clauses. See Manage data quality with Delta Live Tables. \n\nAWS | Azure | GCP",
      "Databricks Capabilities": "Delta Live Tables"
    },
    {
      "Identifier": "R-02-05",
      "Best Practice": "Take a data-centric approach to machine learning",
      "Details": "One guiding principle that continues to lie at the heart of the AI vision for the Databricks Data Intelligence Platform is taking a data-centric approach to machine learning. With the increasing prevalence of generative AI, this perspective remains just as important. The core constituents of any ML project can be viewed simply as data pipelines \n\nFeature engineering, training, inference, and monitoring pipelines are data pipelines. They must be as robust as other production data engineering processes. Data quality is crucial in any ML application, so ML data pipelines should employ systematic approaches to monitoring and mitigating data quality issues. Avoid tools that make it challenging to join data from ML predictions, model monitoring, and so on, with the rest of your data. The simplest way to achieve this is to develop ML applications on the same platform used to manage production data. For example, instead of downloading training data to a laptop, where it is hard to govern and reproduce results, secure the data in cloud storage and make that storage available to your training process. \n\neBook",
      "Databricks Capabilities": "Databricks Platform"
    },
    {
      "Identifier": "R-03-01",
      "Best Practice": "Enable autoscaling for ETL workloads",
      "Details": "Autoscaling allows clusters to resize automatically based on workloads. Autoscaling can benefit many use cases and scenarios from both a cost and performance perspective. The documentation provides considerations for determining whether to use Autoscaling and how to get the most benefit.\n\nFor streaming workloads, Databricks recommends using Delta Live Tables with autoscaling. See Use autoscaling to increase efficiency and reduce resource usage. \n\nAWS | Azure | GCP\nAWS | Azure | GCP",
      "Databricks Capabilities": "Databricks Workflows"
    },
    {
      "Identifier": "R-03-02",
      "Best Practice": "Use autoscaling for SQL Warehouses",
      "Details": "The scaling parameter of a SQL warehouse sets the minimum and the maximum number of clusters over which queries sent to the warehouse are distributed. The default is a minimum of one and a maximum of one cluster.\n\nTo handle more concurrent users for a given warehouse, increase the cluster count. To learn how Databricks adds clusters to and removes clusters from a warehouse, see SQL warehouse sizing, scaling, and queuing behavior. \n\nAWS | Azure | GCP\nAWS | Azure | GCP\n\nDatabricks enhanced autoscaling optimizes cluster utilization by automatically allocating cluster resources based on workload volume, with minimal impact on the data processing latency of your pipelines. \n\nAWS | Azure | GCP\nAWS | Azure | GCP",
      "Databricks Capabilities": "Databricks SQL"
    },
    {
      "Identifier": "R-04-01",
      "Best Practice": "Recover from Structured Streaming query failures",
      "Details": "Structured Streaming provides fault-tolerance and data consistency for streaming queries. Using Databricks workflows, you can easily configure your Structured Streaming queries to restart on failure automatically. The restarted query continues where the failed one left off. \n\nAWS | Azure | GCP",
      "Databricks Capabilities": "Structured Streaming"
    },
    {
      "Identifier": "R-04-02",
      "Best Practice": "Recover ETL jobs using data time travel capabilities",
      "Details": "Despite thorough testing, a job in production can fail or produce some unexpected, even invalid, data. Sometimes this can be fixed with an additional job after understanding the source of the issue and fixing the pipeline that led to the issue in the first place. However, often this is not straightforward, and the respective job should be rolled back. Using Delta Time travel allows users to easily roll back changes to an older version or timestamp, repair the pipeline, and restart the fixed pipeline. See What is Delta Lake time travel?.\n\nA convenient way to do so is the RESTORE command. \n\nAWS | Azure | GCP\nBlog",
      "Databricks Capabilities": "Delta Lake - Delta Time Travel"
    },
    {
      "Identifier": "R-04-03",
      "Best Practice": "Leverage a job automation framework with built-in recovery",
      "Details": "Databricks Workflows are built for recovery. When a task in a multi-task job fails (and, as such, all dependent tasks), Databricks Workflows provide a matrix view of the runs, which lets you examine the issue that led to the failure. See View runs for a job. Whether it was a short network issue or a real issue in the data, you can fix it and start a repair run in Databricks Workflows. It runs only the failed and dependent tasks and keep the successful results from the earlier run, saving time and money. \n\nAWS | Azure | GCP",
      "Databricks Capabilities": "Databricks Workflows"
    },
    {
      "Identifier": "R-04-04",
      "Best Practice": "Configure a disaster recovery pattern",
      "Details": "Databricks is often a core part of an overall data ecosystem that includes many services, including upstream data ingestion services (batch/streaming), cloud-native storage, downstream tools and services such as business intelligence apps, and orchestration tooling. Some of your use cases might be particularly sensitive to a regional service-wide outage.\n\nDisaster recovery involves a set of policies, tools, and procedures that enable the recovery or continuation of vital technology infrastructure and systems following a natural or human-induced disaster. A large cloud service like Azure, AWS, or GCP serves many customers and has built-in guards against a single failure. For example, a region is a group of buildings connected to different power sources to guarantee that a single power loss will not shut down a region. However, cloud region failures can happen, and the degree of disruption and its impact on your organization can vary.\n\nEssential parts of a disaster recovery strategy are selecting a strategy (active/active or active/passive), selecting the right toolset, and testing both failover and restore. \n\nAWS | Azure | GCP",
      "Databricks Capabilities": NaN
    },
    {
      "Identifier": "R-05-01",
      "Best Practice": "Monitor data platform events",
      "Details": "Monitor Data platform and ML events",
      "Databricks Capabilities": NaN
    },
    {
      "Identifier": "R-05-02",
      "Best Practice": "Monitor cloud events",
      "Details": "Monitor events on your cloud provider",
      "Databricks Capabilities": NaN
    }
  ],
  "performance": [
    {
      "Identifier": "PE-01-01",
      "Best Practice": "Use serverless architecture",
      "Details": "With the serverless compute on the Databricks Data Intelligence Platform, the compute layer runs in the customer\u2019s Databricks account. Workspace admins can create serverless SQL warehouses that enable instant compute and are managed by Databricks. A serverless SQL warehouse uses compute clusters hosted in the Databricks customer account. Use them with Databricks SQL queries just like you usually would with the original Databricks SQL warehouses. Serverless compute comes with a very fast starting time for SQL warehouses (10s and below), and the infrastructure is managed by Databricks.\n\nThis leads to improved productivity:\n\nCloud administrators no longer have to manage complex cloud environments, for example by adjusting quotas, creating and maintaining networking assets, and joining billing sources.\n\nUsers benefit from near-zero waiting times for cluster start and improved concurrency on their queries.\n\nCloud administrators can refocus their time on higher-value projects instead of managing low-level cloud components. \n\nAWS | Azure | GCP",
      "Databricks Capabilities": "Databricks Serverless Compute"
    },
    {
      "Identifier": "PE-01-02",
      "Best Practice": "Use an enterprise grade model serving service",
      "Details": "Databricks Model Serving provides a unified interface to deploy, govern, and query AI models. Each model you serve is available as a REST API that you can integrate into your web or client application.\n\nModel Serving provides a highly available and low-latency service for deploying models. The service automatically scales up or down to meet demand changes, saving infrastructure costs while optimizing latency performance. This functionality uses serverless compute. \n\nAWS | Azure | GCP",
      "Databricks Capabilities": "Databricks Model Serving"
    },
    {
      "Identifier": "PE-02-01",
      "Best Practice": "Understand your data ingestion and access patterns",
      "Details": "From a performance perspective, data access patterns - such as \u201caggregations versus point access\u201d or \u201cscan versus search\u201d - behave differently depending on the data size. Large files are more efficient for scan queries and smaller files better for search since you have to read fewer data to find the specific row(s).\n\nFor ingestion patterns, it\u2019s common to use DML statements. DML statements are most performant when the data is clustered, and you can simply isolate the section of data. Keeping the data clustered and isolatable on ingestion is important: Consider keeping a natural time sort order and apply as many filters as possible to the ingest target table. For append-only and overwrite ingestion workloads, there isn't much to consider, as this is a relatively cheap operation.\n\nThe ingestion and access patterns often point to an obvious data layout and clustering. If they do not, decide what is more important to your business and skew toward how to solve that goal better. \n\nAWS | Azure | GCP",
      "Databricks Capabilities": NaN
    },
    {
      "Identifier": "PE-02-02",
      "Best Practice": "Use parallel computation where it is beneficial",
      "Details": "Time to value is an important dimension when working with data. While many use cases can be easily implemented on a single machine (small data, few and simple computation steps), often use cases come up that:\n\nNeed to process large data sets.\n\nHave long running times due to complicated algorithms.\n\nMust be repeated 100s and 1000s of times.\n\nThe cluster environment of the Databricks platform is a great environment to distribute these workloads efficiently. It automatically parallelism SQL queries across all nodes of a cluster and it provides libraries for Python and Scala to do the same. Under the hood, the engines Apache Spark and Photon analyze the queries, determine the optimal way of parallel execution, and manage the distributed execution in a resilient way.\nHere are some parallel procesing capabilities in databricks that works with apache sparrk\n\nAWS | Azure | GCP\nAWS | Azure | GCP\nAWS | Azure | GCP\nMLlib\nAWS | Azure | GCP\nAWS | Azure | GCP",
      "Databricks Capabilities": "Apache Spark"
    },
    {
      "Identifier": "PE-02-03",
      "Best Practice": "Analyze the whole chain of execution",
      "Details": "Most pipelines or consumption patterns use a chain of systems. For example, for BI tools the performance is impacted by several factors:\nThe BI tool itself.\nThe connector that connects the BI tool and the SQL engine.\nThe SQL engine where the BI tool sends the query.\nFor best-in-class performance, the whole chain needs to be taken into account and selected/tuned for best performance. \n\nAWS | Azure | GCP",
      "Databricks Capabilities": "Workflows; Unity Catalog"
    },
    {
      "Identifier": "PE-02-04",
      "Best Practice": "Prefer larger clusters",
      "Details": "Plan for larger clusters, especially when the workload scales linearly. In that case, it is not more expensive to use a large cluster for a workload than to use a smaller one. It\u2019s just faster. The key is that you're renting the cluster for the length of the workload. So, if you spin up two worker clusters and it takes an hour, you're paying for those workers for the full hour. Similarly, if you spin up a four-worker cluster and it takes only half an hour (here comes the linear scalability into play), the costs are the same. If costs are the primary driver with a very flexible SLA, an autoscaling cluster is almost always going to be the cheapest but not necessarily the fastest. ",
      "Databricks Capabilities": "Databricks Cluster Configuration"
    },
    {
      "Identifier": "PE-02-05",
      "Best Practice": "Use native Spark operations",
      "Details": "User Defined Functions (UDFs) are a great way to extend the functionality of Spark SQL. However, don\u2019t use Python or Scala UDFs if a native function exists:\nSpark SQL\nPySpark\n\nReasons:\nTo transfer data between Python and Spark, serialization is needed. This drastically slows down queries.\nHigher efforts for implementing and testing functionality already existing in the platform.\n\nIf native functions are missing and should be implemented as Python UDFs, use Pandas UDFs. Apache Arrow ensures data moves efficiently back and forth between Spark and Python.\n\nAWS | Azure | GCP\nApache Arrow ",
      "Databricks Capabilities": "Apache Spark"
    },
    {
      "Identifier": "PE-02-06",
      "Best Practice": "Use native platform engines",
      "Details": "Photon is the engine on Databricks that provides fast query performance at low cost \u2013 from data ingestion, ETL, streaming, data science, and interactive queries \u2013 directly on your data lake. Photon is compatible with Apache Spark APIs, so getting started is as easy as turning it on \u2013 no code changes and no lock-in.\n\nPhoton is part of a high-performance runtime that runs your existing SQL and DataFrame API calls faster and reduces your total cost per workload. Photon is used by default in Databricks SQL warehouses. \nPhoton is available by default for all Databricks SQL Warehouse Types\n\nAWS | Azure | GCP",
      "Databricks Capabilities": "Photon"
    },
    {
      "Identifier": "PE-02-07",
      "Best Practice": "Understand your hardware and workload type",
      "Details": "Not all cloud VMs are created equally. The different families of machines offered by cloud providers are all different enough to matter. There are obvious differences - RAM and cores - and more subtle differences - processor type and generation, network bandwidth guarantees, and local high-speed storage versus local disk versus remote disk. There are also differences in the \u201cspot\u201d markets. These should be understood before deciding on the best VM type for your workload. \n\nAWS | Azure | GCP\n\nPlease note - Serverless compute manages clusters automatically, so this is not needed for serverless compute.",
      "Databricks Capabilities": "Databricks Cluster Configuration"
    },
    {
      "Identifier": "PE-02-08",
      "Best Practice": "Use caching",
      "Details": "There are two types of caching available in Databricks: Delta caching and Spark caching. \nUse Disk Cache and Avoid Spark Caching\nAWS | Azure | GCP\nSpark performance tuning\n\nAdditional Cache Types: \nQuery Result Cache - AWS | Azure | GCP \nDatabricks SQL UI caching - AWS | Azure | GCP  \nPrewarm Delta cache for BI workloads\n\nPrewarm clusters (Serverless compute manages clusters automatically, so this is not needed for serverless compute.)\nAWS | Azure | GCP",
      "Databricks Capabilities": "Databricks Cluster Configuration"
    },
    {
      "Identifier": "PE-02-09",
      "Best Practice": "Use compaction",
      "Details": "Delta Lake on Databricks can improve the speed of reading queries from a table. One way to improve this speed is to coalesce small files into larger ones. You trigger compaction by running the OPTIMIZE command. See Compact data files with optimize on Delta Lake.\n\nYou can also compact small files automatically using Auto Optimize. See Consider file size tuning. \n\nAWS | Azure | GCP",
      "Databricks Capabilities": "Optimize with ZOrder"
    },
    {
      "Identifier": "PE-02-10",
      "Best Practice": "Use data skipping",
      "Details": "Data skipping: To achieve this, data skipping information is collected automatically when you write data into a Delta table (by default Delta Lake on Databricks collects statistics on the first 32 columns defined in your table schema). Delta Lake on Databricks takes advantage of this information (minimum and maximum values) at query time to provide faster queries. See Data skipping for Delta Lake.\n\nFor best results, apply Z-ordering, a technique to collocate related information in the same set of files. This co-locality is automatically used on Databricks by Delta Lake data-skipping algorithms. This behavior dramatically reduces the amount of data Delta Lake on Databricks needs to read.\n\nDynamic file pruning: Dynamic file pruning (DFP) can significantly improve the performance of many queries on Delta tables. DFP is especially efficient for non-partitioned tables or joins on non-partitioned columns. \nAWS | Azure | GCP\nAWS | Azure | GCP\nAWS | Azure | GCP\nDelta Lake liquid clustering replaces table partitioning and ZORDER to simplify data layout decisions and optimize query performance. Liquid clustering provides flexibility to redefine clustering keys without rewriting existing data, allowing data layout to evolve alongside analytic needs over time.\n\nAWS | Azure | GCP",
      "Databricks Capabilities": "Optimize with ZOrder"
    },
    {
      "Identifier": "PE-02-11",
      "Best Practice": "Enable Predictive Optimization on your metastore",
      "Details": "Predictive optimization removes the need to manually manage maintenance operations for Delta tables on Databricks.\n\nWith predictive optimization enabled, Databricks automatically identifies tables that would benefit from maintenance operations and runs them for the user. Maintenance operations are only run as necessary, eliminating both unnecessary runs for maintenance operations and the burden associated with tracking and troubleshooting performance.\n\nAWS | Azure | GCP",
      "Databricks Capabilities": "Predictive Optimization"
    },
    {
      "Identifier": "PE-02-12",
      "Best Practice": "Avoid over-partitioning",
      "Details": "In the past, partitioning was the most common way to skip data. However, partitioning is static and manifests as a file system hierarchy. There is no easy way to change partitions if the access patterns change over time. Often, partitioning leads to over-partitioning - in other words, too many partitions with too small files, which results in bad query performance. See Partitions.\n\nIn the meantime, a much better choice than partitioning is Z-ordering.\nAWS | Azure | GCP",
      "Databricks Capabilities": "Partitioning"
    },
    {
      "Identifier": "PE-02-14",
      "Best Practice": "Consider file size tuning",
      "Details": "The term auto optimize is sometimes used to describe functionality controlled by the settings delta.autoCompact and delta.optimizeWrite. This term has been retired in favor of describing each setting individually. See Configure Delta Lake to control data file size.\n\nAuto Optimize is particularly useful in the following scenarios:\nStreaming use cases where latency in the order of minutes is acceptable.\nMERGE INTO is the preferred method of writing into Delta Lake.\nCREATE TABLE AS SELECT or INSERT INTO are commonly used operations. \n\nAWS | Azure | GCP",
      "Databricks Capabilities": "Auto Optimize"
    },
    {
      "Identifier": "PE-02-15",
      "Best Practice": "Optimize join performance",
      "Details": "A range join occurs when two relations are joined using a point in interval or interval overlap condition. The range join optimization support in Databricks Runtime can bring orders of magnitude improvement in query performance but requires careful manual tuning \n\nAWS | Azure | GCP\n\nData skew is a condition in which a table\u2019s data is unevenly distributed among partitions in the cluster. Data skew can severely downgrade the performance of queries, especially those with joins. Joins between big tables require shuffling data, and the skew can lead to an extreme imbalance of work in the cluster. \nAWS | Azure | GCP\nAWS | Azure | GCP",
      "Databricks Capabilities": "Databricks Adaptive Query Execution"
    },
    {
      "Identifier": "PE-02-16",
      "Best Practice": "Run analyze table to collect table statistics",
      "Details": "Run analyze table to collect statistics on the entire table for the query plan\nThis information is persisted in the metastore and helps the query optimizer by:\n\nChoosing the proper join type.\nSelecting the correct build side in a hash-join.\nCalibrating the join order in a multi-way join.\n\nIt should be run alongside OPTIMIZE on a daily basis and is recommended on tables < 5TB. The only caveat is that analyze table is not incremental.\nAWS | Azure | GCP",
      "Databricks Capabilities": "Analyze Table"
    },
    {
      "Identifier": "PE-03-01",
      "Best Practice": "Test on data representative of production data",
      "Details": "Run performance testing on production data (read-only) or similar data. When using similar data, characteristics like volume, file layout, and data skews should be like production data, since this has a significant impact on performance.",
      "Databricks Capabilities": NaN
    },
    {
      "Identifier": "PE-03-02",
      "Best Practice": "Take prewarming of resources into account",
      "Details": "Take prewarming of resources into account\nThe first query on a new cluster is slower than all the others:\n\nIn general, cluster resources need to initialize on multiple layers.\n\nWhen caching is part of the setup, the first run ensures that the data is in the cache, which speeds up subsequent jobs.\n\nPrewarming resources - running specific queries for the sake of initializing resources and filling caches (for example, after a cluster restart) - can significantly increase the performance of the first queries. So, to understand the behavior for the different scenarios, test the performance of the first execution (with and without prewarming) and subsequent executions.\n\nAWS | Azure | GCP",
      "Databricks Capabilities": "Databricks Pools"
    },
    {
      "Identifier": "PE-03-03",
      "Best Practice": "Identify bottlenecks",
      "Details": "Bottlenecks are areas in your workload that might worsen the overall performance when the load in production increases. Identifying these at design time and testing against higher workloads will help to keep the workloads stable in production.",
      "Databricks Capabilities": NaN
    },
    {
      "Identifier": "PE-04-01",
      "Best Practice": "Monitor query performanance",
      "Details": "Query Profile: Utilize the query profile feature to troubleshoot performance bottlenecks during a query's execution. It provides visualization of each query task and related metrics such as time spent, number of rows processed, and memory consumption\n\nSQL Warehouse Monitoring: Monitor SQL warehouses by viewing live statistics, peak query count charts, running clusters charts, and query history table",
      "Databricks Capabilities": NaN
    },
    {
      "Identifier": "PE-04-02",
      "Best Practice": "Monitor streaming workloads",
      "Details": "Structured Streaming Monitoring: For streaming queries, use the built-in monitoring in the Spark UI under the Streaming tab or push metrics to external services using Apache Spark\u2019s Streaming Query Listener interfac",
      "Databricks Capabilities": NaN
    },
    {
      "Identifier": "PE-04-03",
      "Best Practice": "Monitor job performance",
      "Details": "View and manage job runs through the Databricks UI, which provides details on job output, logs, metrics, and the success or failure of each task in the job run",
      "Databricks Capabilities": NaN
    }
  ],
  "cost": [
    {
      "Identifier": "CO-01-01",
      "Best Practice": "Use performance optimized data formats",
      "Details": "Delta Lake is an open-source storage layer that enhances data lakes by providing ACID transactions, scalable metadata handling, and schema enforcement, primarily built on Apache Spark and Parquet. It supports a lakehouse architecture, allowing integration with various compute engines and programming languages. Delta Lake offers significant performance optimizations such as data skipping, indexing, and file management techniques like compaction and Z-Ordering. Additionally, it includes features like time travel for data versioning and Delta Live Tables for managing batch and streaming data pipelines efficiently. These capabilities make Delta Lake suitable for handling large-scale, transactional workloads and real-time analytics, ensuring data integrity and consistency across diverse data management scenarios\nBlog",
      "Databricks Capabilities": "Delta Lake"
    },
    {
      "Identifier": "CO-01-02",
      "Best Practice": "Use job clusters",
      "Details": "A job is a way to run non-interactive code in a Databricks cluster. For example, you can run an extract, transform, and load (ETL) workload interactively or on a schedule. Of course, you can also run jobs interactively in the notebook UI. However, on job clusters, the non-interactive workloads will cost significantly less than on all-purpose clusters.\nAn additional advantage is that every job or workflow runs on a new cluster, isolating workloads from one another.\n\nAWS | Azure | GCP\nPricing",
      "Databricks Capabilities": "Databricks Workflows"
    },
    {
      "Identifier": "CO-01-03",
      "Best Practice": "Use SQL warehouse for SQL workloads",
      "Details": "For interactive SQL workloads, a Databricks SQL warehouse is the most cost-efficient engine \n\nAWS | Azure | GCP",
      "Databricks Capabilities": "Databricks Serverless SQL"
    },
    {
      "Identifier": "CO-01-04",
      "Best Practice": "Use up-to-date runtimes for your workloads",
      "Details": "The Databricks platform provides different runtimes that are optimized for data engineering tasks (Databricks Runtime) or for Machine Learning (Databricks Runtime for Machine Learning). The runtimes are built to provide the best selection of libraries for the tasks and ensure that all provided libraries are up-to-date and work together optimally. Databricks Runtime is released on a regular cadence and offers performance improvements between major releases. These improvements in performance often lead to cost savings due to more efficient usage of cluster resources.\n\nAWS | Azure | GCP",
      "Databricks Capabilities": "Databricks Cluster Configuration"
    },
    {
      "Identifier": "CO-01-05",
      "Best Practice": "Only use GPUs for the right workloads",
      "Details": "Virtual machines with GPUs can dramatically speed up computational processes for deep learning, but have a significantly higher price than CPU-only machines. Use GPU instances only for workloads that have GPU-accelerated libraries.\nMost workloads do not use GPU-accelerated libraries do not benefit from GPU-enabled instances. Workspace admins can restrict GPU machines and clusters to prevent unnecessary use. \n\nAWS | Azure | GCP\nBlog",
      "Databricks Capabilities": "Databricks Cluster Configuration"
    },
    {
      "Identifier": "CO-01-06",
      "Best Practice": "Use Serverless for your workloads",
      "Details": "BI workloads typically use data in bursts and generate multiple concurrent queries. For example, someone using a BI tool might update a dashboard, write a query, or simply analyze query results without interacting further with the platform. This example demonstrates two requirements:\n\nTerminate clusters during idle periods to save costs.\nHave compute resources available quickly (for both start-up and scale-up) to satisfy user queries when they request new or updated data with the BI tool.\n\nServerless SQL warehouses start and scale up in seconds, so both immediate availability and termination during idle times can be achieved. This results in a great user experience and overall cost savings.\nAdditionally, serverless SQL warehouses scale down earlier than non-serverless warehouses, resulting lower costs. \n\nAWS | Azure | GCP\n\n\nDatabricks Model Serving provides a unified interface to deploy, govern, and query AI models. Each model you serve is available as a REST API that you can integrate into your web or client application.\n\nModel Serving provides a highly available and low-latency service for deploying models. The service automatically scales up or down to meet demand changes, saving infrastructure costs while optimizing latency performance. This functionality uses serverless compute. \n\nAWS | Azure | GCP",
      "Databricks Capabilities": "Databricks Serverless"
    },
    {
      "Identifier": "CO-01-07",
      "Best Practice": "Use the right instance type",
      "Details": "Based on your workloads, it is also important to choose the right instance family to get the best performance/price ratio. Some simple rules of thumb are:\nMemory optimized for ML, heavy shuffle & spill workloads\nCompute optimized for Structured Streaming workloads, maintenance jobs (e.g. Optimize & Vacuum)\nStorage optimized for workloads that benefit from caching, e.g. ad-hoc & interactive data analysis\nGPU optimized for specific ML & DL workloads\nGeneral purpose in absence of specific requirements",
      "Databricks Capabilities": "Databricks Cluster Configuration"
    },
    {
      "Identifier": "CO-01-08",
      "Best Practice": "Choose the most efficient cluster size",
      "Details": "Number of workers, instance type and size are important factors for compute configurations. Along with those when sizing compute, consider the data consumed, computational complexity, data partitioning, and parallelism required. For simple ETL workloads, focus on compute-optimized configurations, while memory and storage are important for shuffle-heavy workloads. Balancing the number of workers and instance size is crucial, as both can affect network I/O. Lastly, consider caching benefits for workloads that require frequent re-reads of the same data, using storage-optimized configurations with Delta Cache. \n\nAWS | Azure | GCP\nAWS | Azure | GCP",
      "Databricks Capabilities": "Databricks Cluster Configuration"
    },
    {
      "Identifier": "CO-01-09",
      "Best Practice": "Evaluate performance optimized query engines",
      "Details": "Photon is a high-performance Databricks-native vectorized query engine that speeds up your SQL workloads and DataFrame API calls (for data ingestion, ETL, streaming, data science, and interactive queries). Photon is compatible with Apache Spark APIs, so getting started is as easy as turning it on \u2013 no code changes and no lock-in.\nThe observed speedup can lead to significant cost savings, and jobs that run regularly should be evaluated to see whether they are not only faster but also cheaper with Photon.",
      "Databricks Capabilities": "Databricks Cluster Configuration"
    },
    {
      "Identifier": "CO-02-01",
      "Best Practice": "Leverage auto-scaling compute",
      "Details": "Autoscaling allows your workloads to use the right amount of compute required to complete your jobs.\n\nEnable autoscaling for batch workloads.\nEnable autoscaling for SQL warehouse.\nAWS | Azure | GCP\n\nCompute auto-scaling has limitations scaling down cluster size for Structured Streaming workloads. Databricks recommends using Delta Live Tables with Enhanced Autoscaling for streaming workloads.\nUse Delta Live Tables Enhanced Autoscaling.\nAWS | Azure | GCP",
      "Databricks Capabilities": "Databricks Cluster Configuration"
    },
    {
      "Identifier": "CO-02-02",
      "Best Practice": "Use auto termination",
      "Details": "Databricks provides a number of features to help control costs by reducing idle resources and controlling when compute resources can be deployed.\nConfigure auto termination for all interactive clusters. After a specified idle time, the cluster shuts down. See Automatic termination.\n\nIf a starting time that is significantly shorter than a full cluster start would be acceptable, consider using cluster pools. See Pool best practices. Databricks pools reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances. When a cluster is attached to a pool, cluster nodes are created using the pool\u2019s idle instances. If the pool has no idle instances, the pool expands by allocating a new instance from the instance provider in order to accommodate the cluster\u2019s request. When a cluster releases an instance, it returns to the pool and is free for another cluster to use. Only clusters attached to a pool can use that pool\u2019s idle instances.\nDatabricks does not charge DBUs while instances are idle in the pool, resulting in cost savings. Instance provider billing does apply. \n\nAWS | Azure | GCP\nAWS | Azure | GCP",
      "Databricks Capabilities": "Databricks Cluster Configuration"
    },
    {
      "Identifier": "CO-02-03",
      "Best Practice": "Use compute policies to control costs",
      "Details": "Cluster policies can enforce many cost specific restrictions for clusters. See Operational Excellence - Use cluster policies. For example:\n\nEnable cluster autoscaling with a set minimum number of worker nodes.\nEnable cluster auto termination with a reasonable value (for example, 1 hour) to avoid paying for idle times.\nEnsure that only cost-efficient VM instances can be selected. Follow the best practices for cluster configuration. See Compute configuration best practices.\nApply a spot instance strategy.\n\nAWS | Azure | GCP",
      "Databricks Capabilities": "Databricks Cluster Policies"
    },
    {
      "Identifier": "CO-03-01",
      "Best Practice": "Monitor costs",
      "Details": "The account console allows viewing the billable usage. As a Databricks account owner or account admin, you can also use the account console to download billable usage logs. Databricks system tables provide detailed usage details with system tables you can use these to monitor usage.\n\nAs a best practice, the full costs (including VMs, storage, and network infrastructure) should be monitored. This can be achieved by cloud provider cost management tools or by adding third party tools.\n\nAWS | Azure | GCP",
      "Databricks Capabilities": "Unity Catalog"
    },
    {
      "Identifier": "CO-03-02",
      "Best Practice": "Tag clusters for cost attribution",
      "Details": "To monitor cost and accurately attribute Databricks usage to your organization\u2019s business units and teams (for example, for chargebacks), you can tag clusters and pools. These tags propagate to detailed DBU usage reports and to cloud provider VMs and blob storage instances for cost analysis.\n\nEnsure that cost control and attribution are already in mind when setting up workspaces and clusters for teams and use cases. This streamlines tagging and improves the accuracy of cost attributions.\n\nFor the overall costs, DBU virtual machine, disk, and any associated network costs must be considered. For serverless SQL warehouses this is simpler since the DBU costs already include virtual machine and disk costs.\n\nAWS | Azure | GCP",
      "Databricks Capabilities": "Databricks Cluster Configuration"
    },
    {
      "Identifier": "CO-03-03",
      "Best Practice": "Implement observability to track & chargeback cost",
      "Details": "When working with complex technical ecosystems, proactively understanding the unknowns is key to maintaining platform stability and controlling costs. Observability provides a way to analyze and optimize systems based on the data they generate. This is different from monitoring, which focuses on identifying new patterns rather than tracking known issues.\nDatabricks provide great observability capabilities using System tables that are Databricks-hosted analytical stores of a customer account\u2019s operational data found in the system catalog. They provide historical observability across the account and include user-friendly tabular information on platform telemetry.",
      "Databricks Capabilities": "Databricks Cluster Configuration"
    },
    {
      "Identifier": "CO-03-04",
      "Best Practice": "Share cost reports regularly",
      "Details": "Create cost reports every month to track growth and anomalies in consumption. Share these reports broken down to use cases or teams with the teams that own the respective workloads by using cluster tagging. This avoids surprises and allows teams to proactively adapt their workloads if costs get too high.\n\nUse the account console to get high level usage - AWS | Azure | GCP\n\nDatabricks recommends using Unity Catalog system tables to generate detailed usage reports (by usage tags, etc) for enhanced cost reporting.",
      "Databricks Capabilities": "Unity Catalog - System Tables"
    },
    {
      "Identifier": "CO-03-05",
      "Best Practice": "Monitor and manage Delta Sharing egress costs",
      "Details": "Unlike other data sharing platforms, Delta Sharing does not require data replication. This model has many advantages, but it means that your cloud vendor may charge data egress fees when you share data across clouds or regions. See Monitor and manage Delta Sharing egress costs (for providers) to monitor and manage egress charges.",
      "Databricks Capabilities": "System Tables"
    },
    {
      "Identifier": "CO-04-01",
      "Best Practice": "Balance always-on and triggered streaming",
      "Details": "Traditionally, when people think about streaming, terms such as \u201creal-time,\u201d \u201c24/7,\u201d or \u201calways on\u201d come to mind. If data ingestion happens in \u201creal-time\u201d, the underlying cluster needs to run 24/7, producing consumption costs every single hour of the day.\n\nHowever, not every use case that is based on a continuous stream of events needs these events to be added to the analytics data set immediately. If the business requirement for the use case only needs fresh data every few hours or every day, then this requirement can be achieved with only several runs a day, leading to a significant cost reduction for the workload. Databricks recommends using Structured Streaming with trigger AvailableNow for incremental workloads that do not have low latency requirements. See Configuring incremental batch processing.\n\nAWS | Azure | GCP",
      "Databricks Capabilities": "Databricks Streaming"
    },
    {
      "Identifier": "CO-04-02",
      "Best Practice": "Balance between on-demand and capacity excess instances",
      "Details": "Spot instances use cloud virtual machine excess resources that are available at a cheaper price. To save cost, Databricks supports creating clusters using spot instances. It is recommended to always have the first instance (Spark driver) as an on-demand virtual machine. Spot instances are a great selection for workloads when it is acceptable to take longer because one or more spot instances have been evicted by the cloud provider. \n\nAWS | Azure | GCP",
      "Databricks Capabilities": "Databricks Cluster Configuration"
    }
  ]
}