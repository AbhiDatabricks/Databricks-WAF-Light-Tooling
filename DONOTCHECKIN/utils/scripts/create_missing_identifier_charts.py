#!/usr/bin/env python3
"""
Create Charts for MISSING WAF Identifiers
Maps missing identifiers to system tables and creates measurable charts
"""

import json
import uuid
import pandas as pd

def generate_id():
    return str(uuid.uuid4())[:8]

# Load Excel to get missing identifiers
excel_file = 'DONOTCHECKIN/Well-Architected Framework (WAF) - 22042025.xlsx'

print("ðŸ“Š Creating Charts for MISSING WAF Identifiers")
print("="*70)

# Define missing identifiers with their system table mappings
missing_charts = [
    # RELIABILITY - Missing Identifiers
    {
        "waf_id": "R-01-02",
        "practice": "Use a resilient distributed data engine for all workloads",
        "solution": "Apache Spark; Photon",
        "pillar": "Reliability",
        "dataset_name": f"waf_R_01_02_{generate_id()}",
        "display_name": "R-01-02: Spark/Photon Workload Distribution",
        "query": """
SELECT 
  CASE 
    WHEN photon_enabled = true THEN 'Photon Engine'
    ELSE 'Apache Spark'
  END as engine_type,
  COUNT(*) as workload_count,
  ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as percentage
FROM system.compute.clusters
WHERE start_time >= CURRENT_TIMESTAMP - INTERVAL '30' DAY
GROUP BY photon_enabled
        """,
        "chart_type": "pie",
        "fields": {"label_field": "engine_type", "value_field": "percentage", "label_label": "Engine", "value_label": "Percentage"}
    },
    
    {
        "waf_id": "R-01-03",
        "practice": "Automatically rescue invalid or nonconforming data",
        "solution": "Delta Live Tables",
        "pillar": "Reliability",
        "dataset_name": f"waf_R_01_03_{generate_id()}",
        "display_name": "R-01-03: Delta Live Tables Pipeline Health",
        "query": """
SELECT 
  pipeline_name,
  COUNT(*) as update_count,
  SUM(CASE WHEN update_state = 'COMPLETED' THEN 1 ELSE 0 END) as successful,
  SUM(CASE WHEN update_state = 'FAILED' THEN 1 ELSE 0 END) as failed,
  ROUND(SUM(CASE WHEN update_state = 'COMPLETED' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as success_rate
FROM system.lakeflow.pipeline_update_timeline
WHERE start_time >= CURRENT_DATE - INTERVAL '30' DAY
GROUP BY pipeline_name
ORDER BY update_count DESC
LIMIT 20
        """,
        "chart_type": "bar",
        "fields": {"x_field": "pipeline_name", "y_field": "success_rate", "x_label": "Pipeline", "y_label": "Success Rate %"}
    },
    
    {
        "waf_id": "R-01-06",
        "practice": "Use managed services for your workloads",
        "solution": "Databricks Platform",
        "pillar": "Reliability",
        "dataset_name": f"waf_R_01_06_{generate_id()}",
        "display_name": "R-01-06: Serverless vs Managed Compute Usage",
        "query": """
SELECT 
  CASE 
    WHEN cluster_type = 'SERVERLESS' THEN 'Serverless (Managed)'
    WHEN cluster_type = 'JOB' THEN 'Job Clusters (Managed)'
    ELSE 'Traditional (Self-Managed)'
  END as service_type,
  COUNT(*) as usage_count,
  ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as percentage
FROM system.compute.clusters
WHERE start_time >= CURRENT_TIMESTAMP - INTERVAL '30' DAY
GROUP BY service_type
        """,
        "chart_type": "pie",
        "fields": {"label_field": "service_type", "value_field": "percentage", "label_label": "Service Type", "value_label": "Percentage"}
    },
    
    {
        "waf_id": "R-02-01",
        "practice": "Use a layered storage architecture",
        "solution": "Databricks Lakehouse Architecture",
        "pillar": "Reliability",
        "dataset_name": f"waf_R_02_01_{generate_id()}",
        "display_name": "R-02-01: Table Distribution by Layer (Bronze/Silver/Gold)",
        "query": """
SELECT 
  CASE 
    WHEN table_schema LIKE '%bronze%' OR table_schema LIKE '%raw%' THEN 'Bronze (Raw)'
    WHEN table_schema LIKE '%silver%' OR table_schema LIKE '%cleaned%' THEN 'Silver (Cleaned)'
    WHEN table_schema LIKE '%gold%' OR table_schema LIKE '%curated%' THEN 'Gold (Curated)'
    ELSE 'Other'
  END as layer,
  COUNT(*) as table_count,
  ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as percentage
FROM system.information_schema.tables
WHERE table_catalog != 'hive_metastore'
GROUP BY layer
ORDER BY table_count DESC
        """,
        "chart_type": "pie",
        "fields": {"label_field": "layer", "value_field": "table_count", "label_label": "Layer", "value_label": "Table Count"}
    },
    
    {
        "waf_id": "R-02-02",
        "practice": "Improve data integrity by reducing data redundancy",
        "solution": "Databricks Platform",
        "pillar": "Reliability",
        "dataset_name": f"waf_R_02_02_{generate_id()}",
        "display_name": "R-02-02: Table Duplication Analysis (Same Name Across Schemas)",
        "query": """
SELECT 
  table_name,
  COUNT(DISTINCT table_schema) as schema_count,
  COUNT(*) as total_occurrences,
  STRING_AGG(DISTINCT table_schema, ', ') as schemas
FROM system.information_schema.tables
WHERE table_catalog != 'hive_metastore'
GROUP BY table_name
HAVING COUNT(DISTINCT table_schema) > 1
ORDER BY schema_count DESC
LIMIT 50
        """,
        "chart_type": "bar",
        "fields": {"x_field": "table_name", "y_field": "schema_count", "x_label": "Table Name", "y_label": "Schema Count"}
    },
    
    {
        "waf_id": "R-02-03",
        "practice": "Actively manage schemas",
        "solution": "Unity Catalog",
        "pillar": "Reliability",
        "dataset_name": f"waf_R_02_03_{generate_id()}",
        "display_name": "R-02-03: Schema Management - Tables per Schema",
        "query": """
SELECT 
  table_schema,
  COUNT(*) as table_count,
  COUNT(DISTINCT table_type) as table_types,
  MAX(last_altered) as last_modified
FROM system.information_schema.tables
WHERE table_catalog != 'hive_metastore'
GROUP BY table_schema
ORDER BY table_count DESC
LIMIT 30
        """,
        "chart_type": "bar",
        "fields": {"x_field": "table_schema", "y_field": "table_count", "x_label": "Schema", "y_label": "Table Count"}
    },
    
    {
        "waf_id": "R-02-04",
        "practice": "Use constraints and data expectations",
        "solution": "Delta Live Tables",
        "pillar": "Reliability",
        "dataset_name": f"waf_R_02_04_{generate_id()}",
        "display_name": "R-02-04: Data Quality Check Results",
        "query": """
SELECT 
  table_full_name,
  DATE_TRUNC('day', check_time) as date,
  COUNT(*) as total_checks,
  SUM(CASE WHEN check_status = 'PASSED' THEN 1 ELSE 0 END) as passed,
  SUM(CASE WHEN check_status = 'FAILED' THEN 1 ELSE 0 END) as failed,
  ROUND(SUM(CASE WHEN check_status = 'PASSED' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as pass_rate
FROM system.data_quality_monitoring.table_results
WHERE check_time >= CURRENT_DATE - INTERVAL '30' DAY
GROUP BY table_full_name, date
HAVING failed > 0
ORDER BY failed DESC, date DESC
LIMIT 50
        """,
        "chart_type": "bar",
        "fields": {"x_field": "table_full_name", "y_field": "failed", "x_label": "Table", "y_label": "Failed Checks"}
    },
    
    {
        "waf_id": "R-02-05",
        "practice": "Take a data-centric approach to machine learning",
        "solution": "Databricks Platform",
        "pillar": "Reliability",
        "dataset_name": f"waf_R_02_05_{generate_id()}",
        "display_name": "R-02-05: MLflow Experiment Activity (Data-Centric ML)",
        "query": """
SELECT 
  DATE_TRUNC('month', creation_time) as month,
  COUNT(*) as experiment_count,
  COUNT(DISTINCT user_id) as unique_users,
  COUNT(DISTINCT experiment_id) as active_experiments
FROM system.mlflow.experiments_latest
WHERE creation_time >= CURRENT_DATE - INTERVAL '12' MONTH
GROUP BY month
ORDER BY month DESC
        """,
        "chart_type": "line",
        "fields": {"x_field": "month", "y_field": "experiment_count", "x_label": "Month", "y_label": "Experiments"}
    },
    
    {
        "waf_id": "R-04-02",
        "practice": "Recover ETL jobs using data time travel capabilities",
        "solution": "Delta Lake - Delta Time Travel",
        "pillar": "Reliability",
        "dataset_name": f"waf_R_04_02_{generate_id()}",
        "display_name": "R-04-02: Delta Table Versions (Time Travel Capability)",
        "query": """
SELECT 
  table_schema,
  COUNT(DISTINCT table_name) as delta_tables,
  AVG(CASE WHEN table_type = 'MANAGED' THEN 1 ELSE 0 END) * 100 as delta_adoption_percent
FROM system.information_schema.tables
WHERE table_catalog != 'hive_metastore'
  AND table_type = 'MANAGED'
GROUP BY table_schema
ORDER BY delta_tables DESC
LIMIT 30
        """,
        "chart_type": "bar",
        "fields": {"x_field": "table_schema", "y_field": "delta_tables", "x_label": "Schema", "y_label": "Delta Tables"}
    },
    
    {
        "waf_id": "R-04-03",
        "practice": "Leverage a job automation framework with built-in recovery",
        "solution": "Databricks Workflows",
        "pillar": "Reliability",
        "dataset_name": f"waf_R_04_03_{generate_id()}",
        "display_name": "R-04-03: Job Run Success Rate with Auto-Recovery",
        "query": """
SELECT 
  DATE_TRUNC('day', start_time) as date,
  COUNT(*) as total_runs,
  SUM(CASE WHEN state = 'SUCCEEDED' THEN 1 ELSE 0 END) as succeeded,
  SUM(CASE WHEN state = 'FAILED' THEN 1 ELSE 0 END) as failed,
  ROUND(SUM(CASE WHEN state = 'SUCCEEDED' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as success_rate
FROM system.lakeflow.job_run_timeline
WHERE start_time >= CURRENT_DATE - INTERVAL '30' DAY
GROUP BY date
ORDER BY date DESC
        """,
        "chart_type": "line",
        "fields": {"x_field": "date", "y_field": "success_rate", "x_label": "Date", "y_label": "Success Rate %"}
    },
    
    {
        "waf_id": "R-05-01",
        "practice": "Monitor data platform events",
        "solution": "System Tables / Audit Logs",
        "pillar": "Reliability",
        "dataset_name": f"waf_R_05_01_{generate_id()}",
        "display_name": "R-05-01: Data Platform Event Monitoring",
        "query": """
SELECT 
  action_name,
  COUNT(*) as event_count,
  COUNT(DISTINCT user_identity.email) as unique_users,
  DATE_TRUNC('day', event_time) as date
FROM system.access.audit
WHERE event_time >= CURRENT_DATE - INTERVAL '30' DAY
  AND action_name IN ('getTable', 'createTable', 'deleteTable', 'updateTable', 'createJob', 'runJob')
GROUP BY action_name, date
ORDER BY date DESC, event_count DESC
LIMIT 100
        """,
        "chart_type": "bar",
        "fields": {"x_field": "action_name", "y_field": "event_count", "x_label": "Event Type", "y_label": "Event Count"}
    },
    
    {
        "waf_id": "R-05-02",
        "practice": "Monitor cloud events",
        "solution": "System Tables / Cloud Integration",
        "pillar": "Reliability",
        "dataset_name": f"waf_R_05_02_{generate_id()}",
        "display_name": "R-05-02: Cloud Resource Usage Monitoring",
        "query": """
SELECT 
  workspace_id,
  DATE_TRUNC('day', usage_start_time) as date,
  SUM(usage_quantity) as total_usage,
  COUNT(DISTINCT usage_type) as resource_types
FROM system.billing.usage
WHERE usage_start_time >= CURRENT_DATE - INTERVAL '30' DAY
GROUP BY workspace_id, date
ORDER BY date DESC, total_usage DESC
LIMIT 100
        """,
        "chart_type": "line",
        "fields": {"x_field": "date", "y_field": "total_usage", "x_label": "Date", "y_label": "Usage Quantity"}
    }
]

print(f"\nâœ… Created {len(missing_charts)} chart definitions for MISSING identifiers")
print(f"\nðŸ“Š Charts by Pillar:")
pillar_counts = {}
for chart in missing_charts:
    pillar = chart['pillar']
    pillar_counts[pillar] = pillar_counts.get(pillar, 0) + 1

for pillar, count in pillar_counts.items():
    print(f"   â€¢ {pillar}: {count} charts")

# Save chart definitions
with open('DONOTCHECKIN/missing_identifier_charts.json', 'w') as f:
    json.dump(missing_charts, f, indent=2)

print(f"\nâœ… Saved chart definitions to: DONOTCHECKIN/missing_identifier_charts.json")
print(f"\n{'='*70}")
print(f"ðŸ“‹ SUMMARY:")
print(f"   â€¢ Total missing identifier charts: {len(missing_charts)}")
print(f"   â€¢ All use system tables (real data)")
print(f"   â€¢ All map to WAF identifiers from Excel")
print(f"   â€¢ Ready to add to dashboard")
